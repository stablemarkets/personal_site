---
title: Bayesian Survival Analysis with Data Augmentation
author: Arman Oganisian
date: '2019-03-06'
slug: bayesian-survival-analysis-with-data-augmentation
categories:
  - Bayesian
  - Survival Analysis
  - MCMC
  - R
image:
  caption: ''
  focal_point: 'center'
output:
  blogdown::html_page:
    toc: true
    
---

```{r options, echo=F}
knitr::opts_chunk$set(warning = F, message = F, error = F, eval = T, cache = T) 
```
## Motivation
When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there's very little available online about Bayesian approaches even for fully parametric models. Here I'll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.

As with most of my posts, all MCMC is coded from scratch. It helps me and it helps readers understand the underlying algorithm - an intuition that is more difficult to get if you're just specifying the model in Stan.

## Model Set Up
Suppose we observe $i=1,\dots, r$ survival times, $T^o_i$. Survival times past the end of our study (at time $\tau$) are censored for subjects $i=r+1, \dots, n$. We know that the survival times for these subjects are greater than $\tau$, but that is all. Say we also have some $p\times 1$ covariate vector, $x_i$. Finally, we have indicator of whether survival time is observed $\delta_{1:n}$ for each subject. A parametric approach follows by assuming a model for $T$, we choose the Weibull

$$ T^o_i \sim Weibull(\alpha, \lambda_i) $$
Where $\alpha$ is the shape parameter and $\lambda_i$ is a subject-specific scale. An Accelerated Failure Time model (AFT) follows from modeling a reparameterization of the scale function $\lambda_i = exp(-\mu_i\alpha)$, where $\mu_i = x_i^T\beta$.

We'll consider the setting where we regress on a binary treatment indicator, $\mu_i = \beta_0 + \beta_1A$ where $A=1$ indicates treated and $A=0$ indicates untreated/placebo. This is a funky reparameterization, but it yields intuitive interpretations for $\beta_1$ in terms of the Weibull's hazard function, $h(t|\beta,x, \alpha) = \lambda_i\alpha x^{\alpha-1}$. Substituting $\lambda_i$, we see the hazard for treated subjects is $h(t|A=1) = e^{-(\beta_0 + \beta_1)*\alpha}\alpha t^{\alpha-1}$ and for untreated subjects it is $h(t|A=1) = e^{-(\beta_0)*\alpha}\alpha t^{\alpha-1}$. The hazard ratio is,

$$HR = \frac{h(t|A=1) }{h(t|A=0)} = e^{-\beta_1*\alpha} $$
If $HR=.5$, then the hazard of death, for example, at time $t$ is $50\%$ lower in the treated group, relative to the untreated.

From a Bayesian point of view, we are interested in the posterior $p(\beta, \alpha | T^o_{1:r} , \delta_{1:n},  \tau)$. Once we have this, we can get a whole posterior distribution for the survival function itself - as well as any quantity derived from it. For example, posterior mean and credible intervals for $HR$ (just a function of $\beta_1$ and $\alpha$). We can also get posterior survival curve estimates for each treatment group. For the Weibull, the survival curve is given by $S(t|\beta,\alpha, A) = exp(-\lambda t^\alpha)$ - again just a function of $\beta_1$ and $\alpha$.

## Data Augmentation
We'll first look at the joint data distribution (the likelihood) for this problem. The central idea is to view the survival times for the $n-r$ censored subjects as missing data, $T^m_{r+1:n}$. We refer to the full data as $T_{i=1:n} = (T_{i:r}^o, T_{r+1:n}^m)$. Now we construct a complete-data (augmented) likelihood with these values. The observed likelihood and complete-data likelihood are related by

$$
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, \alpha) & = \int p(T_{1:n}, \delta_{1:n} | \tau, \beta, \alpha) \ dT^m_{r+1:n} \\
  & = \int p(\delta_{1:n} | T_{1:n}, \tau, \beta, \alpha) \ p(T_{1:n} | \tau, \beta, \alpha) \ dT^m_{r+1:n}
\end{aligned}
$$
Now in this ideal, complete-data setting, we observe patients with either $\delta_i = 1 \ \cap \ T_i > \tau$ or with $\delta_i = 0 \ \cap \ T_i < \tau$. That is, $p(\delta_{i} | T_i, \tau, \beta, \alpha)=1$ if either of these conditions hold and $0$ otherwise.

We also assume that subjects are independent so that $p(T_{i=1:n} | \tau, \beta, \alpha) = p(T^o_{1:r}| \tau, \beta, \alpha)p( T^m_{r+1:n} | \tau, \beta, \alpha)$. So the likelihood simplifies to:
$$
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, \alpha) & = \prod_{i=1}^n\int p(\delta_{i} | T_{i}, \tau, \beta, \alpha) \ p(T_{i} | \tau, \beta, \alpha) \ dT^m_{r+1:n} \\
 & = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int p(\delta_{i} | T^m_{i}, \tau, \beta, \alpha) \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
& = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int I(T_i^m > \tau) \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
& = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int_\tau^\infty \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
\end{aligned}
$$
The first line follows by independence of observations. The second line follows by separating censored and uncensored subjects. $p(\delta_i | -)=1$ for all uncensored subjects, but $p(\delta_i | -)=1$ for censored subjects only when $T_i^m \in (0, \infty)$. Otherwise, the integrand is 0. Therefore, in the fourth line we only need to integrate of the region where the integrand is non-zero. 

Now the integral is over the region $T_i^m \in (0, \infty)$. But in this region $p(\delta_{i} | T^m_{i}, \tau, \beta, \alpha)=1$ only when $T_i^m >\tau$. 

This is the usual likelihood for frequentist survival models: uncensored subjects contribute to the likelihood via the density while censored subjects contribute to the likelihood via the survival function $\int_\tau^\infty \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i}$. Functions for this integral exist in for most basic distributions in `R`. For our Weibull model, it is `1-pweibull()`. We would simply place priors on $\beta$ and $\alpha$, then sample from the posterior using MCMC. 

But what if this integral was too hard to evaluate (as it may be for more complicated censoring mechanisms) and the complete data likelihood given below is easier?

$$
\begin{aligned}
 p(T^o_{1:r}, T^m_{r+1:n}, \delta_{1:n}| \tau, \beta, \alpha) & = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} I(T_i^m > \tau)\ p(T_{i}^m | \tau, \beta, \alpha)\\
\end{aligned}
$$
Then we can design a Gibbs sampler around this complete data likelihood.

## Metropolis-in-Gibbs Sampler
The target posterior of interest is 
$$p(\beta, \alpha, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n}) = p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) \ p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n})$$
Where each conditional posterior is known up to a proportionality constant. With a joint prior $p(\beta, \alpha)$ specified, we have

$$
\begin{aligned}
p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) & \propto \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} I(T_i^m > \tau)\ p(T_{i}^m | \tau, \beta, \alpha)  \\
& \propto p(\beta, \alpha) \prod_{i=1}^n p(T_{i}| \tau, \beta, \alpha) \\
\end{aligned}
$$
Note here that $p(T_{i}| \tau, \beta, \alpha)$ is the assumed Weibull density. We can use a Metropolis step to sample $(\beta, \alpha)$ from this distribution. 

The second conditional posterior is 
\begin{equation}
\begin{aligned}
p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n}) \propto \prod_{i| \delta_i=1} I(T_i^m > \tau)\ p(T_{i}^m | \tau, \beta, \alpha)
\end{aligned}
\end{equation}
This is a truncated Weibull distribution (truncated at the bottom by $\tau$). We can also sample from this using a Metropolis step.


The Gibbs sampler alternates between sampling from these two conditionals:

1. Given parameters $(\beta, \alpha)$, impute $T^m_i$ by drawing from $p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n})$, for each $i=r+1,\dots, n$.
2. Combine these imputed values, $T^m_{r+1:n}$, with observed data $T_{1:n}^o$, and update the parameters $(\beta, \alpha)$ from $p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n})$. 

As the parameter estimates update, the imputations get better. As the imputations get better, the parameter estimates improve. Over time the process yields draws from the joint posterior $p(\beta, \alpha, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n})$

We retain the sample of $(\beta, \alpha)$ for inference and toss samples of $T^m$.

## Simulation Example in R

All of the code implementing the augmented sampler (from scratch!) can be found on my [GitHub](https://github.com/stablemarkets/BayesianTutorials/tree/master/BayesianSurvival). Basically I simulate a data set with a binary treatment indicator for 1,000 subjects with censoring and survival times independently drawn from a Weibull. \\

For the $\beta$ vector, I use independent $N(0,sd=100)$ priors. For the shape parameter, I use an $Exp(1)$ prior. I run a single MCMC chain for 20,000 iterations and toss the first 15,000 out as burn-in. 

Here is the estimated survival function for each treatment group. Overlayed are the non-parametric estimates from a stratified Kaplan-Meier (KM) estimator. Note the parametric model is correctly specified here, so it does just as well as the KM in terms of estimating the mean curve. But the parametric model provides a less noisy fit - notice the credible bands are narrower at later time points when the at-risk counts get low in each treatment arm. 

```{r helpers, echo=FALSE}
log_post_beta <- function(beta, log_alpha, X, survt){ # shape
  alpha <- exp(log_alpha)
  mu <-  X %*% beta
  lambda <- exp(-1*mu*alpha)

  lik <- sum(dweibull( survt, shape = alpha, scale = lambda, log=T))
  pr <- sum(dnorm(x = beta, mean = 0, sd = 100, log = T))
  return(lik + pr)
}

log_post_alpha <- function(log_alpha, beta, X, survt){ # scale
  alpha <- exp(log_alpha)
  
  mu <-  X %*% beta
  lambda <- exp(-1*mu*alpha)
    
  lik <- sum(dweibull( survt, shape = alpha, scale = lambda, log=T))
  pr <- dexp(x = alpha, rate = 1, log = T)
  return(lik + pr)
}


metrop_hastings<-function(x_0, iter=1, log_post_density,
                          proposal_dist = function(x, prop_sigma){ 
                            MASS::mvrnorm(1, mu = x, Sigma = prop_sigma )
                          }, 
                          lower=-Inf, upper=Inf, prop_sigma,
                          ... ){
  for(i in 1:iter){
    # draw from proposal distribution
    x_star <- proposal_dist(x_0, prop_sigma) 
    
    # calculate ratio of conditional posterior densities
    r_num <- do.call(log_post_density, c(list(x_star), list(...)) )
    r_denom <- do.call(log_post_density, c(list(x_0), list(...)) )
    r <- exp(r_num - r_denom)
    rmin<-min(r,1)
    if(is.na(rmin)) browser()
    # accept / reject proposal
    if(rbinom(1,1,rmin)==1){ 
      x_0<-x_star
    }
  }
  
  res<-list(x_0 = x_0, accept_prob = rmin )
  return(res)
}
```


```{r simulate_data, echo=F}
library(survival)
library(truncdist)

################################################################################
### 0 - Simulate Data
################################################################################
set.seed(1)

n <- 1000
A <- rbinom(n, 1, .5)

X <- model.matrix(~ A)

true_beta <- (1/2)*matrix(c(-1/3, 2), ncol=1)
true_mu <- X %*% true_beta

true_sigma <- 1

true_alpha <- 1/true_sigma
true_lambda <- exp(-1*true_mu*true_alpha)

# simulate censoring and survival times
survt = rweibull(n, shape=true_alpha, scale = true_lambda) 
cent = rweibull(n, shape=true_alpha, scale = true_lambda)

## observed data:
#censoring indicator
delta <- cent < survt
survt[delta==1] <- cent[delta==1] # censor survival time.

# survt_all will combine observed and imputed survival times.
survt_all <- survt

# count number of missing/censored survival times
n_miss <- sum(delta) # number of censored subjects
row_miss <- c(1:n)[delta] # index for which rows are censored
```

```{r samp_settings, echo=F}
iter <- 20000 # number of gibbs iterations
burnin <- 15000 # burn-in iterations

# shells for storing parameters
hazard_ratio <- numeric(iter - burnin)

# initial values
beta_shell <- matrix(c(0,0), ncol=1)
lalpha_shell <- c(0)

# proposal distribution for betas
prop_covar <- diag(c(.01,.01))
```

```{r sampler, echo=F}
# plot stratified Kaplan-Meier
par(mfrow=c(1,1))
plot(survfit(Surv(survt, 1-delta) ~ A),conf.int = F, col=c('blue','red'),
             xlab=c('Time'),ylab='Survival Probability',
             main = 'Data augmentation with all subjects')

for(i in 2:iter){
  ## sample from posterior of parameters, 
  ## conditional on observed and missing survival times
  
  # metrop_hastings() is a custom function for generating a draw 
  # from conditional posterior of beta: log_post_beta
  beta_shell <- metrop_hastings(x_0 = beta_shell, 
                                    iter = 1,
                                    log_post_density = log_post_beta,
                                    prop_sigma = prop_covar, 
                                    X=X, survt=survt_all, 
                                    log_alpha=lalpha_shell )$x_0
  
  # sample from conditional posterior of alpha: log_post_alpha
  lalpha_shell <- metrop_hastings(x_0 = lalpha_shell, 
                                     iter = 1,
                                     log_post_density = log_post_alpha,
                                     prop_sigma = matrix(.001), 
                                     X=X, survt=survt_all, 
                                     beta=beta_shell)$x_0
  
  ## sample from conditional posterior of missing survival times
  mu_curr <-  X %*% beta_shell
  alpha_curr <- exp(lalpha_shell)
  
  for(m in row_miss){
    lambda_curr <- exp(-1*mu_curr[m]*alpha_curr)
    
    survt_all[m] <- rtrunc(1, spec = 'weibull', 
                           a = survt[m], 
                           shape =  alpha_curr, 
                           scale =  lambda_curr)
  }
  
  if(i>burnin){
    # store hazard ratio
    hazard_ratio[i-burnin] <- exp(-beta_shell[2]/alpha_curr) 

    if(i>iter - 500){
      # plot 500 posterior survival curve draws for treated and placebo
      mu_trt <-  sum(beta_shell)
      mu_pbo <-  beta_shell[1]
  
      post_draw <- rweibull(n, shape = alpha_curr, 
                            scale = exp(-1*mu_trt*alpha_curr)  )
      post_ecdf <- ecdf(post_draw)
      curve(1-post_ecdf(x), add=T, from=0, to=15, col='lightblue')
      
      post_draw <- rweibull(n, shape = alpha_curr, 
                            scale = exp(-1*mu_pbo*alpha_curr)  )
      post_ecdf <- ecdf(post_draw)
      curve(1-post_ecdf(x), add=T, from=0, to=15, col='darkgray')
    }
  }
}

# overlay KM curve and plot legend
lines(survfit(Surv(survt, 1-delta) ~ A),conf.int = T, col=c('black','blue'))
legend('topright', 
       legend = c('KM Curve and Intervals (TRT)',
                  'Posterior Survival Draws (TRT)',
                  'KM Curve and Intervals (PBO)',
                  'Posterior Survival Draws (PBO)'),
       col=c('black','gray','blue','lightblue'), 
       lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty='n')
```

That's just a helpful reminder of the efficiency gains parametric models have over nonparametric ones (when they're correctly specified. Let's take a look at the posterior distribution of the hazard ratio. The true value is indicated by the red line.

```{r hazard_ratio, echo=F, fig.width=12, fig.height=5}

par(mfrow=c(1,2))
plot(hazard_ratio, type='l',
     xlab='MCMC Iterations', ylab='Hazard Ratio', main='MCMC chain for hazard ratio')
abline(h=exp(-true_beta[2]*true_alpha), col='red')

hist(hazard_ratio,xlab='Hazard Ratio', 
     main='Posterior distribution of hazard ratio')
abline(v=exp(-true_beta[2]*true_alpha), col='red')
```
We could have run this thing for longer (and with multiple chains with different starting values). But I think this gets the point across. The posterior mean and $95\%$ credible interval are $.32 \ (.24-.40)$. The true value is $.367$. Not too bad. Remember this is only a single simulated dataset.


