---
title: Bayesian Survival Analysis with Data Augmentation
author: Arman Oganisian
date: '2019-03-06'
slug: bayesian-survival-analysis-with-data-augmentation
categories:
  - Bayesian
  - Survival Analysis
  - MCMC
tags:
  - R
  - Bayesian
image:
  caption: ''
  focal_point: ''
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#model-set-up">Model Set Up</a></li>
<li><a href="#data-augmentation">Data Augmentation</a></li>
<li><a href="#metropolis-in-gibbs-sampler">Metropolis-in-Gibbs Sampler</a></li>
<li><a href="#simulation-example-in-r">Simulation Example in R</a></li>
</ul>
</div>

<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.</p>
<p>As with most of my posts, all MCMC is coded from scratch. It helps me and it helps readers understand the underlying algorithm - an intuition that is more difficult to get if you’re just specifying the model in Stan.</p>
</div>
<div id="model-set-up" class="section level2">
<h2>Model Set Up</h2>
<p>Suppose we observe <span class="math inline">\(i=1,\dots, r\)</span> survival times, <span class="math inline">\(T^o_i\)</span>. Survival times past the end of our study (at time <span class="math inline">\(\tau\)</span>) are censored for subjects <span class="math inline">\(i=r+1, \dots, n\)</span>. We know that the survival times for these subjects are greater than <span class="math inline">\(\tau\)</span>, but that is all. Say we also have some <span class="math inline">\(p\times 1\)</span> covariate vector, <span class="math inline">\(x_i\)</span> and our task is to make inference about the survival function <span class="math inline">\(P(T \geq t)\)</span>. Finally, we have indicator of whether survival time is observed <span class="math inline">\(\delta_{1:n}\)</span> for each subject. A parametric approach follows by assuming a model for <span class="math inline">\(T\)</span>, we choose the Weibull</p>
<p><span class="math display">\[ T^o_i \sim Weibull(a_i, b) \]</span> Where <span class="math inline">\(a\)</span> is the shape parameter and <span class="math inline">\(b\)</span> is the scale. We model the shape parameter as a function of covariates <span class="math inline">\(a_i = exp(x_i^T \beta)\)</span>.</p>
<p>In this parametric setting, the target of interest (the survival function) is determined by the Weibull parameters, <span class="math inline">\(P(T&gt;t| a_i, b)\)</span> - which we’ll need to estimate.</p>
<p>From a Bayesian point of view, we are interested in the posterior <span class="math inline">\(p(\beta, b | T^o_{1:r} , \tau)\)</span>. Once we have this, we can get a whole posterior distribution for the survival function itself - as well as any quantity derived from it (e.g. mean survival time, hazard, etc).</p>
</div>
<div id="data-augmentation" class="section level2">
<h2>Data Augmentation</h2>
<p>We’ll first look at the joint data distribution (the likelihood) for this problem. The central idea is to view the survival times for the <span class="math inline">\(n-r\)</span> censored subjects as missing data, <span class="math inline">\(T^m_{r+1:n}\)</span>. We refer to the full data as <span class="math inline">\(T_{i=1:n} = (T_{i:r}^o, T_{r+1:n}^m)\)</span>. Now we construct a complete-data (augmented) likelihood with these values. The observed likelihood and complete-data likelihood are related by</p>
<p><span class="math display">\[
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, b) &amp; = \int p(T_{1:n}, \delta_{1:n} | \tau, \beta, b) \ dT^m_{r+1:n} \\
  &amp; = \int p(\delta_{1:n} | T_{1:n}, \tau, \beta, b) \ p(T_{1:n} | \tau, \beta, b) \ dT^m_{r+1:n}
\end{aligned}
\]</span> Now in this ideal, complete-data setting, we observe patients with either <span class="math inline">\(\delta_i = 1 \ \cap \ T_i &gt; \tau\)</span> or with <span class="math inline">\(\delta_i = 0 \ \cap \ T_i &lt; \tau\)</span>. That is, <span class="math inline">\(p(\delta_{i} | T_i, \tau, \beta, b)=1\)</span> if either of these conditions hold and <span class="math inline">\(0\)</span> otherwise.</p>
<p>We also assume that subjects are independent so that <span class="math inline">\(p(T_{i=1:n} | \tau, \beta, b) = p(T^o_{1:r}| \tau, \beta, b)p( T^m_{r+1:n} | \tau, \beta, b)\)</span>. So the likelihood simplifies to: <span class="math display">\[
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, b) &amp; = \prod_{i=1}^n\int p(\delta_{i} | T_{i}, \tau, \beta, b) \ p(T_{i} | \tau, \beta, b) \ dT^m_{r+1:n} \\
 &amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, b) \prod_{i| \delta_i=1} \int p(\delta_{i} | T^m_{i}, \tau, \beta, b) \ p(T_{i}^m | \tau, \beta, b) \ dT^m_{i} \\
&amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, b) \prod_{i| \delta_i=1} \int I(T_i^m &gt; \tau) \ p(T_{i}^m | \tau, \beta, b) \ dT^m_{i} \\
&amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, b) \prod_{i| \delta_i=1} \int_\tau^\infty \ p(T_{i}^m | \tau, \beta, b) \ dT^m_{i} \\
\end{aligned}
\]</span> The first line follows by independence of observations. The second line follows by separating censored and uncensored subjects. <span class="math inline">\(p(\delta_i | -)=1\)</span> for all uncensored subjects, but <span class="math inline">\(p(\delta_i | -)=1\)</span> for censored subjects only when <span class="math inline">\(T_i^m \in (0, \infty)\)</span>. Otherwise, the integrand is 0. Therefore, in the fourth line we only need to integrate of the region where the integrand is non-zero.</p>
<p>Now the integral is over the region <span class="math inline">\(T_i^m \in (0, \infty)\)</span>. But in this region <span class="math inline">\(p(\delta_{i} | T^m_{i}, \tau, \beta, b)=1\)</span> only when <span class="math inline">\(T_i^m &gt;\tau\)</span>.</p>
<p>This is the usual likelihood for frequentist survival models: uncensored subjects contribute to the likelihood via the density while censored subjects contribute to the likelihood via the survival function <span class="math inline">\(\int_\tau^\infty \ p(T_{i}^m | \tau, \beta, b) \ dT^m_{i}\)</span>. Functions for this integral exist in for most basic distributions in <code>R</code>. For our Weibull model, it is <code>1-pweibull()</code>. We would simply place priors on <span class="math inline">\(\beta\)</span> and <span class="math inline">\(b\)</span>, then sample from the posterior using MCMC.</p>
<p>But what if this integral was too hard to evaluate (as it may be for more complicated censoring mechanisms) and the complete data likelihood given below is easier?</p>
<p><span class="math display">\[
\begin{aligned}
 p(T^o_{1:r}, T^m_{r+1:n}, \delta_{1:n}| \tau, \beta, b) &amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, b) \prod_{i| \delta_i=1} I(T_i^m &gt; \tau)\ p(T_{i}^m | \tau, \beta, b)\\
\end{aligned}
\]</span> Then we can design a Gibbs sampler around this complete data likelihood.</p>
</div>
<div id="metropolis-in-gibbs-sampler" class="section level2">
<h2>Metropolis-in-Gibbs Sampler</h2>
<p>The target posterior of interest is <span class="math display">\[p(\beta, b, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n}) = p(\beta, b | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) \ p(T_{r+1:n}^m | \beta, b, T^o_{1:r}, \delta_{1:n})\]</span> Where each conditional posterior is known up to a proportionality constant. With a joint prior <span class="math inline">\(p(\beta, b)\)</span> specified, we have</p>
<p><span class="math display">\[
\begin{aligned}
p(\beta, b | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) &amp; \propto \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, b) \prod_{i| \delta_i=1} I(T_i^m &gt; \tau)\ p(T_{i}^m | \tau, \beta, b)  \\
&amp; \propto p(\beta, b) \prod_{i=1}^n p(T_{i}| \tau, \beta, b) \\
\end{aligned}
\]</span> Note here that <span class="math inline">\(p(T_{i}| \tau, \beta, b)\)</span> is the assumed Weibull density. We can use a Metropolis step to sample <span class="math inline">\((\beta, b)\)</span> from this distribution.</p>
The second conditional posterior is
<span class="math display">\[\begin{equation}
\begin{aligned}
p(T_{r+1:n}^m | \beta, b, T^o_{1:r}, \delta_{1:n}) \propto \prod_{i| \delta_i=1} I(T_i^m &gt; \tau)\ p(T_{i}^m | \tau, \beta, b)
\end{aligned}
\end{equation}\]</span>
<p>This is a truncated Weibull distribution (truncated at the bottom by <span class="math inline">\(\tau\)</span>). We can also sample from this using a Metropolis step.</p>
<p>The Gibbs sampler alternates between sampling from these two conditionals:</p>
<ol style="list-style-type: decimal">
<li>Given parameters <span class="math inline">\((\beta, b)\)</span>, impute <span class="math inline">\(T^m_i\)</span> by drawing from <span class="math inline">\(p(T_{r+1:n}^m | \beta, b, T^o_{1:r}, \delta_{1:n})\)</span>, for each <span class="math inline">\(i=r+1,\dots, n\)</span>.</li>
<li>Combine these imputed values, <span class="math inline">\(T^m_{r+1:n}\)</span>, with observed data <span class="math inline">\(T_{1:n}^o\)</span>, and update the parameters <span class="math inline">\((\beta, b)\)</span> from <span class="math inline">\(p(\beta, b | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n})\)</span>.</li>
</ol>
<p>As the parameter estimates update, the imputations get better. As the imputations get better, the parameter estimates improve. Over time the process yields draws from the joint posterior <span class="math inline">\(p(\beta, b, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n})\)</span></p>
<p>We retain the sample of <span class="math inline">\((\beta, b)\)</span> for inference and toss samples of <span class="math inline">\(T^m\)</span></p>
</div>
<div id="simulation-example-in-r" class="section level2">
<h2>Simulation Example in R</h2>
<p>All of the code can be found on my <a href="https://github.com/stablemarkets/BayesianTutorials/tree/master/BayesianSurvival">GitHub</a>. Here I simulate a data set with three covariates for 1,000 subjects with censoring and survival times independently drawn from a Weibull.</p>
<p>Here are the results using the augmentated sampler described above. The posterior distribution of survival is concentrated around the truth. The Kaplan-Meier also does a nice job, but gives noisier estimates near the end.</p>
<p><img src="/post/2019-03-06-bayesian-survival-analysis-with-data-augmentation_files/figure-html/augmented_sampler-1.png" width="672" /></p>
<p>Below the Weibull survival estimated using only uncensored data. The Kaplan-Meier is estimated using all the data. As expected, ignoring censored subjects leads us to underestimate survival. <img src="/post/2019-03-06-bayesian-survival-analysis-with-data-augmentation_files/figure-html/naive_analysis-1.png" width="672" /></p>
</div>
