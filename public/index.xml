<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://stablemarkets.netlify.app/</link>
      <atom:link href="https://stablemarkets.netlify.app/index.xml" rel="self" type="application/rss+xml" />
    <description></description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 03 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://stablemarkets.netlify.app/media/icon_hu56437e0029043a6980b2632a303bfe7c_22443_512x512_fill_lanczos_center_3.png</url>
      <title></title>
      <link>https://stablemarkets.netlify.app/</link>
    </image>
    
    <item>
      <title>Python basics</title>
      <link>https://stablemarkets.netlify.app/courses/example/python/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/courses/example/python/</guid>
      <description>&lt;p&gt;Build a foundation in Python.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rfscVS0vtbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the difference between lists and tuples?&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Lists&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lists are mutable - they can be changed&lt;/li&gt;
&lt;li&gt;Slower than tuples&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_list = [1, 2.0, &#39;Hello world&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tuples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuples are immutable - they can&amp;rsquo;t be changed&lt;/li&gt;
&lt;li&gt;Tuples are faster than lists&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_tuple = (1, 2.0, &#39;Hello world&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Is Python case-sensitive?&lt;/summary&gt;
  &lt;p&gt;Yes&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Visualization</title>
      <link>https://stablemarkets.netlify.app/courses/example/visualization/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/courses/example/visualization/</guid>
      <description>&lt;p&gt;Learn how to visualize data with Plotly.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hSPmj7mK6ng&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;When is a heatmap useful?&lt;/summary&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Write Plotly code to render a bar chart&lt;/summary&gt;
  &lt;p&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.express as px
data_canada = px.data.gapminder().query(&amp;quot;country == &#39;Canada&#39;&amp;quot;)
fig = px.bar(data_canada, x=&#39;year&#39;, y=&#39;pop&#39;)
fig.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Statistics</title>
      <link>https://stablemarkets.netlify.app/courses/example/stats/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/courses/example/stats/</guid>
      <description>&lt;p&gt;Introduction to statistics for data science.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;The general form of the &lt;strong&gt;normal&lt;/strong&gt; probability density function is:&lt;/p&gt;
&lt;p&gt;$$
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the parameter $\mu$?&lt;/summary&gt;
  &lt;p&gt;The parameter $\mu$ is the mean or expectation of the distribution.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Bayesian Sequential Decision-Making with Informative Timing</title>
      <link>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is a brief summary of highlights from our &lt;a href=&#34;https://arxiv.org/abs/2211.16393&#34;&gt;recent working paper&lt;/a&gt; titled “Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing.” It’s part of a series of projects on developing robust Bayesian nonparametric models for sequential decision making in a variety of complex settings. In this case, we deal with a situation in which the decisions are informatively timed - with a motivating application in chemotherapy treatments for pediatric acute myeloid leukemia (AML).&lt;/p&gt;
&lt;p&gt;This is partially funded by a &lt;a href=&#34;https://www.pcori.org/research-results/2022/statistical-methods-optimizing-dynamic-patient-level-treatment-and-monitoring-strategies&#34;&gt;PCORI grant&lt;/a&gt; grant awarded earlier this year (2022).&lt;/p&gt;
&lt;div id=&#34;setting-and-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting and Problem&lt;/h1&gt;
&lt;p&gt;Chemotherapy treatment in AML is not “one and done” but involves making a sequence of treatment decisions over time, with each subsequent treatment decision depending on how the patient has responded to previous trts and the evolution of their disease process. There are many chemo agents. This work focuses on understanding the effect of anthracyclines (ACT) in particular on survival. ACT is known to be effective in certain cases, but it can also lead to cardiotoxicity and subsequent early death.&lt;/p&gt;
&lt;p&gt;This presents clinicians with a risk-reward tradeoff - trting too aggressively or too conservatively with ACT may reduce survival. To help inform decisions, an echocardiogram is done to help decide if the heart is healthy enough to tolerate ACT. Clinicians follow various rules of thumb in practice. For instance: withholding ACT if a patient’s current ejection fraction (measured via echocardiogram) falls below some absolute threshold (eg 50%) or if it declines more than some relative (e.g. declines more than 20% from time of enrollment). Briefly, ejection fraction (EF) is the proportion of blood that is pumped out of the heart’s left ventricle during a beat. In healthy individuals, this is fairly high (from 50-75%) but can be lower among patients with cardiotoxicity. These treatment rules are “dynamic” in that ACT is decided based on evolving ejection fraction. This is in contrast to static rules such as “always trt with ACT”, “never trt with ACT” or “alternate ACT” - regardless of EF. The goal is to develop a strategy for estimating survival rates under various hypothetical ACT assignment rules. If we have such a method, we could evaluate the efficacy of the various rules of thumb employed in clinics and arrive at a more data-driven approach.&lt;/p&gt;
&lt;p&gt;Luckily, using data from the Phase III AAML1031 trial, we can estimate effects of such ACT trt strategies on survival. In the trial, patients move through a sequence of 4 chemo courses. Ahead of each course, an echo is conducted &amp;amp; used to decide ACT inclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Challenges&lt;/h1&gt;
&lt;p&gt;There are many impediments to valid statistical estimation. 1) ACT is not randomized in the trial but informed by time-varying features such as EF. We need to adjust for such features to get an apples-to-apples comparison of different ACT rules. 2) Some patients die or drop out before ever completing the sequence. In the latter case, we are left with incomplete survival information. 3) Treatment courses are not initiated at pre-fixed times, but depending on when subjects recover from the previous chemotherapy course.&lt;/p&gt;
&lt;p&gt;This last point suggests that the waiting times between treatments are potential confounders (e.g. slower recovery after previous treatment may inform subsequent treatment and drive survival) - quite a unique type of time-varying confounding which requires modifications to the usual g-computation algorithm for sequential treatments&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-bayesian-semiparametric-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Bayesian Semiparametric Method&lt;/h1&gt;
&lt;p&gt;We model the causal structure using a non-homogenous continuous-time transition process. After each course, patients can transition into a state of subsequent death or transition into a state of subsequent treatment - whichever comes first - in continuous-time. These process is characterized by a pair of transition probabilities: one pair for each treatment course. We use Bayesian semiparametric hazard models to estimate the transition probabilities at each stage as a function of features available at the start of this course.&lt;/p&gt;
&lt;p&gt;Causal effects of certain rules are computed by simulating from the transition process under a specified rule and computing the proportion of simulated subjects who survive past a certain time point (e.g. 3 years, if we want to compute 3-year survival).&lt;/p&gt;
&lt;p&gt;Here is a twitter thread with some more details and images:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Wanted to share some work I’ve been doing on semiparametric Bayesian methods for sequential decision making.&lt;br&gt;&lt;br&gt;This work is motivated by a need to understand the efficacy and toxicity of chemotherapy agents in pediatric acute myeloid leukemia (aml).&lt;a href=&#34;https://t.co/ivRMd6O7Vq&#34;&gt;https://t.co/ivRMd6O7Vq&lt;/a&gt;
&lt;/p&gt;
— Arman Oganisian (&lt;span class=&#34;citation&#34;&gt;@StableMarkets&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/StableMarkets/status/1598711965857087489?ref_src=twsrc%5Etfw&#34;&gt;December 2, 2022&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing</title>
      <link>https://stablemarkets.netlify.app/publication/bayesdtraml/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/bayesdtraml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Collaborative Causal and Bayesian Modeling</title>
      <link>https://stablemarkets.netlify.app/project/applied/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/project/applied/</guid>
      <description>&lt;p&gt;This is a collection of some applied work with particularly interesting Bayesian and/or causal modeling. For instance, Hubbard et al. use a Bayesian mixture model to infer patients&#39; unknown diabetes status in noisy EHR data. Harrison et al. use a Bayesian hierarchical zero-inflated model to assess difference in costs associated with a cost-lowering healthcare intervention. Takvorian et al. use a difference-in-differences strategy to assess the impact of medicare expansion under ACA on cancer treatment delivery.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Nonparametric Bayes</title>
      <link>https://stablemarkets.netlify.app/talk/introduction-to-nonparametric-bayes/</link>
      <pubDate>Wed, 18 Nov 2020 15:12:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/introduction-to-nonparametric-bayes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Discussion on Bayesian Causal Forests</title>
      <link>https://stablemarkets.netlify.app/talk/invited-discussion-on-bayesian-causal-forests/</link>
      <pubDate>Sat, 26 Sep 2020 15:05:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/invited-discussion-on-bayesian-causal-forests/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Bayesian Bootstrap for Heterogeneous Treatment Effect Estimation</title>
      <link>https://stablemarkets.netlify.app/publication/hbb/</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/hbb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Causal Inference with Stan</title>
      <link>https://stablemarkets.netlify.app/talk/bayesian-causal-inference-with-stan/</link>
      <pubDate>Thu, 13 Aug 2020 15:05:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/bayesian-causal-inference-with-stan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nonparametric Cost-Effectiveness Analyses</title>
      <link>https://stablemarkets.netlify.app/talk/nonparametric-cost-effectiveness-analyses/</link>
      <pubDate>Thu, 06 Aug 2020 15:10:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/nonparametric-cost-effectiveness-analyses/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non-parametric Bayes</title>
      <link>https://stablemarkets.netlify.app/project/bnp/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/project/bnp/</guid>
      <description>&lt;p&gt;Bayesian nonparametrics is a powerful class of methods including Gaussian processes, Bayesian Additive Regression Trees (BART), Gamma Processes, Dirichlet Process, etc. The central idea is to abstract away from parameters. For instance, standard methods may assume a regression function is linear, indexed by finitely many slope/intercept parameters. A prior on these finitely many parameters then induces a prior on the regression function. To avoid specifying such restrictive functional form, we need to cut out the middle-man (the parameters) and specify priors over the function &lt;em&gt;directly&lt;/em&gt;. Bayesian nonparametrics is characterized by priors over such abstract objects: priors over regressions, priors over baseline hazard functions, prior distributions over distributions themselves.&lt;/p&gt;
&lt;p&gt;These methods give you the flexibility of machine learning, with the added benefit uncertainty quantitification via full posterior inference.&lt;/p&gt;
&lt;p&gt;See below for work related to nonparametric Bayesian inference.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Causal Inference</title>
      <link>https://stablemarkets.netlify.app/project/causalbayes/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/project/causalbayes/</guid>
      <description>&lt;p&gt;Causal inference is broadly concerned with estimating parameters governing the causal mechanisms between an intervention or treatment of interest and an outcome. Causal inference provides a framework for 1) constructing different estimands that have explicitly causal, rather than associational, interpretations 2) formulating the assumptions under which we can estimate these using observed data, 3) devising sensitivity analyses around violations of these assumptions, and 4) making valid inferences about these estimans. These are just some of the many advances made in the causal literature.&lt;/p&gt;
&lt;p&gt;In practice, causal inference requires complex, high-dimensional models. Here, the Bayesian paradigm has a lot to offer. For instance,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Shrinking&lt;/strong&gt; heterogeneous (stratum-specific) causal effects towards an overall average causal effect for sparse strata.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparsity priors&lt;/strong&gt; such as Horseshoes and Spike-and-Slab that can regularize high-dimensional nuissance parameters. Such parameters are common in g-computation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity Analyses&lt;/strong&gt; around causal identification assumptions. Uncertainty about the direction and magnitude of the bias can be expressed via a prior and baked into posterior inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See below for work related to Bayesian causal modeling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample versus Population ATE in Bayesian Caual Estimation</title>
      <link>https://stablemarkets.netlify.app/post/post5/tempered-mcmc/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post5/tempered-mcmc/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Writing up a quick post to clarify a point of common confusion when doing posterior inference for causal effects. All causal inference (regardless of statistical modeling paradigm in consideration) begins with expressing the target estimand in terms of unobservables (e.g. potential outcomes) and linking them to observed data with ``identification assumptions.’’ Two common estimands (which are often confused) are the population level average treatment effect (ATE) and the sample level ATE. These are two very different estimands and inferential procedures therefore differ for each. Yet, they seem to be confused with each other quite often. This post clarifies the distinction.&lt;/p&gt;
&lt;p&gt;Suppose we have a binary treatment &lt;span class=&#34;math inline&#34;&gt;\(A_i\in\{0,1\}\)&lt;/span&gt;, outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;, and a set of pre-treatment confounders &lt;span class=&#34;math inline&#34;&gt;\(L_i\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent subjects. Let the observed data be &lt;span class=&#34;math inline&#34;&gt;\(D = \{Y_i, A_i, L_i \}_{i=1}^n\)&lt;/span&gt;. Much of the following can be found in &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8761&#34;&gt;Oganisian &amp;amp; Roy, 2020&lt;/a&gt; and &lt;a href=&#34;https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Causal-Inference-A-Missing-Data-Perspective/10.1214/18-STS645.full&#34;&gt;Ding and Li, 2018&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;the-population-level-ate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The population-level ATE&lt;/h2&gt;
&lt;p&gt;The population-level in potential outcome notation can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[ \Psi = [Y^1 - Y^0] \]&lt;/span&gt;
Under certain identification assumptions, this is identified via the g-formula:
&lt;span class=&#34;math display&#34;&gt;\[ \Psi(\mu, P_L) = \int_{\mathcal{L}} \Big( \mu(1, l)- \mu(0, l)\Big) dP_L(l) \]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(\mu(a, l) = E[Y\mid A=a, L=l]\)&lt;/span&gt; is the outcome regression function. Here the notation &lt;span class=&#34;math inline&#34;&gt;\(\Psi(\mu, P_L)\)&lt;/span&gt; makes it explicit that the causal estimand is a function of two unknowns: the unknown regression function and the unknown confounder distribution. If we have estimates of these objects, &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat P_L\)&lt;/span&gt;, frequentist inference can be done via a plug-in estimator
&lt;span class=&#34;math display&#34;&gt;\[ \hat\Psi(\hat \mu, \hat P_L) = \int_{\mathcal{L}} \Big( \hat\mu(1, l)- \hat\mu(0, l)\Big) d\hat P_L(l) \]&lt;/span&gt;
For example, if we could fit a standard GLM with some inverse-link function,&lt;span class=&#34;math inline&#34;&gt;\(g^{-1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu(a, l) = g^{-1}(\hat \beta_0 + \hat \beta_1 A + \hat L&amp;#39;\beta_2)\)&lt;/span&gt;. Typically we use the empirical distribution for the confounder distribution estimate, &lt;span class=&#34;math inline&#34;&gt;\(\hat P_L(l) = \sum_{i=1}^n \frac{1}{n} I(L_i = l)\)&lt;/span&gt;. So we have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\hat\Psi(\hat \mu, \hat P_L) &amp;amp; = \int_{\mathcal{L}} \Big( \hat\mu(1, l)- \hat\mu(0, l)\Big) d\hat P_L(l) \\
                              &amp;amp; = \sum_{i=1}^n \frac{1}{n} \Big( \hat\mu(1, L_i)- \hat\mu(0, L_i)\Big) \\
\end{align*}
\]&lt;/span&gt;
That is, we end up average the difference in the mean function,&lt;span class=&#34;math inline&#34;&gt;\(\hat\mu(a, l)\)&lt;/span&gt;, under both treatments over the empirical distribution of confounders. Typically bootstrap is used to compute interval estimates for the population ATE.&lt;/p&gt;
&lt;p&gt;Bayesian inference for this quantity is also straightforward: obtain the &lt;span class=&#34;math inline&#34;&gt;\(m^{th}\)&lt;/span&gt; posterior draw of the regression function &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(m)}(a,l)\)&lt;/span&gt; under your favorite Bayesian model (a Generalized Linear Model, Gaussian process, Dirichlet Process, BART, etc). Then obtain the &lt;span class=&#34;math inline&#34;&gt;\(m^{th}\)&lt;/span&gt; posterior draw of the confounder distribution. It is common to use the ``Bayesian Bootstrap’’ for this - a Bayesian analogue of the empirical distribution: &lt;span class=&#34;math inline&#34;&gt;\(P_L^{(m)}(l) = \sum_{i=1}^n \gamma_i^{(m)} I(L_i = l)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\((\gamma_1^{(m)}, \gamma_2^{(m)}, \dots, \gamma_n^{(m)}) \sim Dir(1_n)\)&lt;/span&gt; are drawn froma Dirichlet Distribution. Then the posterior draw of the population-level ATE is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\Psi(\mu^{(m)}, P_L^{(m)}) &amp;amp; = \int_{\mathcal{L}} \Big(\mu^{(m)}(1, l)-\mu^{(m)}(0, l)\Big) d P_L^{(m)}(l) \\
                           &amp;amp; = \sum_{i=1}^n \gamma_i\Big(\mu^{(m)}(1, L_i)-\mu^{(m)}(0, L_i)\Big) \\
\end{align*}
\]&lt;/span&gt;
Again, we are taking a weighted average of the difference in the mean function draw, &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(m)}(a, l)\)&lt;/span&gt;, under each intervention. Under the Bayesian bootstrap, the posterior expectation of each &lt;span class=&#34;math inline&#34;&gt;\(\gamma_i\)&lt;/span&gt; is 1/n - so you think can think of this as being centered around the empirical distribution. Doing this for &lt;span class=&#34;math inline&#34;&gt;\(m=1,2,\dots, M\)&lt;/span&gt; we get a set of posterior draws for the population ATE which can be used for point and interval estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-sample-level-ate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The sample-level ATE&lt;/h2&gt;
&lt;p&gt;The sample-level ATE is given by
&lt;span class=&#34;math display&#34;&gt;\[ \psi = \frac{1}{n} \sum_{i=1}^n (Y_i^1 - Y_i^0) \]&lt;/span&gt;
This is a very different estimand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The uncertainty in the population-level ATE is due to uncertainty about the unknown regression function and confounder distribution.&lt;/li&gt;
&lt;li&gt;The uncertainty in the sample-level estimand is due to uncertainty about the unknown counterfactual. After all, under SUTVA for subject with assignment &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt;, we only observe &lt;span class=&#34;math inline&#34;&gt;\(Y_i^{A_i}\)&lt;/span&gt; while &lt;span class=&#34;math inline&#34;&gt;\(Y_i^{1-A_i}\)&lt;/span&gt; is unobserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Bayesian solutions, of course, differ as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To estimate the population-level ATE, we must draw the unknonwn regression function and confounder distributions from the posterior, &lt;span class=&#34;math inline&#34;&gt;\(f(\mu, P_L \mid D)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;To estimate the sample-level ATE, we must draw the unknown counterfactual for each subject &lt;span class=&#34;math inline&#34;&gt;\(Y_i^{1-A_i}\)&lt;/span&gt; from the posterior &lt;span class=&#34;math inline&#34;&gt;\(f( \{ Y_i^{1-A_i} \}_{i=1}^n \mid D)\)&lt;/span&gt; .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last bullet is Bayesian inference for causal effects as originally described by Donald Rubin. Specifically, by Bayes’ rule
&lt;span class=&#34;math display&#34;&gt;\[ f( \{ Y_i^{1-A_i} \}_{i=1}^n \mid D) \propto f_A(A_i \mid L_i, Y_i^{A_i}, Y_i^{1-A_i} ) f_*(Y^A_i, Y_i^{1-A_i}\mid A_i, L_i) f_L(L_i) \]&lt;/span&gt;
if the usual ignorability holds (due to say a randomized treatment) - i.e. &lt;span class=&#34;math inline&#34;&gt;\(Y^1, Y^0 \perp A \mid L\)&lt;/span&gt; - the selection mechanism no longer depends on potential outcomes: &lt;span class=&#34;math display&#34;&gt;\[f_A(A_i \mid L_i, Y_i^{A_i}, Y_i^{1-A_i} ) = f_A(A_i \mid L_i )\]&lt;/span&gt;
and, along with &lt;span class=&#34;math inline&#34;&gt;\(f_L\)&lt;/span&gt; can be dropped while maintaining proportionality. But the crucial thing here is that the remaining joint distribution &lt;span class=&#34;math inline&#34;&gt;\(f_*(Y^A_i, Y_i^{1-A_i}\mid A_i, L_i)\)&lt;/span&gt; is  - we never observe both potential outcomes for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. This complicates sample-level inference. We could still make inferences. For instance, by invoking de Finetti, we could model it as &lt;span class=&#34;math inline&#34;&gt;\(N_2(Y_i^{A_i}, Y_i^{1-A_i}\mid A_i, L_i; \eta, \Sigma ) p(\eta)p(\Sigma)\)&lt;/span&gt;. Where &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; indicates a bivariate normal distribution with mean vector &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and 2x2 covariance matrix, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. But the off-diagonal terms of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, $ Cov( Y_i^{A_i}, Y_i^{1-A_i} ) $, cannot be learned from data. The posterior is still defined, but will be completely driven by the prior &lt;span class=&#34;math inline&#34;&gt;\(p(\Sigma)\)&lt;/span&gt;. Thus, the sample-level effect is significantly more complicated .&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tempered MCMC for Multimodal Posteriors</title>
      <link>https://stablemarkets.netlify.app/post/post4/tempered-mcmc/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post4/tempered-mcmc/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;previous-posts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Previous Posts&lt;/h2&gt;
&lt;p&gt;This is part of a sequence of posts chronicling my journey to manually implement as many MCMC samplers as I can from scratch. Code from previous psots can be found on &lt;a href=&#34;https://github.com/stablemarkets/BayesianTutorials&#34;&gt;GitHub&lt;/a&gt;. Also I tweet more than I should: &lt;a href=&#34;https://twitter.com/StableMarkets&#34;&gt;StableMarkets&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multimodal-posterior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Multimodal posterior&lt;/h2&gt;
&lt;p&gt;I wanted to write up my own implementation of coupled MCMC chains using a tempered posterior along with an animation of the process. This is a classic sampling strategy used to deal with multi-modal posteriors. Here I have a tri-modal target posterior:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid D) = \frac{1}{3}N(-20,1) + \frac{1}{3}N(0,1) + \frac{1}{3}N(20,1)\]&lt;/span&gt;
The density looks like this&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post4/2020-03-22-tempered-mcmc_files/figure-html/plot_post-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice the regions of flat posterior density at about &lt;span class=&#34;math inline&#34;&gt;\((-15,-5)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((5,15)\)&lt;/span&gt;…these are often referred to as “bottlenecks”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problems-with-standard-mh&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problems with standard MH&lt;/h2&gt;
&lt;p&gt;These bottlenecks causes standard MCMC algorithms like Metropolis-Hastings (MH) to get stuck at one of these modes. Suppose at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; of a standard MH sampler, the current value of the parameter is &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(t-1)}= - 5\)&lt;/span&gt;. Suppose we use a Gaussian jumping distribution, so that we propose &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(t)}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(t)} \sim N( -5, \sigma)\)&lt;/span&gt;. Let’s say that &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; so the proposal distribution is proportional to the green density below&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post4/2020-03-22-tempered-mcmc_files/figure-html/plot_proposal-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s clear here that we’re almost never going to propose draws from the other two modes from this jumping distribution. Vast majority of the proposals to the left will end up in the bottlenecks and get rejected. We could increase &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to make the proposal distribution is wide enough to jump over these bottlenecks. However, we know in MH that increasing &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; tends to reduce acceptance probability in general. So maybe that helps us explore the other two modes, but we won’t be accepting frequently - slowing down how efficiently the chain explores the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tempered-posterior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Tempered Posterior&lt;/h2&gt;
&lt;p&gt;The idea behind tempering is to have two chains: one that is exploring the tempered posterior and another that explores the posterior. Ideally, the tempered posterior won’t have these bottlenecks, so a chain exploring it won’t have trouble getting from mode to mode. Then, we can propose jumps of the chain exploring the posterior to the tempered chain. This increases the chance of our chain of interest jumping to other modes.&lt;/p&gt;
&lt;p&gt;So when we say “tempered” we mean raising the posterior to some power (temperature) &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid D)^T\)&lt;/span&gt;. Let’s see what &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid D)^T\)&lt;/span&gt; looks like (proportional to gray density):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post4/2020-03-22-tempered-mcmc_files/figure-html/plot_temper-1.png&#34; width=&#34;960&#34; /&gt;
Notice that the tempered posterior has no bottlenecks. So an MH chain exploring this distribution won’t get stuck in bottlenecks of the posterior. So now we set up two chains: one exploring the tempered posterior and another exploring the posterior - both with standard MH updates. In each iteration, once we’ve update the two chains, we propose a swap between the two chains that is accepted with some probability. We say that the chains “meet” when these swaps occur. That is, we’ve in a sense “coupled” the chains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linking-both-chains&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linking both chains&lt;/h2&gt;
&lt;p&gt;Above is a gif of this playing out over 200 iterations. The gray chain is the standard MH chain (not including the swaps) that explores the tempered distribution. The blue chain is the chain exploring the posterior. The red dots indicate values of the blue chain that are swaps from the tempered chain. I.e. at these red points, the chains meet. Notice that the blue chain now easily hops between the modes by occasionally jumping to the gray chain.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post4/2020-03-22-tempered-mcmc_files/figure-html/make_gif-.gif&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How this scales to higher dimensions was a topic of much research - still sort of is. The choice of temperatures is crucial. Often, we need to use several chains, not just two chains as we did here.&lt;/p&gt;
&lt;p&gt;Some references: &lt;a href=&#34;https://watermark.silverchair.com/btg427.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAnIwggJuBgkqhkiG9w0BBwagggJfMIICWwIBADCCAlQGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM04FQsJU_yuBlk6pUAgEQgIICJU5z_DCSyt4AWDbdYc5DKoYVAwlXc4EAxGX3JiQSB-xRmw1yVhp7HxYz7FChVxgevsfxYge4-YiegspbC3DHo7YoF2f0am8rPPZiH8R2bHbm-jRG5FrlgIARRVCPqOvH4RzJpAyOSfMujiZapiJJ-1TQzBx2vWWOYl9rcKNsndj25dj19gNC8ac0qNlfe9kaNmZTkjJvkw3OGVhkBhYINM0MxVx22VBI0BDfGHlPUU8pfKIWm-IlCOJWGYOafYrxp5Jkx1D_tQcAGhfnUUx_IDNQkxnfi6zrfun_PBw9X846gb3HEaZl-6CPzWSwy9-eyx0B_hw_YzAuv1a1ykwlsVcFxRFqatSGy_qQNcHOnV5mDNqssmKvk4YFtbBjvRlhcoE3lIocOplAOO3WiVMfqG1s16N2z-t6UDKGCLJINsQtiW2bExmkhip4zd0jZiYG7RemKRlnSKVjMFrs98bHgNmOkeAZomX-ICG8dUmmQG6y9hL-LtGVtd7MR5jbop71eCMtfKT-02RkSSZrR-SN9czgiNlxf46w1IEu3ayAITwwCQkqi9Jme4lQBvo9FXpE8EVE75-qryOmfqIDbcVrq4uylsw149rTPupS5EVfMI_Rb9ULQAgLAugE5LU3hKz8rFDJzbcK1OBPmR36ZUBX7Ly_5jupsTZ-Qsm2HZkDLHXsgSlnB6xst7mLE9gL1jpjO3RVCrOp87n351p3-4Fb8SIYa35XNw&#34;&gt;Altekar (2004)&lt;/a&gt; is a nice outline and has references to seminal works by Geyers, Gilks and Roberts, etc. I based my sampler on the math they provide in the paper. Also this &lt;a href=&#34;https://darrenjw.wordpress.com/2013/09/29/parallel-tempering-and-metropolis-coupled-mcmc/&#34;&gt;post&lt;/a&gt; by Darren Wilkinson on MH coupled MCMC is a really nice treatment on this topic as well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Association of ACA Expansion with Timely Cancer Treatment</title>
      <link>https://stablemarkets.netlify.app/publication/acadid/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/acadid/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Practical Introduction to Bayesian Estimation of Causal Effects: Parametric and Nonparametric Approaches</title>
      <link>https://stablemarkets.netlify.app/publication/bayescausal/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/bayescausal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation and Adaptive Subgroup Discovery</title>
      <link>https://stablemarkets.netlify.app/publication/bnpce/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/bnpce/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Economic Outcomes of Insurer-Led Care Management for High-Cost Medicaid Patients</title>
      <link>https://stablemarkets.netlify.app/publication/ajmc/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/ajmc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Nonparametric Method for Zero-Inflated Outcomes: Prediction, Clustering, and Causal Inference</title>
      <link>https://stablemarkets.netlify.app/publication/zdp/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/zdp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BNP Model for Zero-Inflated Outcomes: Clustering, Prediction, Causal Inference</title>
      <link>https://stablemarkets.netlify.app/talk/bnp-model-for-zero-inflated-outcomes-clustering-prediction-causal-inference/</link>
      <pubDate>Wed, 31 Jul 2019 15:05:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/bnp-model-for-zero-inflated-outcomes-clustering-prediction-causal-inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gamma Process Prior for Semiparametric Survival Analysis</title>
      <link>https://stablemarkets.netlify.app/post/post3/gamma-process-prior/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post3/gamma-process-prior/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motviation&#34;&gt;Motviation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-gamma-process-prior&#34;&gt;The Gamma Process Prior&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#independent-hazards&#34;&gt;Independent Hazards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlated-hazards&#34;&gt;Correlated Hazards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Heads up: equations may not render on blog aggregation sites. See original post &lt;a href=&#34;https://stablemarkets.netlify.com/post/post3/gamma-process-prior/&#34;&gt;here&lt;/a&gt; for good formatting. If you like this post, you can follow me on &lt;a href=&#34;https://twitter.com/StableMarkets&#34;&gt;twitter&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;motviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motviation&lt;/h2&gt;
&lt;p&gt;Suppose we observe survival/event times from some distribution
&lt;span class=&#34;math display&#34;&gt;\[T_{i\in1:n} \stackrel{iid}{\sim} f(t)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the density and &lt;span class=&#34;math inline&#34;&gt;\(F(t)=1-S(t)\)&lt;/span&gt; is the corresponding CDF expressed in terms of the survival function &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;. We can represent the hazard function of this distribution in terms of the density,
&lt;span class=&#34;math display&#34;&gt;\[\lambda(t) = \frac{f(t)}{S(t)}\]&lt;/span&gt;
The hazard, CDF, and survival functions are all related. Thus, if we have a model for the hazard, we also have a model for the survival function and the survival time distribution. The well-known Cox proportional hazard approach models the hazard as a function of covariates &lt;span class=&#34;math inline&#34;&gt;\(x_i \in \mathbb{R}^p\)&lt;/span&gt; that multiply some baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[ \lambda(t_i) = \lambda_0(t_i)\exp(x_i&amp;#39;\theta)\]&lt;/span&gt;
Frequentist estimation of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; follows from maximizing the profile likelihood - which avoids the need to specify the baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;. The model is semi-parametric because, while we don’t model the baseline hazard, we require that the multiplicative relationship between covariates and the hazard is correct.&lt;/p&gt;
&lt;p&gt;This already works fine, so why go Bayesian? Here are just a few (hopefully) compelling reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We may want to nonparametrically estimate the baseline hazard itself.&lt;/li&gt;
&lt;li&gt;Posterior inference is exact, so we don’t need to rely on asymptotic uncertainty estimates (though we may want to evaluate the frequentist properties of resulting point and interval estimates).&lt;/li&gt;
&lt;li&gt;Easy credible interval estimation for any function of the parameters. If we have posterior samples for the hazard, we also get automatic inference for the survival function as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full Bayesian inference requires a proper probability model for both &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt;. This post walks through a Bayesian approach that places a nonparametric prior on &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt; - specifically the Gamma Process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gamma-process-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Gamma Process Prior&lt;/h2&gt;
&lt;div id=&#34;independent-hazards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independent Hazards&lt;/h3&gt;
&lt;p&gt;Most of this comes from &lt;a href=&#34;https://www.jstor.org/stable/pdf/2984758.pdf?refreqid=excelsior%3Ad44a383839b71ed572992049f08f2932&#34;&gt;Kalbfleisch (1978)&lt;/a&gt;, with an excellent technical outline by &lt;a href=&#34;https://www.springer.com/us/book/9780387952772&#34;&gt;Ibrahim (2001)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recall that the cumulative baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(H_0(t) = \int_0^t \lambda_0(t) dt\)&lt;/span&gt; where the integral is the Riemann-Stieltjes integral. The central idea is to develop a prior for the cumulative hazard &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt;, which will then admit a prior for the hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Gamma Process is such a prior. Each realization of a Gamma Process is a cumulative hazard function that is centered around some prior cumulative hazard function, &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;, with a sort of dispersion/concentration parameter, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that controls how tightly the realizations are distributed around the prior &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Okay, now the math. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}(\alpha, \beta)\)&lt;/span&gt; denote the Gamma distribution with shape parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(H^*(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t\geq 0\)&lt;/span&gt; be our prior cumulative hazard function. For example we could choose &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt; to be the exponential cumulative hazard, &lt;span class=&#34;math inline&#34;&gt;\(H^*(t)= \eta\cdot t\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is a fixed hyperparameter. By definition &lt;span class=&#34;math inline&#34;&gt;\(H^*(0)=0\)&lt;/span&gt;. The Gamma Process is defined as having the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0(0) = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t) = H_0(t) - H_0(s) \sim \mathcal G \Big(\ \beta\big(H^*(t) - H^*(s)\big)\ , \ \beta \ \Big)\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The increments in the cumulative hazard is the hazard function. The gamma process has the property that these increments are independent and Gamma-distributed. For a set of time increments &lt;span class=&#34;math inline&#34;&gt;\(t\geq0\)&lt;/span&gt;, we can use the properties above to generate one realization of hazards &lt;span class=&#34;math inline&#34;&gt;\(\{\lambda_0(t) \}_{t\geq0}\)&lt;/span&gt;. Equivaltently, one realization of the cumulative hazard function is &lt;span class=&#34;math inline&#34;&gt;\(\{H_0(t)\}_{t\geq0}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(H_0(t) = \sum_{k=0}^t \lambda_0(k)\)&lt;/span&gt;. We denote the Gamma Process just described as
&lt;span class=&#34;math display&#34;&gt;\[H_0(t) \sim \mathcal{GP}\Big(\ \beta H^*(t), \ \beta \Big),  \ \ t\geq0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Below in Panel A are some prior realizations of &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt; with a Weibull &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt; prior for various concentration parameters, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Notice for low &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; the realizations are widely dispersed around the mean cumulative hazard. Higher &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; yields to tighter dispersion around &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since there’s a correspondence between the &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(S_0(t)\)&lt;/span&gt;, we could also plot prior realizations of the baseline survival function &lt;span class=&#34;math inline&#34;&gt;\(S_0(t) = \exp\big\{- H_0(t) \big\}\)&lt;/span&gt; using the realization &lt;span class=&#34;math inline&#34;&gt;\(\{H_0(t)\}_{t\geq0}\)&lt;/span&gt;. This is shown in Panel B with the Weibull survival function &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; corresponding to &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post3/2019-05-11-the-gamma-process-prior_files/figure-html/unnamed-chunk-1-.gif&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlated-hazards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlated Hazards&lt;/h3&gt;
&lt;p&gt;In the previous section, the hazards &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt; between increments were a priori independent - a naive prior belief perhaps. Instead, we might expect that the hazard at the current time point is centered around the hazard in the previous time point. We’d also expect that a higher hazard at the previous time point likely means a higher hazard at the current time point (positive correlation across time).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00298&#34;&gt;Nieto‐Barajas et al (2002)&lt;/a&gt; came up with a correlated Gamma Process that expresses exactly this prior belief. The basic idea is to introduce a latent stochastic process &lt;span class=&#34;math inline&#34;&gt;\(\{u_t\}_{t\geq0}\)&lt;/span&gt; that links &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t-1)\)&lt;/span&gt;. Here is the correlated Gamma Process,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Draw a hazard rate for the first time interval, &lt;span class=&#34;math inline&#34;&gt;\(I_1=[0, t_1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \sim \mathcal G(\beta H^*(t_1), \beta)\)&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw a latent variable &lt;span class=&#34;math inline&#34;&gt;\(u_1 | \lambda_1 \sim Pois(c \cdot \lambda_1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw a hazard rate for second time interval &lt;span class=&#34;math inline&#34;&gt;\(I_2 = [t_1, t_2)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2 \sim \mathcal G(\beta( H^*(t_2) - H^*(t_1) ) + u_1, \beta + c )\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In general for &lt;span class=&#34;math inline&#34;&gt;\(k\geq1\)&lt;/span&gt;, define &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \beta( H^*(t_k) - H^*(t_{k-1}) )\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(u_k | \lambda_k \sim Pois(c\cdot \lambda_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1} | u_k \sim \mathcal G( \alpha_k + u_k, \beta + c )\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that if &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(u=0\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and the process reduces to the independent Gamma Process in the previous section. Now consider &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt;. Then, the hazard rate in the next interval has mean &lt;span class=&#34;math inline&#34;&gt;\(E[\lambda_{k+1} | u_k] = \frac{ \alpha_k + u_k }{\beta+c}\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(u_k \sim Pois(\lambda_k)\)&lt;/span&gt; is centered around the current &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt; - allowinng &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt; to influence &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1}\)&lt;/span&gt; through the latent variable &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;. The higher the current hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt;, the higher &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;, and the higher the mean of the next hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Below are some realizations of a correlated and independent Gamma processes centered around the &lt;span class=&#34;math inline&#34;&gt;\(Weibull(2,1.5)\)&lt;/span&gt; hazard shown in red. One realization is higlighted in blue to make it easier to see the differences between correlated and independent realizations&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post3/2019-05-11-the-gamma-process-prior_files/figure-html/unnamed-chunk-2-.gif&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice the correlated gamma process looks very snake-y. This is because of the autoregressive structure on &lt;span class=&#34;math inline&#34;&gt;\(\{\lambda_0(t)\}_{t\geq0}\)&lt;/span&gt; induced by the latent process&lt;span class=&#34;math inline&#34;&gt;\(\{ u_t\}_{t\geq0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Specifying Accelerated Failure Time Models in STAN</title>
      <link>https://stablemarkets.netlify.app/post/post2/specifying-accelerated-failure-time-models-in-stan/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post2/specifying-accelerated-failure-time-models-in-stan/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is an add-on to my &lt;a href=&#34;https://stablemarkets.netlify.com/post/post1/bayesian-survival-analysis-with-data-augmentation/&#34;&gt;previous post&lt;/a&gt; about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in &lt;a href=&#34;https://mc-stan.org/users/interfaces/rstan&#34;&gt;Stan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start with simulating some randomly censored data from a Weibull model. In this case, we just include a binary indicator and are interested in characterizing survival between these two groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

n &amp;lt;- 1000

# simulate covariates (just a binary treatment indicator)
A &amp;lt;- rbinom(n, 1, .5)
X &amp;lt;- model.matrix(~ A)

# true parameters
true_beta &amp;lt;- (1/2)*matrix(c(-1/3, 2), ncol=1)
true_mu &amp;lt;- X %*% true_beta

true_sigma &amp;lt;- 1

true_alpha &amp;lt;- 1/true_sigma
true_lambda &amp;lt;- exp(-1*true_mu*true_alpha)

# simulate censoring and survival times
survt = rweibull(n, shape=true_alpha, scale = true_lambda) 
cent = rweibull(n, shape=true_alpha, scale = true_lambda)

## observed data:
#censoring indicator
delta &amp;lt;- cent &amp;lt; survt
survt[delta==1] &amp;lt;- cent[delta==1] # censor survival time.

# count number of missing/censored survival times
n_miss &amp;lt;- sum(delta)

d_list &amp;lt;- list(N_m = n_miss, N_o = n - n_miss, P=2, # number of betas
               # data for censored subjects
               y_m=survt[delta==1], X_m=X[delta==1,],
               # data for uncensored subjects
               y_o=survt[delta==0], X_o=X[delta==0,])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The list &lt;code&gt;d_list&lt;/code&gt; is what we’ll eventually feed to Stan. Below is the Stan model for Weibull distributed survival times. Note in the transformed parameters block we specify the canonical accelerated failure time (AFT) parameterization - modeling the scale as a function of the shape parameter, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, and covariates.&lt;/p&gt;
&lt;p&gt;In the model block, we specify the likelihood as the Weibull density for uncensored subjects, and then augment the likelihood with evaluations from the Weibull survival function (&lt;code&gt;_lccdf&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The generated quantities block transforms the parameters to get posterior draws of the hazard ratio (as specified in my &lt;a href=&#34;https://stablemarkets.netlify.com/post/post1/bayesian-survival-analysis-with-data-augmentation/&#34;&gt;previous post&lt;/a&gt; ) as well as posterior draws of the survival function.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower=0&amp;gt; P; // number of beta parameters
  
  // data for censored subjects
  int&amp;lt;lower=0&amp;gt; N_m;
  matrix[N_m,P] X_m;
  vector[N_m] y_m;
  
  // data for observed subjects
  int&amp;lt;lower=0&amp;gt; N_o;
  matrix[N_o,P] X_o;
  real y_o[N_o];
}

parameters {
  vector[P] beta;                
  real&amp;lt;lower=0&amp;gt; alpha; // Weibull Shape      
}

transformed parameters{
  // model Weibull rate as function of covariates
  vector[N_m] lambda_m;
  vector[N_o] lambda_o;
  
  // standard weibull AFT re-parameterization
  lambda_m = exp((X_m*beta)*alpha);
  lambda_o = exp((X_o*beta)*alpha);
}

model {
  beta ~ normal(0, 100);
  alpha ~ exponential(1);
  
  // evaluate likelihood for censored and uncensored subjects
  target += weibull_lpdf(y_o | alpha, lambda_o);
  target += weibull_lccdf(y_m | alpha, lambda_m);
}


// generate posterior quantities of interest
generated quantities{
  vector[1000] post_pred_trt;
  vector[1000] post_pred_pbo;
  real lambda_trt; 
  real lambda_pbo; 
  real hazard_ratio;
  
  // generate hazard ratio
  lambda_trt = exp((beta[1] + beta[2])*alpha ) ;
  lambda_pbo = exp((beta[1])*alpha ) ;
  
  hazard_ratio = exp(beta[2]*alpha ) ;
  
  // generate survival times (for plotting survival curves)
  for(i in 1:1000){
    post_pred_trt[i] = weibull_rng(alpha,  lambda_trt);
    post_pred_pbo[i] = weibull_rng(alpha,  lambda_pbo);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan model specified above is stored in an object called &lt;code&gt;weibull_mod&lt;/code&gt;, which is called below in &lt;code&gt;sampling()&lt;/code&gt;. The code below samples from the posterior and outputs posterior draws of the hazard and predicted survival times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weibull_fit &amp;lt;- sampling(weibull_mod,
                data = d_list, 
                chains = 1, iter=20000, warmup=19000, save_warmup=F,
                pars= c(&amp;#39;hazard_ratio&amp;#39;,&amp;#39;post_pred_trt&amp;#39;,&amp;#39;post_pred_pbo&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;80acc0f9293b946800a710dd7f5e211c&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000172 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.72 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)
## Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)
## Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)
## Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)
## Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)
## Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)
## Chain 1: Iteration: 12000 / 20000 [ 60%]  (Warmup)
## Chain 1: Iteration: 14000 / 20000 [ 70%]  (Warmup)
## Chain 1: Iteration: 16000 / 20000 [ 80%]  (Warmup)
## Chain 1: Iteration: 18000 / 20000 [ 90%]  (Warmup)
## Chain 1: Iteration: 19001 / 20000 [ 95%]  (Sampling)
## Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 12.2212 seconds (Warm-up)
## Chain 1:                1.03363 seconds (Sampling)
## Chain 1:                13.2548 seconds (Total)
## Chain 1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_draws&amp;lt;-extract(weibull_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot posterior distribution of the hazard ratio. The red line indicates the true value under which we generated the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(post_draws$hazard_ratio,
     xlab=&amp;#39;Hazard Ratio&amp;#39;, main=&amp;#39;Hazard Ratio Posterior Distribution&amp;#39;)
abline(v=exp(-1*true_beta[2,1]*true_alpha), col=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post2/2019-03-09-specifying-accelerated-failure-time-models-in-stan_files/figure-html/plot_hazard_ratio-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(post_draws$hazard_ratio)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3658342&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(post_draws$hazard_ratio, probs = c(.025, .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.3039049 0.4376196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot the survival functions. Note these results are very similar to the augmented sampler coded in the previous post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(survfit(Surv(survt, 1-delta) ~ A ), col=c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;),
     xlab=&amp;#39;Time&amp;#39;,ylab=&amp;#39;Survival Probability&amp;#39;, conf.int=T)

for(i in 1:1000){
  trt_ecdf &amp;lt;- ecdf(post_draws$post_pred_trt[i,])
  curve(1 - trt_ecdf(x), from = 0, to=4, add=T, col=&amp;#39;gray&amp;#39;)
  
  pbo_ecdf &amp;lt;- ecdf(post_draws$post_pred_pbo[i,])
  curve(1 - pbo_ecdf(x), from = 0, to=4, add=T, col=&amp;#39;lightblue&amp;#39;)
}

lines(survfit(Surv(survt, 1-delta) ~ A ), col=c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;), add=T,
      conf.int=T)

legend(&amp;#39;topright&amp;#39;, 
       legend = c(&amp;#39;KM Curve and Intervals (TRT)&amp;#39;,
                  &amp;#39;Posterior Survival Draws (TRT)&amp;#39;,
                  &amp;#39;KM Curve and Intervals (PBO)&amp;#39;,
                  &amp;#39;Posterior Survival Draws (PBO)&amp;#39;),
       col=c(&amp;#39;black&amp;#39;,&amp;#39;gray&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;lightblue&amp;#39;), 
       lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty=&amp;#39;n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post2/2019-03-09-specifying-accelerated-failure-time-models-in-stan_files/figure-html/plot_survival-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Survival Analysis with Data Augmentation</title>
      <link>https://stablemarkets.netlify.app/post/post1/bayesian-survival-analysis-with-data-augmentation/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post1/bayesian-survival-analysis-with-data-augmentation/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-set-up&#34;&gt;Model Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-augmentation&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metropolis-in-gibbs-sampler&#34;&gt;Metropolis-in-Gibbs Sampler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simulation-example-in-r&#34;&gt;Simulation Example in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.&lt;/p&gt;
&lt;p&gt;As with most of my posts, all MCMC is coded from scratch. It helps me and it helps readers understand the underlying algorithm - an intuition that is more difficult to get if you’re just specifying the model in Stan.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Set Up&lt;/h2&gt;
&lt;p&gt;Suppose we observe &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots, r\)&lt;/span&gt; survival times, &lt;span class=&#34;math inline&#34;&gt;\(T^o_i\)&lt;/span&gt;. Survival times past the end of our study (at time &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;) are censored for subjects &lt;span class=&#34;math inline&#34;&gt;\(i=r+1, \dots, n\)&lt;/span&gt;. We know that the survival times for these subjects are greater than &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, but that is all. Say we also have some &lt;span class=&#34;math inline&#34;&gt;\(p\times 1\)&lt;/span&gt; covariate vector, &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Finally, we have indicator of whether survival time is observed &lt;span class=&#34;math inline&#34;&gt;\(\delta_{1:n}\)&lt;/span&gt; for each subject. A parametric approach follows by assuming a model for &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, we choose the Weibull&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T^o_i \sim Weibull(\alpha, \lambda_i) \]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the shape parameter and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; is a subject-specific scale. An Accelerated Failure Time model (AFT) follows from modeling a reparameterization of the scale function &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i = exp(-\mu_i\alpha)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mu_i = x_i^T\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We’ll consider the setting where we regress on a binary treatment indicator, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i = \beta_0 + \beta_1A\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A=1\)&lt;/span&gt; indicates treated and &lt;span class=&#34;math inline&#34;&gt;\(A=0\)&lt;/span&gt; indicates untreated/placebo. This is a funky reparameterization, but it yields intuitive interpretations for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; in terms of the Weibull’s hazard function, &lt;span class=&#34;math inline&#34;&gt;\(h(t|\beta,x, \alpha) = \lambda_i\alpha x^{\alpha-1}\)&lt;/span&gt;. Substituting &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt;, we see the hazard for treated subjects is &lt;span class=&#34;math inline&#34;&gt;\(h(t|A=1) = e^{-(\beta_0 + \beta_1)*\alpha}\alpha t^{\alpha-1}\)&lt;/span&gt; and for untreated subjects it is &lt;span class=&#34;math inline&#34;&gt;\(h(t|A=1) = e^{-(\beta_0)*\alpha}\alpha t^{\alpha-1}\)&lt;/span&gt;. The hazard ratio is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[HR = \frac{h(t|A=1) }{h(t|A=0)} = e^{-\beta_1*\alpha} \]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(HR=.5\)&lt;/span&gt;, then the hazard of death, for example, at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(50\%\)&lt;/span&gt; lower in the treated group, relative to the untreated.&lt;/p&gt;
&lt;p&gt;From a Bayesian point of view, we are interested in the posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\beta, \alpha | T^o_{1:r} , \delta_{1:n}, \tau)\)&lt;/span&gt;. Once we have this, we can get a whole posterior distribution for the survival function itself - as well as any quantity derived from it. For example, posterior mean and credible intervals for &lt;span class=&#34;math inline&#34;&gt;\(HR\)&lt;/span&gt; (just a function of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;). We can also get posterior survival curve estimates for each treatment group. For the Weibull, the survival curve is given by &lt;span class=&#34;math inline&#34;&gt;\(S(t|\beta,\alpha, A) = exp(-\lambda t^\alpha)\)&lt;/span&gt; - again just a function of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-augmentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Augmentation&lt;/h2&gt;
&lt;p&gt;We’ll first look at the joint data distribution (the likelihood) for this problem. The central idea is to view the survival times for the &lt;span class=&#34;math inline&#34;&gt;\(n-r\)&lt;/span&gt; censored subjects as missing data, &lt;span class=&#34;math inline&#34;&gt;\(T^m_{r+1:n}\)&lt;/span&gt;. We refer to the full data as &lt;span class=&#34;math inline&#34;&gt;\(T_{i=1:n} = (T_{i:r}^o, T_{r+1:n}^m)\)&lt;/span&gt;. Now we construct a complete-data (augmented) likelihood with these values. The observed likelihood and complete-data likelihood are related by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, \alpha) &amp;amp; = \int p(T_{1:n}, \delta_{1:n} | \tau, \beta, \alpha) \ dT^m_{r+1:n} \\
  &amp;amp; = \int p(\delta_{1:n} | T_{1:n}, \tau, \beta, \alpha) \ p(T_{1:n} | \tau, \beta, \alpha) \ dT^m_{r+1:n}
\end{aligned}
\]&lt;/span&gt;
Now in this ideal, complete-data setting, we observe patients with either &lt;span class=&#34;math inline&#34;&gt;\(\delta_i = 1 \ \cap \ T_i &amp;gt; \tau\)&lt;/span&gt; or with &lt;span class=&#34;math inline&#34;&gt;\(\delta_i = 0 \ \cap \ T_i &amp;lt; \tau\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(p(\delta_{i} | T_i, \tau, \beta, \alpha)=1\)&lt;/span&gt; if either of these conditions hold and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise.&lt;/p&gt;
&lt;p&gt;We also assume that subjects are independent so that &lt;span class=&#34;math inline&#34;&gt;\(p(T_{i=1:n} | \tau, \beta, \alpha) = p(T^o_{1:r}| \tau, \beta, \alpha)p( T^m_{r+1:n} | \tau, \beta, \alpha)\)&lt;/span&gt;. So the likelihood simplifies to:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
 p(T^o_{1:r}, \delta_{1:n}| \tau, \beta, \alpha) &amp;amp; = \prod_{i=1}^n\int p(\delta_{i} | T_{i}, \tau, \beta, \alpha) \ p(T_{i} | \tau, \beta, \alpha) \ dT^m_{r+1:n} \\
 &amp;amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int p(\delta_{i} | T^m_{i}, \tau, \beta, \alpha) \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
&amp;amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int I(T_i^m &amp;gt; \tau) \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
&amp;amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} \int_\tau^\infty \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i} \\
\end{aligned}
\]&lt;/span&gt;
The first line follows by independence of observations. The second line follows by separating censored and uncensored subjects. &lt;span class=&#34;math inline&#34;&gt;\(p(\delta_i | -)=1\)&lt;/span&gt; for all uncensored subjects, but &lt;span class=&#34;math inline&#34;&gt;\(p(\delta_i | -)=1\)&lt;/span&gt; for censored subjects only when &lt;span class=&#34;math inline&#34;&gt;\(T_i^m \in (0, \infty)\)&lt;/span&gt;. Otherwise, the integrand is 0. Therefore, in the fourth line we only need to integrate of the region where the integrand is non-zero.&lt;/p&gt;
&lt;p&gt;Now the integral is over the region &lt;span class=&#34;math inline&#34;&gt;\(T_i^m \in (0, \infty)\)&lt;/span&gt;. But in this region &lt;span class=&#34;math inline&#34;&gt;\(p(\delta_{i} | T^m_{i}, \tau, \beta, \alpha)=1\)&lt;/span&gt; only when &lt;span class=&#34;math inline&#34;&gt;\(T_i^m &amp;gt;\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is the usual likelihood for frequentist survival models: uncensored subjects contribute to the likelihood via the density while censored subjects contribute to the likelihood via the survival function &lt;span class=&#34;math inline&#34;&gt;\(\int_\tau^\infty \ p(T_{i}^m | \tau, \beta, \alpha) \ dT^m_{i}\)&lt;/span&gt;. Functions for this integral exist in for most basic distributions in &lt;code&gt;R&lt;/code&gt;. For our Weibull model, it is &lt;code&gt;1-pweibull()&lt;/code&gt;. We would simply place priors on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, then sample from the posterior using MCMC.&lt;/p&gt;
&lt;p&gt;But what if this integral was too hard to evaluate (as it may be for more complicated censoring mechanisms) and the complete data likelihood given below is easier?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
 p(T^o_{1:r}, T^m_{r+1:n}, \delta_{1:n}| \tau, \beta, \alpha) &amp;amp; = \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} I(T_i^m &amp;gt; \tau)\ p(T_{i}^m | \tau, \beta, \alpha)\\
\end{aligned}
\]&lt;/span&gt;
Then we can design a Gibbs sampler around this complete data likelihood.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;metropolis-in-gibbs-sampler&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metropolis-in-Gibbs Sampler&lt;/h2&gt;
&lt;p&gt;The target posterior of interest is
&lt;span class=&#34;math display&#34;&gt;\[p(\beta, \alpha, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n}) = p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) \ p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n})\]&lt;/span&gt;
Where each conditional posterior is known up to a proportionality constant. With a joint prior &lt;span class=&#34;math inline&#34;&gt;\(p(\beta, \alpha)\)&lt;/span&gt; specified, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n}) &amp;amp; \propto \prod_{i| \delta_i=0} p(T_{i}^o | \tau, \beta, \alpha) \prod_{i| \delta_i=1} I(T_i^m &amp;gt; \tau)\ p(T_{i}^m | \tau, \beta, \alpha)  \\
&amp;amp; \propto p(\beta, \alpha) \prod_{i=1}^n p(T_{i}| \tau, \beta, \alpha) \\
\end{aligned}
\]&lt;/span&gt;
Note here that &lt;span class=&#34;math inline&#34;&gt;\(p(T_{i}| \tau, \beta, \alpha)\)&lt;/span&gt; is the assumed Weibull density. We can use a Metropolis step to sample &lt;span class=&#34;math inline&#34;&gt;\((\beta, \alpha)\)&lt;/span&gt; from this distribution.&lt;/p&gt;
&lt;p&gt;The second conditional posterior is
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}
p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n}) \propto \prod_{i| \delta_i=1} I(T_i^m &amp;gt; \tau)\ p(T_{i}^m | \tau, \beta, \alpha)
\end{aligned}
\end{equation}\]&lt;/span&gt;
This is a truncated Weibull distribution (truncated at the bottom by &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;). We can also sample from this using a Metropolis step.&lt;/p&gt;
&lt;p&gt;The Gibbs sampler alternates between sampling from these two conditionals:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Given parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta, \alpha)\)&lt;/span&gt;, impute &lt;span class=&#34;math inline&#34;&gt;\(T^m_i\)&lt;/span&gt; by drawing from &lt;span class=&#34;math inline&#34;&gt;\(p(T_{r+1:n}^m | \beta, \alpha, T^o_{1:r}, \delta_{1:n})\)&lt;/span&gt;, for each &lt;span class=&#34;math inline&#34;&gt;\(i=r+1,\dots, n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Combine these imputed values, &lt;span class=&#34;math inline&#34;&gt;\(T^m_{r+1:n}\)&lt;/span&gt;, with observed data &lt;span class=&#34;math inline&#34;&gt;\(T_{1:n}^o\)&lt;/span&gt;, and update the parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta, \alpha)\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(\beta, \alpha | T_{r+1:n}^m, T^o_{1:r}, \delta_{1:n})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the parameter estimates update, the imputations get better. As the imputations get better, the parameter estimates improve. Over time the process yields draws from the joint posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\beta, \alpha, T_{r+1:n}^m | T^o_{1:r}, \delta_{1:n})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We retain the sample of &lt;span class=&#34;math inline&#34;&gt;\((\beta, \alpha)\)&lt;/span&gt; for inference and toss samples of &lt;span class=&#34;math inline&#34;&gt;\(T^m\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-example-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation Example in R&lt;/h2&gt;
&lt;p&gt;All of the code implementing the augmented sampler (from scratch!) can be found on my &lt;a href=&#34;https://github.com/stablemarkets/BayesianTutorials/tree/master/BayesianSurvival&#34;&gt;GitHub&lt;/a&gt;. Basically I simulate a data set with a binary treatment indicator for 1,000 subjects with censoring and survival times independently drawn from a Weibull. \&lt;/p&gt;
&lt;p&gt;For the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; vector, I use independent &lt;span class=&#34;math inline&#34;&gt;\(N(0,sd=100)\)&lt;/span&gt; priors. For the shape parameter, I use an &lt;span class=&#34;math inline&#34;&gt;\(Exp(1)\)&lt;/span&gt; prior. I run a single MCMC chain for 20,000 iterations and toss the first 15,000 out as burn-in.&lt;/p&gt;
&lt;p&gt;Here is the estimated survival function for each treatment group. Overlayed are the non-parametric estimates from a stratified Kaplan-Meier (KM) estimator. Note the parametric model is correctly specified here, so it does just as well as the KM in terms of estimating the mean curve. But the parametric model provides a less noisy fit - notice the credible bands are narrower at later time points when the at-risk counts get low in each treatment arm.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post1/2019-03-06-bayesian-survival-analysis-with-data-augmentation_files/figure-html/sampler-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s just a helpful reminder of the efficiency gains parametric models have over nonparametric ones (when they’re correctly specified. Let’s take a look at the posterior distribution of the hazard ratio. The true value is indicated by the red line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post1/2019-03-06-bayesian-survival-analysis-with-data-augmentation_files/figure-html/hazard_ratio-1.png&#34; width=&#34;1152&#34; /&gt;
We could have run this thing for longer (and with multiple chains with different starting values). But I think this gets the point across. The posterior mean and &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; credible interval are &lt;span class=&#34;math inline&#34;&gt;\(.32 \ (.24-.40)\)&lt;/span&gt;. The true value is &lt;span class=&#34;math inline&#34;&gt;\(.367\)&lt;/span&gt;. Not too bad. Remember this is only a single simulated dataset.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://stablemarkets.netlify.app/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian latent class approach for EHR‐based phenotyping</title>
      <link>https://stablemarkets.netlify.app/publication/bayes_latent_phenotype/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/publication/bayes_latent_phenotype/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Nonparametric Method for Zero-Inflated Data with Applications to Medical Costs</title>
      <link>https://stablemarkets.netlify.app/talk/a-bayesian-nonparametric-method-for-zero-inflated-data-with-applications-to-medical-costs/</link>
      <pubDate>Mon, 30 Jul 2018 15:05:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/talk/a-bayesian-nonparametric-method-for-zero-inflated-data-with-applications-to-medical-costs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ChiRP</title>
      <link>https://stablemarkets.netlify.app/project/chirp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/project/chirp/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://stablemarkets.netlify.app/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
