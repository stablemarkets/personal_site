[{"authors":["admin"],"categories":null,"content":"I am a 5th year biostatistics PhD candidate at the University of Pennsylvania, an Associate Fellow at the LDI, and a member of the Center for Causal Inference. My methodological research centers around developing nonparametric Bayesian methods for flexibly estimating causal effects with observational data. This involves, among other things, devising methods for efficient posterior computation via MCMC. Though broadly applicable, these methodological interests are particularly motivated by applied problems in health economics and, more recently, oncology.\nHere you will find a link to my full CV and a selection of some past and current research work. I sometimes blog about statistics, bayesian methods, computation/MCMC, and other things I happen to stumble upon during research. These posts are syndicated on R-bloggers.\n","date":1605745964,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1605745964,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://stablemarkets.netlify.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a 5th year biostatistics PhD candidate at the University of Pennsylvania, an Associate Fellow at the LDI, and a member of the Center for Causal Inference. My methodological research centers around developing nonparametric Bayesian methods for flexibly estimating causal effects with observational data. This involves, among other things, devising methods for efficient posterior computation via MCMC. Though broadly applicable, these methodological interests are particularly motivated by applied problems in health economics and, more recently, oncology.","tags":null,"title":"Arman Oganisian","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1551886505,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://stablemarkets.netlify.com/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1605730320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605745637,"objectID":"ecdaaa5fc33d505b0e83082f602b8d65","permalink":"https://stablemarkets.netlify.com/talk/generable/","publishdate":"2020-11-18T15:12:00-05:00","relpermalink":"/talk/generable/","section":"talk","summary":"","tags":[],"title":"Introduction to Nonparametric Bayes","type":"talk"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1601147100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605820687,"objectID":"2d9da934ad02948776cbc8aa669abb5b","permalink":"https://stablemarkets.netlify.com/talk/bcfdiscussion/","publishdate":"2020-09-26T15:05:00-04:00","relpermalink":"/talk/bcfdiscussion/","section":"talk","summary":"Invited Discussion of Bayesian Causal Forests hosted by *Bayesian Analysis*","tags":[],"title":"Invited Discussion on Bayesian Causal Forests","type":"talk"},{"authors":["**A. Oganisian**","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1600747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605745637,"objectID":"a3afc80a31f7ac8f2a98a0569746a049","permalink":"https://stablemarkets.netlify.com/publication/hbb/","publishdate":"2020-09-22T00:00:00-04:00","relpermalink":"/publication/hbb/","section":"publication","summary":"","tags":[],"title":"Hierarchical Bayesian Bootstrap for Heterogeneous Treatment Effect Estimation","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1597345500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605745637,"objectID":"253937e9ab37acc63c4c788c0ac85784","permalink":"https://stablemarkets.netlify.com/talk/stancon2020/","publishdate":"2020-08-13T15:05:00-04:00","relpermalink":"/talk/stancon2020/","section":"talk","summary":"","tags":[],"title":"Bayesian Causal Inference with Stan","type":"talk"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1596741000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605745637,"objectID":"3d8604754d102df78531465171a60078","permalink":"https://stablemarkets.netlify.com/talk/jsm2020/","publishdate":"2020-08-06T15:10:00-04:00","relpermalink":"/talk/jsm2020/","section":"talk","summary":"","tags":[],"title":"Nonparametric Cost-Effectiveness Analyses","type":"talk"},{"authors":null,"categories":null,"content":"Bayesian nonparametrics is a powerful class of methods including Gaussian processes, Bayesian Additive Regression Trees (BART), Gamma Processes, Dirichlet Process, etc. The central idea is to abstract away from parameters. For instance, standard methods may assume a regression function is linear, indexed by finitely many slope/intercept parameters. A prior on these finitely many parameters then induces a prior on the regression function. To avoid specifying such restrictive functional form, we need to cut out the middle-man (the parameters) and specify priors over the function directly. Bayesian nonparametrics is characterized by priors over such abstract objects: priors over regressions, priors over baseline hazard functions, prior distributions over distributions themselves.\nThese methods give you the flexibility of machine learning, with the added benefit uncertainty quantitification via full posterior inference.\nSee below for work related to nonparametric Bayesian inference.\n","date":1594353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"a3571e5251f66a929b20a69d499d2346","permalink":"https://stablemarkets.netlify.com/project/bnp/","publishdate":"2020-07-10T00:00:00-04:00","relpermalink":"/project/bnp/","section":"project","summary":"Bayesian modeling - flexiblilty, uncertainty quantification, full posterior inference.","tags":["bnp"],"title":"Non-parametric Bayes","type":"project"},{"authors":null,"categories":null,"content":"Causal inference is broadly concerned with estimating parameters governing the causal mechanisms between an intervention or treatment of interest and an outcome. Causal inference provides a framework for 1) constructing different estimands that have explicitly causal, rather than associational, interpretations 2) formulating the assumptions under which we can estimate these using observed data, 3) devising sensitivity analyses around violations of these assumptions, and 4) making valid inferences about these estimans. These are just some of the many advances made in the causal literature.\nIn practice, causal inference requires complex, high-dimensional models. Here, the Bayesian paradigm has a lot to offer. For instance,\n Shrinking heterogeneous (stratum-specific) causal effects towards an overall average causal effect for sparse strata. Sparsity priors such as Horseshoes and Spike-and-Slab that can regularize high-dimensional nuissance parameters. Such parameters are common in g-computation. Sensitivity Analyses around causal identification assumptions. Uncertainty about the direction and magnitude of the bias can be expressed via a prior and baked into posterior inference.  See below for work related to Bayesian causal modeling.\n","date":1591329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"a9a59f14fccb40cb8ed638e6ab450059","permalink":"https://stablemarkets.netlify.com/project/causalbayes/","publishdate":"2020-06-05T00:00:00-04:00","relpermalink":"/project/causalbayes/","section":"project","summary":"Shrinkage, partial pooling, nonparametrics, and sensitivity analysis via priors - just some of the value Bayesian modeling can add to causal inference. ","tags":["causalbayes"],"title":"Bayesian Causal Inference","type":"project"},{"authors":null,"categories":["Bayesian","MCMC","R"],"content":"  Previous Posts This is part of a sequence of posts chronicling my journey to manually implement as many MCMC samplers as I can from scratch. Code from previous psots can be found on GitHub. Also I tweet more than I should: StableMarkets.\n The Multimodal posterior I wanted to write up my own implementation of coupled MCMC chains using a tempered posterior along with an animation of the process. This is a classic sampling strategy used to deal with multi-modal posteriors. Here I have a tri-modal target posterior:\n\\[ p(\\theta \\mid D) = \\frac{1}{3}N(-20,1) + \\frac{1}{3}N(0,1) + \\frac{1}{3}N(20,1)\\] The density looks like this\nNotice the regions of flat posterior density at about \\((-15,-5)\\) and \\((5,15)\\)…these are often referred to as “bottlenecks”.\n Problems with standard MH These bottlenecks causes standard MCMC algorithms like Metropolis-Hastings (MH) to get stuck at one of these modes. Suppose at iteration \\(t\\) of a standard MH sampler, the current value of the parameter is \\(\\theta^{(t-1)}= - 5\\). Suppose we use a Gaussian jumping distribution, so that we propose \\(\\theta^{(t)}\\) from \\(\\theta^{(t)} \\sim N( -5, \\sigma)\\). Let’s say that \\(\\sigma=1\\) so the proposal distribution is proportional to the green density below\nIt’s clear here that we’re almost never going to propose draws from the other two modes from this jumping distribution. Vast majority of the proposals to the left will end up in the bottlenecks and get rejected. We could increase \\(\\sigma\\) to make the proposal distribution is wide enough to jump over these bottlenecks. However, we know in MH that increasing \\(\\sigma\\) tends to reduce acceptance probability in general. So maybe that helps us explore the other two modes, but we won’t be accepting frequently - slowing down how efficiently the chain explores the posterior.\n The Tempered Posterior The idea behind tempering is to have two chains: one that is exploring the tempered posterior and another that explores the posterior. Ideally, the tempered posterior won’t have these bottlenecks, so a chain exploring it won’t have trouble getting from mode to mode. Then, we can propose jumps of the chain exploring the posterior to the tempered chain. This increases the chance of our chain of interest jumping to other modes.\nSo when we say “tempered” we mean raising the posterior to some power (temperature) \\(T\\): \\(p(\\theta \\mid D)^T\\). Let’s see what \\(p(\\theta \\mid D)^T\\) looks like (proportional to gray density):\nNotice that the tempered posterior has no bottlenecks. So an MH chain exploring this distribution won’t get stuck in bottlenecks of the posterior. So now we set up two chains: one exploring the tempered posterior and another exploring the posterior - both with standard MH updates. In each iteration, once we’ve update the two chains, we propose a swap between the two chains that is accepted with some probability. We say that the chains “meet” when these swaps occur. That is, we’ve in a sense “coupled” the chains.\n Linking both chains Above is a gif of this playing out over 200 iterations. The gray chain is the standard MH chain (not including the swaps) that explores the tempered distribution. The blue chain is the chain exploring the posterior. The red dots indicate values of the blue chain that are swaps from the tempered chain. I.e. at these red points, the chains meet. Notice that the blue chain now easily hops between the modes by occasionally jumping to the gray chain.\nHow this scales to higher dimensions was a topic of much research - still sort of is. The choice of temperatures is crucial. Often, we need to use several chains, not just two chains as we did here.\nSome references: Altekar (2004) is a nice outline and has references to seminal works by Geyers, Gilks and Roberts, etc. I based my sampler on the math they provide in the paper. Also this post by Darren Wilkinson on MH coupled MCMC is a really nice treatment on this topic as well.\n ","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585086247,"objectID":"dc31a7dab7b65695ebd51686396e0dea","permalink":"https://stablemarkets.netlify.com/post/post4/tempered-mcmc/","publishdate":"2020-03-22T00:00:00Z","relpermalink":"/post/post4/tempered-mcmc/","section":"post","summary":"Previous Posts This is part of a sequence of posts chronicling my journey to manually implement as many MCMC samplers as I can from scratch. Code from previous psots can be found on GitHub. Also I tweet more than I should: StableMarkets.\n The Multimodal posterior I wanted to write up my own implementation of coupled MCMC chains using a tempered posterior along with an animation of the process.","tags":null,"title":"Tempered MCMC for Multimodal Posteriors","type":"post"},{"authors":null,"categories":null,"content":"Below is a collection of some applied work with particularly interesting Bayesian and/or causal modeling. For instance, Hubbard et al. use a Bayesian mixture model to infer patients\u0026rsquo; unknown diabetes status in noisy EHR data. Harrison et al. use a Bayesian hierarchical zero-inflated model to assess difference in costs associated with a cost-lowering healthcare intervention. Takvorian et al. use a difference-in-differences strategy to assess the impact of medicare expansion under ACA on cancer treatment delivery.\n","date":1583211600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"1681684a919254f094a7eefaea0afe2f","permalink":"https://stablemarkets.netlify.com/project/applied/","publishdate":"2020-03-03T00:00:00-05:00","relpermalink":"/project/applied/","section":"project","summary":"Collaborative projects with interesting causal and Bayesian projects.","tags":["applied"],"title":"Applied Causal and Bayesian Modeling","type":"project"},{"authors":["SU Takvorian","**A. Oganisian**","R. Mamtani","N. Mitra","LN Shulman","JE Bekelman","RM Werner"],"categories":null,"content":"","date":1582088400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"fab9c353a453ba78e3826b7ba348f08d","permalink":"https://stablemarkets.netlify.com/publication/acadid/","publishdate":"2020-02-19T00:00:00-05:00","relpermalink":"/publication/acadid/","section":"publication","summary":"","tags":[],"title":"Association of ACA Expansion with Timely Cancer Treatment","type":"publication"},{"authors":["**A. Oganisian**","Jason Roy"],"categories":null,"content":"","date":1581397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605745637,"objectID":"55d6157f4ebf1a0d997228a3d4835e60","permalink":"https://stablemarkets.netlify.com/publication/bayescausal/","publishdate":"2020-02-11T00:00:00-05:00","relpermalink":"/publication/bayescausal/","section":"publication","summary":"","tags":[],"title":"A Practical Introduction to Bayesian Estimation of Causal Effects: Parametric and Nonparametric Approaches","type":"publication"},{"authors":["**A. Oganisian**","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1581397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"1e1b6240aabfa9239a9352c9177e52fc","permalink":"https://stablemarkets.netlify.com/publication/bnpce/","publishdate":"2020-02-11T00:00:00-05:00","relpermalink":"/publication/bnpce/","section":"publication","summary":"","tags":[],"title":"Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation and Adaptive Subgroup Discovery","type":"publication"},{"authors":["JM Harrison","**A. Oganisian**","DT Grande","N. Mitra","Manik Chhabra","KH Chaiyachati"],"categories":null,"content":"","date":1581397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"1a7898100da94e3d22a8b2ec09afc4c7","permalink":"https://stablemarkets.netlify.com/publication/ajmc/","publishdate":"2020-02-11T00:00:00-05:00","relpermalink":"/publication/ajmc/","section":"publication","summary":"","tags":[],"title":"Economic Outcomes of Insurer-Led Care Management for High-Cost Medicaid Patients","type":"publication"},{"authors":["**A. Oganisian**","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1580533200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"6e0ffb78f851063e9879b31a13d0ea87","permalink":"https://stablemarkets.netlify.com/publication/zdp/","publishdate":"2020-02-01T00:00:00-05:00","relpermalink":"/publication/zdp/","section":"publication","summary":"","tags":[],"title":"Bayesian Nonparametric Method for Zero-Inflated Outcomes: Prediction, Clustering, and Causal Inference","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1564599900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"4599b005e30173d606617e39930ff91a","permalink":"https://stablemarkets.netlify.com/talk/jsm2019/","publishdate":"2019-07-31T15:05:00-04:00","relpermalink":"/talk/jsm2019/","section":"talk","summary":"","tags":[],"title":"BNP Model for Zero-Inflated Outcomes: Clustering, Prediction, Causal Inference","type":"talk"},{"authors":null,"categories":["Bayesian","Nonparametrics","MCMC","Survival Analysis","R"],"content":"   Motviation The Gamma Process Prior  Independent Hazards Correlated Hazards    Heads up: equations may not render on blog aggregation sites. See original post here for good formatting. If you like this post, you can follow me on twitter.\nMotviation Suppose we observe survival/event times from some distribution \\[T_{i\\in1:n} \\stackrel{iid}{\\sim} f(t)\\] where \\(f\\) is the density and \\(F(t)=1-S(t)\\) is the corresponding CDF expressed in terms of the survival function \\(S(t)\\). We can represent the hazard function of this distribution in terms of the density, \\[\\lambda(t) = \\frac{f(t)}{S(t)}\\] The hazard, CDF, and survival functions are all related. Thus, if we have a model for the hazard, we also have a model for the survival function and the survival time distribution. The well-known Cox proportional hazard approach models the hazard as a function of covariates \\(x_i \\in \\mathbb{R}^p\\) that multiply some baseline hazard \\(\\lambda_0(t)\\), \\[ \\lambda(t_i) = \\lambda_0(t_i)\\exp(x_i\u0026#39;\\theta)\\] Frequentist estimation of \\(\\theta\\) follows from maximizing the profile likelihood - which avoids the need to specify the baseline hazard \\(\\lambda_0(t)\\). The model is semi-parametric because, while we don’t model the baseline hazard, we require that the multiplicative relationship between covariates and the hazard is correct.\nThis already works fine, so why go Bayesian? Here are just a few (hopefully) compelling reasons:\n We may want to nonparametrically estimate the baseline hazard itself. Posterior inference is exact, so we don’t need to rely on asymptotic uncertainty estimates (though we may want to evaluate the frequentist properties of resulting point and interval estimates). Easy credible interval estimation for any function of the parameters. If we have posterior samples for the hazard, we also get automatic inference for the survival function as well.  Full Bayesian inference requires a proper probability model for both \\(\\theta\\) and \\(\\lambda_0\\). This post walks through a Bayesian approach that places a nonparametric prior on \\(\\lambda_0\\) - specifically the Gamma Process.\n The Gamma Process Prior Independent Hazards Most of this comes from Kalbfleisch (1978), with an excellent technical outline by Ibrahim (2001).\nRecall that the cumulative baseline hazard \\(H_0(t) = \\int_0^t \\lambda_0(t) dt\\) where the integral is the Riemann-Stieltjes integral. The central idea is to develop a prior for the cumulative hazard \\(H_0(t)\\), which will then admit a prior for the hazard, \\(\\lambda_0(t)\\).\nThe Gamma Process is such a prior. Each realization of a Gamma Process is a cumulative hazard function that is centered around some prior cumulative hazard function, \\(H^*\\), with a sort of dispersion/concentration parameter, \\(\\beta\\) that controls how tightly the realizations are distributed around the prior \\(H^*\\).\nOkay, now the math. Let \\(\\mathcal{G}(\\alpha, \\beta)\\) denote the Gamma distribution with shape parameter \\(\\alpha\\) and rate parameter \\(\\beta\\). Let \\(H^*(t)\\) for \\(t\\geq 0\\) be our prior cumulative hazard function. For example we could choose \\(H^*\\) to be the exponential cumulative hazard, \\(H^*(t)= \\eta\\cdot t\\), where \\(\\eta\\) is a fixed hyperparameter. By definition \\(H^*(0)=0\\). The Gamma Process is defined as having the following properties:\n \\(H_0(0) = 0\\) \\(\\lambda_0(t) = H_0(t) - H_0(s) \\sim \\mathcal G \\Big(\\ \\beta\\big(H^*(t) - H^*(s)\\big)\\ , \\ \\beta \\ \\Big)\\), for \\(t\u0026gt;s\\)  The increments in the cumulative hazard is the hazard function. The gamma process has the property that these increments are independent and Gamma-distributed. For a set of time increments \\(t\\geq0\\), we can use the properties above to generate one realization of hazards \\(\\{\\lambda_0(t) \\}_{t\\geq0}\\). Equivaltently, one realization of the cumulative hazard function is \\(\\{H_0(t)\\}_{t\\geq0}\\), where \\(H_0(t) = \\sum_{k=0}^t \\lambda_0(k)\\). We denote the Gamma Process just described as \\[H_0(t) \\sim \\mathcal{GP}\\Big(\\ \\beta H^*(t), \\ \\beta \\Big), \\ \\ t\\geq0\\]\nBelow in Panel A are some prior realizations of \\(H_0(t)\\) with a Weibull \\(H^*\\) prior for various concentration parameters, \\(\\beta\\). Notice for low \\(\\beta\\) the realizations are widely dispersed around the mean cumulative hazard. Higher \\(\\beta\\) yields to tighter dispersion around \\(H^*\\).\nSince there’s a correspondence between the \\(H_0(t)\\), \\(\\lambda_0(t)\\), and \\(S_0(t)\\), we could also plot prior realizations of the baseline survival function \\(S_0(t) = \\exp\\big\\{- H_0(t) \\big\\}\\) using the realization \\(\\{H_0(t)\\}_{t\\geq0}\\). This is shown in Panel B with the Weibull survival function \\(S^*\\) corresponding to \\(H^*\\).\n Correlated Hazards In the previous section, the hazards \\(\\lambda_0(t)\\) between increments were a priori independent - a naive prior belief perhaps. Instead, we might expect that the hazard at the current time point is centered around the hazard in the previous time point. We’d also expect that a higher hazard at the previous time point likely means a higher hazard at the current time point (positive correlation across time).\nNieto‐Barajas et al (2002) came up with a correlated Gamma Process that expresses exactly this prior belief. The basic idea is to introduce a latent stochastic process \\(\\{u_t\\}_{t\\geq0}\\) that links \\(\\lambda_0(t)\\) with \\(\\lambda_0(t-1)\\). Here is the correlated Gamma Process,\n Draw a hazard rate for the first time interval, \\(I_1=[0, t_1)\\), \\(\\lambda_1 \\sim \\mathcal G(\\beta H^*(t_1), \\beta)\\),\n Draw a latent variable \\(u_1 | \\lambda_1 \\sim Pois(c \\cdot \\lambda_1)\\)\n Draw a hazard rate for second time interval \\(I_2 = [t_1, t_2)\\), \\(\\lambda_2 \\sim \\mathcal G(\\beta( H^*(t_2) - H^*(t_1) ) + u_1, \\beta + c )\\)\n In general for \\(k\\geq1\\), define \\(\\alpha_k = \\beta( H^*(t_k) - H^*(t_{k-1}) )\\)\n \\(u_k | \\lambda_k \\sim Pois(c\\cdot \\lambda_k)\\) \\(\\lambda_{k+1} | u_k \\sim \\mathcal G( \\alpha_k + u_k, \\beta + c )\\)   Notice that if \\(c=0\\), then \\(u=0\\) with probability \\(1\\) and the process reduces to the independent Gamma Process in the previous section. Now consider \\(c=1\\). Then, the hazard rate in the next interval has mean \\(E[\\lambda_{k+1} | u_k] = \\frac{ \\alpha_k + u_k }{\\beta+c}\\). Now \\(u_k \\sim Pois(\\lambda_k)\\) is centered around the current \\(\\lambda_k\\) - allowinng \\(\\lambda_k\\) to influence \\(\\lambda_{k+1}\\) through the latent variable \\(u_k\\). The higher the current hazard, \\(\\lambda_k\\), the higher \\(u_k\\), and the higher the mean of the next hazard, \\(\\lambda_{k+1}\\).\nBelow are some realizations of a correlated and independent Gamma processes centered around the \\(Weibull(2,1.5)\\) hazard shown in red. One realization is higlighted in blue to make it easier to see the differences between correlated and independent realizations\nNotice the correlated gamma process looks very snake-y. This is because of the autoregressive structure on \\(\\{\\lambda_0(t)\\}_{t\\geq0}\\) induced by the latent process\\(\\{ u_t\\}_{t\\geq0}\\).\n  ","date":1557619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557711210,"objectID":"67d84bd128a77efde8ec1373bf36b72a","permalink":"https://stablemarkets.netlify.com/post/post3/gamma-process-prior/","publishdate":"2019-05-12T00:00:00Z","relpermalink":"/post/post3/gamma-process-prior/","section":"post","summary":"Motviation The Gamma Process Prior  Independent Hazards Correlated Hazards    Heads up: equations may not render on blog aggregation sites. See original post here for good formatting. If you like this post, you can follow me on twitter.\nMotviation Suppose we observe survival/event times from some distribution \\[T_{i\\in1:n} \\stackrel{iid}{\\sim} f(t)\\] where \\(f\\) is the density and \\(F(t)=1-S(t)\\) is the corresponding CDF expressed in terms of the survival function \\(S(t)\\).","tags":null,"title":"Gamma Process Prior for Semiparametric Survival Analysis","type":"post"},{"authors":null,"categories":["Bayesian","MCMC","Survival Analysis","R","Stan"],"content":"  This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in Stan.\nLet’s start with simulating some randomly censored data from a Weibull model. In this case, we just include a binary indicator and are interested in characterizing survival between these two groups.\nset.seed(1) n \u0026lt;- 1000 # simulate covariates (just a binary treatment indicator) A \u0026lt;- rbinom(n, 1, .5) X \u0026lt;- model.matrix(~ A) # true parameters true_beta \u0026lt;- (1/2)*matrix(c(-1/3, 2), ncol=1) true_mu \u0026lt;- X %*% true_beta true_sigma \u0026lt;- 1 true_alpha \u0026lt;- 1/true_sigma true_lambda \u0026lt;- exp(-1*true_mu*true_alpha) # simulate censoring and survival times survt = rweibull(n, shape=true_alpha, scale = true_lambda) cent = rweibull(n, shape=true_alpha, scale = true_lambda) ## observed data: #censoring indicator delta \u0026lt;- cent \u0026lt; survt survt[delta==1] \u0026lt;- cent[delta==1] # censor survival time. # count number of missing/censored survival times n_miss \u0026lt;- sum(delta) d_list \u0026lt;- list(N_m = n_miss, N_o = n - n_miss, P=2, # number of betas # data for censored subjects y_m=survt[delta==1], X_m=X[delta==1,], # data for uncensored subjects y_o=survt[delta==0], X_o=X[delta==0,]) The list d_list is what we’ll eventually feed to Stan. Below is the Stan model for Weibull distributed survival times. Note in the transformed parameters block we specify the canonical accelerated failure time (AFT) parameterization - modeling the scale as a function of the shape parameter, \\(\\alpha\\), and covariates.\nIn the model block, we specify the likelihood as the Weibull density for uncensored subjects, and then augment the likelihood with evaluations from the Weibull survival function (_lccdf).\nThe generated quantities block transforms the parameters to get posterior draws of the hazard ratio (as specified in my previous post ) as well as posterior draws of the survival function.\ndata { int\u0026lt;lower=0\u0026gt; P; // number of beta parameters // data for censored subjects int\u0026lt;lower=0\u0026gt; N_m; matrix[N_m,P] X_m; vector[N_m] y_m; // data for observed subjects int\u0026lt;lower=0\u0026gt; N_o; matrix[N_o,P] X_o; real y_o[N_o]; } parameters { vector[P] beta; real\u0026lt;lower=0\u0026gt; alpha; // Weibull Shape } transformed parameters{ // model Weibull rate as function of covariates vector[N_m] lambda_m; vector[N_o] lambda_o; // standard weibull AFT re-parameterization lambda_m = exp((X_m*beta)*alpha); lambda_o = exp((X_o*beta)*alpha); } model { beta ~ normal(0, 100); alpha ~ exponential(1); // evaluate likelihood for censored and uncensored subjects target += weibull_lpdf(y_o | alpha, lambda_o); target += weibull_lccdf(y_m | alpha, lambda_m); } // generate posterior quantities of interest generated quantities{ vector[1000] post_pred_trt; vector[1000] post_pred_pbo; real lambda_trt; real lambda_pbo; real hazard_ratio; // generate hazard ratio lambda_trt = exp((beta[1] + beta[2])*alpha ) ; lambda_pbo = exp((beta[1])*alpha ) ; hazard_ratio = exp(beta[2]*alpha ) ; // generate survival times (for plotting survival curves) for(i in 1:1000){ post_pred_trt[i] = weibull_rng(alpha, lambda_trt); post_pred_pbo[i] = weibull_rng(alpha, lambda_pbo); } }  The Stan model specified above is stored in an object called weibull_mod, which is called below in sampling(). The code below samples from the posterior and outputs posterior draws of the hazard and predicted survival times.\nweibull_fit \u0026lt;- sampling(weibull_mod, data = d_list, chains = 1, iter=20000, warmup=19000, save_warmup=F, pars= c(\u0026#39;hazard_ratio\u0026#39;,\u0026#39;post_pred_trt\u0026#39;,\u0026#39;post_pred_pbo\u0026#39;)) ## ## SAMPLING FOR MODEL \u0026#39;80acc0f9293b946800a710dd7f5e211c\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000295 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.95 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 1: Iteration: 4000 / 20000 [ 20%] (Warmup) ## Chain 1: Iteration: 6000 / 20000 [ 30%] (Warmup) ## Chain 1: Iteration: 8000 / 20000 [ 40%] (Warmup) ## Chain 1: Iteration: 10000 / 20000 [ 50%] (Warmup) ## Chain 1: Iteration: 12000 / 20000 [ 60%] (Warmup) ## Chain 1: Iteration: 14000 / 20000 [ 70%] (Warmup) ## Chain 1: Iteration: 16000 / 20000 [ 80%] (Warmup) ## Chain 1: Iteration: 18000 / 20000 [ 90%] (Warmup) ## Chain 1: Iteration: 19001 / 20000 [ 95%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 9.2897 seconds (Warm-up) ## Chain 1: 0.815333 seconds (Sampling) ## Chain 1: 10.105 seconds (Total) ## Chain 1: post_draws\u0026lt;-extract(weibull_fit) Below we plot posterior distribution of the hazard ratio. The red line indicates the true value under which we generated the data.\nhist(post_draws$hazard_ratio, xlab=\u0026#39;Hazard Ratio\u0026#39;, main=\u0026#39;Hazard Ratio Posterior Distribution\u0026#39;) abline(v=exp(-1*true_beta[2,1]*true_alpha), col=\u0026#39;red\u0026#39;) mean(post_draws$hazard_ratio) ## [1] 0.3646075 quantile(post_draws$hazard_ratio, probs = c(.025, .975)) ## 2.5% 97.5% ## 0.3037961 0.4438446 Below we plot the survival functions. Note these results are very similar to the augmented sampler coded in the previous post.\nplot(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), xlab=\u0026#39;Time\u0026#39;,ylab=\u0026#39;Survival Probability\u0026#39;, conf.int=T) for(i in 1:1000){ trt_ecdf \u0026lt;- ecdf(post_draws$post_pred_trt[i,]) curve(1 - trt_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;gray\u0026#39;) pbo_ecdf \u0026lt;- ecdf(post_draws$post_pred_pbo[i,]) curve(1 - pbo_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;lightblue\u0026#39;) } lines(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), add=T, conf.int=T) legend(\u0026#39;topright\u0026#39;, legend = c(\u0026#39;KM Curve and Intervals (TRT)\u0026#39;, \u0026#39;Posterior Survival Draws (TRT)\u0026#39;, \u0026#39;KM Curve and Intervals (PBO)\u0026#39;, \u0026#39;Posterior Survival Draws (PBO)\u0026#39;), col=c(\u0026#39;black\u0026#39;,\u0026#39;gray\u0026#39;,\u0026#39;blue\u0026#39;,\u0026#39;lightblue\u0026#39;), lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty=\u0026#39;n\u0026#39;) ","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605820687,"objectID":"bfde801dcb422036eb2e93e4bb69973f","permalink":"https://stablemarkets.netlify.com/post/post2/specifying-accelerated-failure-time-models-in-stan/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/post/post2/specifying-accelerated-failure-time-models-in-stan/","section":"post","summary":"This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in Stan.\nLet’s start with simulating some randomly censored data from a Weibull model.","tags":null,"title":"Specifying Accelerated Failure Time Models in STAN","type":"post"},{"authors":null,"categories":["Bayesian","Survival Analysis","MCMC","R"],"content":"   Motivation Model Set Up Data Augmentation Metropolis-in-Gibbs Sampler Simulation Example in R   Motivation When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.\nAs with most of my posts, all MCMC is coded from scratch. It helps me and it helps readers understand the underlying algorithm - an intuition that is more difficult to get if you’re just specifying the model in Stan.\n Model Set Up Suppose we observe \\(i=1,\\dots, r\\) survival times, \\(T^o_i\\). Survival times past the end of our study (at time \\(\\tau\\)) are censored for subjects \\(i=r+1, \\dots, n\\). We know that the survival times for these subjects are greater than \\(\\tau\\), but that is all. Say we also have some \\(p\\times 1\\) covariate vector, \\(x_i\\). Finally, we have indicator of whether survival time is observed \\(\\delta_{1:n}\\) for each subject. A parametric approach follows by assuming a model for \\(T\\), we choose the Weibull\n\\[ T^o_i \\sim Weibull(\\alpha, \\lambda_i) \\] Where \\(\\alpha\\) is the shape parameter and \\(\\lambda_i\\) is a subject-specific scale. An Accelerated Failure Time model (AFT) follows from modeling a reparameterization of the scale function \\(\\lambda_i = exp(-\\mu_i\\alpha)\\), where \\(\\mu_i = x_i^T\\beta\\).\nWe’ll consider the setting where we regress on a binary treatment indicator, \\(\\mu_i = \\beta_0 + \\beta_1A\\) where \\(A=1\\) indicates treated and \\(A=0\\) indicates untreated/placebo. This is a funky reparameterization, but it yields intuitive interpretations for \\(\\beta_1\\) in terms of the Weibull’s hazard function, \\(h(t|\\beta,x, \\alpha) = \\lambda_i\\alpha x^{\\alpha-1}\\). Substituting \\(\\lambda_i\\), we see the hazard for treated subjects is \\(h(t|A=1) = e^{-(\\beta_0 + \\beta_1)*\\alpha}\\alpha t^{\\alpha-1}\\) and for untreated subjects it is \\(h(t|A=1) = e^{-(\\beta_0)*\\alpha}\\alpha t^{\\alpha-1}\\). The hazard ratio is,\n\\[HR = \\frac{h(t|A=1) }{h(t|A=0)} = e^{-\\beta_1*\\alpha} \\] If \\(HR=.5\\), then the hazard of death, for example, at time \\(t\\) is \\(50\\%\\) lower in the treated group, relative to the untreated.\nFrom a Bayesian point of view, we are interested in the posterior \\(p(\\beta, \\alpha | T^o_{1:r} , \\delta_{1:n}, \\tau)\\). Once we have this, we can get a whole posterior distribution for the survival function itself - as well as any quantity derived from it. For example, posterior mean and credible intervals for \\(HR\\) (just a function of \\(\\beta_1\\) and \\(\\alpha\\)). We can also get posterior survival curve estimates for each treatment group. For the Weibull, the survival curve is given by \\(S(t|\\beta,\\alpha, A) = exp(-\\lambda t^\\alpha)\\) - again just a function of \\(\\beta_1\\) and \\(\\alpha\\).\n Data Augmentation We’ll first look at the joint data distribution (the likelihood) for this problem. The central idea is to view the survival times for the \\(n-r\\) censored subjects as missing data, \\(T^m_{r+1:n}\\). We refer to the full data as \\(T_{i=1:n} = (T_{i:r}^o, T_{r+1:n}^m)\\). Now we construct a complete-data (augmented) likelihood with these values. The observed likelihood and complete-data likelihood are related by\n\\[ \\begin{aligned} p(T^o_{1:r}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\int p(T_{1:n}, \\delta_{1:n} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\\\ \u0026amp; = \\int p(\\delta_{1:n} | T_{1:n}, \\tau, \\beta, \\alpha) \\ p(T_{1:n} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\end{aligned} \\] Now in this ideal, complete-data setting, we observe patients with either \\(\\delta_i = 1 \\ \\cap \\ T_i \u0026gt; \\tau\\) or with \\(\\delta_i = 0 \\ \\cap \\ T_i \u0026lt; \\tau\\). That is, \\(p(\\delta_{i} | T_i, \\tau, \\beta, \\alpha)=1\\) if either of these conditions hold and \\(0\\) otherwise.\nWe also assume that subjects are independent so that \\(p(T_{i=1:n} | \\tau, \\beta, \\alpha) = p(T^o_{1:r}| \\tau, \\beta, \\alpha)p( T^m_{r+1:n} | \\tau, \\beta, \\alpha)\\). So the likelihood simplifies to: \\[ \\begin{aligned} p(T^o_{1:r}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\prod_{i=1}^n\\int p(\\delta_{i} | T_{i}, \\tau, \\beta, \\alpha) \\ p(T_{i} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int p(\\delta_{i} | T^m_{i}, \\tau, \\beta, \\alpha) \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int I(T_i^m \u0026gt; \\tau) \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int_\\tau^\\infty \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \\end{aligned} \\] The first line follows by independence of observations. The second line follows by separating censored and uncensored subjects. \\(p(\\delta_i | -)=1\\) for all uncensored subjects, but \\(p(\\delta_i | -)=1\\) for censored subjects only when \\(T_i^m \\in (0, \\infty)\\). Otherwise, the integrand is 0. Therefore, in the fourth line we only need to integrate of the region where the integrand is non-zero.\nNow the integral is over the region \\(T_i^m \\in (0, \\infty)\\). But in this region \\(p(\\delta_{i} | T^m_{i}, \\tau, \\beta, \\alpha)=1\\) only when \\(T_i^m \u0026gt;\\tau\\).\nThis is the usual likelihood for frequentist survival models: uncensored subjects contribute to the likelihood via the density while censored subjects contribute to the likelihood via the survival function \\(\\int_\\tau^\\infty \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i}\\). Functions for this integral exist in for most basic distributions in R. For our Weibull model, it is 1-pweibull(). We would simply place priors on \\(\\beta\\) and \\(\\alpha\\), then sample from the posterior using MCMC.\nBut what if this integral was too hard to evaluate (as it may be for more complicated censoring mechanisms) and the complete data likelihood given below is easier?\n\\[ \\begin{aligned} p(T^o_{1:r}, T^m_{r+1:n}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha)\\\\ \\end{aligned} \\] Then we can design a Gibbs sampler around this complete data likelihood.\n Metropolis-in-Gibbs Sampler The target posterior of interest is \\[p(\\beta, \\alpha, T_{r+1:n}^m | T^o_{1:r}, \\delta_{1:n}) = p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n}) \\ p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n})\\] Where each conditional posterior is known up to a proportionality constant. With a joint prior \\(p(\\beta, \\alpha)\\) specified, we have\n\\[ \\begin{aligned} p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n}) \u0026amp; \\propto \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\\\ \u0026amp; \\propto p(\\beta, \\alpha) \\prod_{i=1}^n p(T_{i}| \\tau, \\beta, \\alpha) \\\\ \\end{aligned} \\] Note here that \\(p(T_{i}| \\tau, \\beta, \\alpha)\\) is the assumed Weibull density. We can use a Metropolis step to sample \\((\\beta, \\alpha)\\) from this distribution.\nThe second conditional posterior is \\[\\begin{equation} \\begin{aligned} p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n}) \\propto \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\end{aligned} \\end{equation}\\] This is a truncated Weibull distribution (truncated at the bottom by \\(\\tau\\)). We can also sample from this using a Metropolis step.\nThe Gibbs sampler alternates between sampling from these two conditionals:\nGiven parameters \\((\\beta, \\alpha)\\), impute \\(T^m_i\\) by drawing from \\(p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n})\\), for each \\(i=r+1,\\dots, n\\). Combine these imputed values, \\(T^m_{r+1:n}\\), with observed data \\(T_{1:n}^o\\), and update the parameters \\((\\beta, \\alpha)\\) from \\(p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n})\\).  As the parameter estimates update, the imputations get better. As the imputations get better, the parameter estimates improve. Over time the process yields draws from the joint posterior \\(p(\\beta, \\alpha, T_{r+1:n}^m | T^o_{1:r}, \\delta_{1:n})\\)\nWe retain the sample of \\((\\beta, \\alpha)\\) for inference and toss samples of \\(T^m\\).\n Simulation Example in R All of the code implementing the augmented sampler (from scratch!) can be found on my GitHub. Basically I simulate a data set with a binary treatment indicator for 1,000 subjects with censoring and survival times independently drawn from a Weibull. \\\nFor the \\(\\beta\\) vector, I use independent \\(N(0,sd=100)\\) priors. For the shape parameter, I use an \\(Exp(1)\\) prior. I run a single MCMC chain for 20,000 iterations and toss the first 15,000 out as burn-in.\nHere is the estimated survival function for each treatment group. Overlayed are the non-parametric estimates from a stratified Kaplan-Meier (KM) estimator. Note the parametric model is correctly specified here, so it does just as well as the KM in terms of estimating the mean curve. But the parametric model provides a less noisy fit - notice the credible bands are narrower at later time points when the at-risk counts get low in each treatment arm.\nThat’s just a helpful reminder of the efficiency gains parametric models have over nonparametric ones (when they’re correctly specified. Let’s take a look at the posterior distribution of the hazard ratio. The true value is indicated by the red line.\nWe could have run this thing for longer (and with multiple chains with different starting values). But I think this gets the point across. The posterior mean and \\(95\\%\\) credible interval are \\(.32 \\ (.24-.40)\\). The true value is \\(.367\\). Not too bad. Remember this is only a single simulated dataset.\n ","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554258875,"objectID":"1ed5a14dc1eb43b855339c99bfc17580","permalink":"https://stablemarkets.netlify.com/post/post1/bayesian-survival-analysis-with-data-augmentation/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/post1/bayesian-survival-analysis-with-data-augmentation/","section":"post","summary":"Motivation Model Set Up Data Augmentation Metropolis-in-Gibbs Sampler Simulation Example in R   Motivation When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.","tags":null,"title":"Bayesian Survival Analysis with Data Augmentation","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://stablemarkets.netlify.com/tutorial/example/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Rebecca A. Hubbard","Jing Huang","Joanna Harton","**A. Oganisian**","Grace Choi","Levon Utidjian","Ihuoma Eneli","L. Charles Bailey","Yong Chen"],"categories":null,"content":"","date":1535947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"ada066a1cbb5930c0d855c946c9e9e37","permalink":"https://stablemarkets.netlify.com/publication/bayes_latent_phenotype/","publishdate":"2018-09-03T00:00:00-04:00","relpermalink":"/publication/bayes_latent_phenotype/","section":"publication","summary":"","tags":[],"title":"A Bayesian latent class approach for EHR‐based phenotyping","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1532977500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"4e1d43d1c2ed5b55119cc8b6d77030a5","permalink":"https://stablemarkets.netlify.com/talk/jsm2017/","publishdate":"2018-07-30T15:05:00-04:00","relpermalink":"/talk/jsm2017/","section":"talk","summary":"","tags":[],"title":"A Bayesian Nonparametric Method for Zero-Inflated Data with Applications to Medical Costs","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597368538,"objectID":"8dac710f4894c97d90d24755584b2109","permalink":"https://stablemarkets.netlify.com/project/chirp/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/chirp/","section":"project","summary":"R Package for Dirichlet Process Mixtures of zero-inflated, logistic, and linear regressions.","tags":["bnp"],"title":"ChiRP","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://stablemarkets.netlify.com/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]