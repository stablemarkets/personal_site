[{"authors":null,"categories":null,"content":"News  09/01/2025: New paper accepted in Epidemiology (in press) on causal inference methods for informatively timed, sequential treatments. 07/01/2025: I\u0026rsquo;m honored to be appointed to the Thomas J. \u0026amp; Alice M. Tisch named professorship effective July. 06/01/2025: I\u0026rsquo;m giving two talks at WNAR 2025 in Whistler, Canada: one on this paper and another on this paper. 05/01/2025: Honored to have received the Dean\u0026rsquo;s Award for Excellence in Research Collaboration. 01/12/2025: New paper accepted in Observational Studies (in press) developing causalBETA R package for Bayesian Semiparametric causal inference with survival outcomes. 11/15/2024: New paper published in Biometrics developing semiparametric Bayesian models for causal inference with recurrent event outcomes.   Bio I am the Thomas J. \u0026amp; Alice M. Tisch Assistant Professor of Biostatistics at Brown University. I received my PhD in Biostatistics from the University of Pennsylvania. My methodological research centers around developing Bayesian nonparametric methods for causal estimation, with a focus on analyzing sequential treatments with incomplete information. These methods blend principled causal reasoning, nonparametric Bayesian modeling, and efficient computation to build systems for data-driven decision making.\nMany of my motivating applications are in chronic diseases such as oncology and rheumatology. Some current work is partially funded by a PCORI contract and focuses on developing Bayesian semiparametric methods for estimating (and optimizing) effects of sequential treatment strategies in acute myeloid leukemia. I have recently been awarded another PCORI contract to develop Bayesian nonparametric methods for causal estimation with incomplete covariate information.\nOn this site you will find a link to my CV and a selection of some past and current research, talks, etc. I sometimes blog about statistics, Bayesian methods, computation/MCMC, and other things I happen to stumble upon during research.\nIf you are an ScM or PhD student at Brown University and are considering working with me, please see this page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://stablemarkets.netlify.app/author/arman-oganisian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/arman-oganisian/","section":"authors","summary":"News  09/01/2025: New paper accepted in Epidemiology (in press) on causal inference methods for informatively timed, sequential treatments. 07/01/2025: I\u0026rsquo;m honored to be appointed to the Thomas J. \u0026amp; Alice M.","tags":null,"title":"Arman Oganisian","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Arman Oganisian FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://stablemarkets.netlify.app/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"📊 Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://stablemarkets.netlify.app/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://stablemarkets.netlify.app/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://stablemarkets.netlify.app/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":null,"content":"","date":1743033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743033600,"objectID":"8b7dfe11fbe9eddd2f1b0ae585e22d86","permalink":"https://stablemarkets.netlify.app/project/causalbeta/","publishdate":"2025-03-27T00:00:00Z","relpermalink":"/project/causalbeta/","section":"project","summary":"R Package for Semiparametric Bayesian Causal Inference with Survival/Event-Time outcomes.","tags":["bnp","causalbayes","software"],"title":"causalBETA","type":"project"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1742828700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742828700,"objectID":"89c72be8af781fad6ad20b3c970936ec","permalink":"https://stablemarkets.netlify.app/talk/bayesian-causal-inference-with-recurrent-event-outcomes/","publishdate":"2025-03-24T15:05:00Z","relpermalink":"/talk/bayesian-causal-inference-with-recurrent-event-outcomes/","section":"event","summary":"Talk presented at ENAR 2025.","tags":[],"title":"Bayesian Causal Inference with Recurrent Event Outcomes","type":"event"},{"authors":["A. Oganisian","A. Girard","JA. Steingrimsson","P. Moyo"],"categories":null,"content":"","date":1731628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731628800,"objectID":"e0948b209754826815adfdaf280fd4fe","permalink":"https://stablemarkets.netlify.app/publication/bayesrecurrent/","publishdate":"2024-11-15T00:00:00Z","relpermalink":"/publication/bayesrecurrent/","section":"publication","summary":" ","tags":[],"title":"A Bayesian Framework for Causal Analysis of Recurrent Events with Timing Misalignment","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1719932700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719932700,"objectID":"0aa8974a0c187d3fcfaf667576bf1e40","permalink":"https://stablemarkets.netlify.app/talk/bayesian-counterfactual-prediction-and-optimization-with-incomplete-information/","publishdate":"2024-07-02T15:05:00Z","relpermalink":"/talk/bayesian-counterfactual-prediction-and-optimization-with-incomplete-information/","section":"event","summary":"Talk presented at ISBA 2024.","tags":[],"title":"Bayesian Counterfactual Prediction and Optimization with Incomplete Information","type":"event"},{"authors":["A. Oganisian","K. Getz","T. Alonzo","R. Aplenc","J. Roy"],"categories":null,"content":"","date":1704412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704412800,"objectID":"071f285e3a3e746908523eb98edf0a8f","permalink":"https://stablemarkets.netlify.app/publication/bayesdtraml/","publishdate":"2024-01-05T00:00:00Z","relpermalink":"/publication/bayesdtraml/","section":"publication","summary":" ","tags":[],"title":"Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1690902300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690902300,"objectID":"686211e59482b95bd37c2aa8070b595d","permalink":"https://stablemarkets.netlify.app/talk/bayesian-semiparametric-inference-for-dynamic-treatment-strategies-with-informative-timing/","publishdate":"2023-08-01T15:05:00Z","relpermalink":"/talk/bayesian-semiparametric-inference-for-dynamic-treatment-strategies-with-informative-timing/","section":"event","summary":"Talk presented at EcoStat 2023 Meeting.","tags":[],"title":"Bayesian Semiparametric Inference for Dynamic Treatment Strategies with Informative Timing","type":"event"},{"authors":null,"categories":["Education"],"content":" Advising Working with graduate students is a very fulfilling part of my job. Philosophically, I am not a very hierarchical person and I see students as fellow researchers. We work as part of a team to solve a common problem. The learning/mentoring happens on the journey towards finding these solutions and I sometimes find myself learning from students as well. I also like to foster independence, typically starting off with a lot of guidance and direction and then tapering incrementally over time.\nI typically work with Brown University graduate students in the capacity of a thesis or dissertation adviser for the ScM and PhD programs in Biostatistics. How many students I take on depends on availability of funding and data projects. I generally do not take on more than two new students per year.\nIf you think you may like to work with me on a thesis, I first suggest you read the “Assessing Interest’’ and the ``Finding a Project” sections below and go through some of the papers listed. If you feel like you are enjoying what you are reading and find it stimulating, feel free to email me to set up a meeting to discuss your interests and goals. I also summarize some placement outcomes for past advisees at the end of this post..\n Assessing Interest My current research interests are in causal inference for sequential (aka time-varying) treatment settings. Lately, most of the problems that have come my way deal with survival or, more generally, event-time outcomes. When it comes to methods for estimation or inference on causal effects of treatments on these survival outcomes, I tend to work within the Bayesian paradigm. Thus, working with me typically requires working at the intersection of 1) causal inference, 2) survival analysis, and 3) Bayesian methods. Projects need not involve these - for instance, I also like working on causal inference using frequentist methods when applicable or on other types of outcomes aside from survival. But generally, at least causal inference and Bayesian methods are a common thread.\nIn terms of computing, knowledge in a statistical programming language like R is critical. For Bayesian inference, knowledge of probabilistic programming language such as Stan is also critical. Some more elaborate Bayesian models cannot be fit in Stan and the Markov Chain Monte Carlo (MCMC) methods must be coded from scratch.\nBelow, I have resources for each of these topics to help you learn and gauge your own interest in these topics.\nCausal Inference:\n What If  Chapters 1-3: single treatment setting. Chapters 11-13: Marginal structural models: estimation via IPTW estimation and g-formula. Chapters 19-21: time-varying treatments.   Bayesian Inference:\n Bayesian Data Analysis  Chapters 1, 2, 14, and 11  Primer on MCMC here Bayesian nonparametric data analysis - access free via Brown University library. A good rule of thumb is that you should be able to derive the posterior (up to a proportionality constant) for a logistic regression coefficient vector under, say, a multivariate Gaussian prior. You should also be able code a Metropolis-Hastings sampling for obtaining draws from that posterior. It’s also a good way of gauging whether you like computation.  Bayesian causal inference:\n Practical Introduction to Bayesian estimation of causal effects Bayesian Nonparametrics for Causal Inference and Missing Data Causal Inference: A Missing Data Perspective Bayesian causal inference: a critical review  Survival outcomes:\n Bayesian causal inference with survival outcomes see here: Bayesian Survival Analysis by Ibrahim. Available via Brown University library. See Chapters 1,2, and 3.   Finding a Project Though it can happen many ways, typically a a thesis advising position has arisen in these ways for me:\n A student approaches me and has read some of the literature in the “Assessing Interest” section in detail and concluded they are interested in working in this area. If I have an appropriate project that can be completed within 1-1.5 years ready, I will offer this project as a thesis project for the student. If I do not have a project ready, I will keep the student in mind for when an appropriate project does come my way.\n A student approaches me usually after having done a previous RA-ship working with data. They are very interested in, say, a causal (or Bayesian) extension of the analysis and would like me to supervise. This is an example of a student coming to me with a project, which does sometimes happen and which I like because it can expose me to new and interesting problems.\n  My work is very application-driven. I’m quite bad at (and generally do not enjoy) making up methodological problems to solve and. Instead, I generate research ideas by working with clinical and applied collaborators in, say, the Department of Epidemiology or the medical school on concrete data analysis. Over the course of working on these projects, I often notice complexities for which I find existing methods to be unsatisfactory or have shortcomings. This then motivates new methodological research to overcome these shortcomings. Thesis work often arises from these concrete data analyses projects with collaborators - the arrival of these projects and their appropriateness for thesis work is stochastic in nature. I may not have an appropriate project lined up when you approach me. If I do not, you should keep searching for other projects and not assume one will pop up in the meantime.\n Current Students and Past Placements Below are previous students, degree completed, year of completion, and first industry or academic placement after graduation. Brown archives completed ScM theses here. I recommend prospective advisees read through some of the theses from my former students to get a sense of the type of projects they can work on.\n Current Students  Esteban Fernandes-Morales, PhD 2026. Valentin Kirilenko, ScM 2027. Yuzhou Peng, ScM 2027. Zizhao Xie, ScM 2027.  Previous Students  Zhaoxiang Ding, ScM 2025 - Statistician @ Eli Lilly. Zihan Zhou, ScM 2024 - Biostatistics PhD @ Pennsylvania State University. Tova Ibbotson, ScM 2024 - Senior Statistician @ GSK. Nancy Liu, ScM 2023 - Biostatistician @ Rutgers University. Anthony Girard, ScM 2023 - Statistical Analyst @ Leonard Davis Institute, University of Pennsylvania.    ","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"6ec87b7dac7b2d1ac698af969b82b5e6","permalink":"https://stablemarkets.netlify.app/post/post7/advising/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/post/post7/advising/","section":"post","summary":"Advising Working with graduate students is a very fulfilling part of my job. Philosophically, I am not a very hierarchical person and I see students as fellow researchers. We work as part of a team to solve a common problem.","tags":null,"title":"For prospective Brown advisees","type":"post"},{"authors":["A. Oganisian","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"a3afc80a31f7ac8f2a98a0569746a049","permalink":"https://stablemarkets.netlify.app/publication/hbb/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/publication/hbb/","section":"publication","summary":"A major focus of causal inference is the estimation of heterogeneous average treatment effects (HTE) - average treatment effects within strata of another variable of interest such as levels of a biomarker, education, or age strata. Inference involves estimating a stratum-specific regression and integrating it over the distribution of confounders in that stratum - which itself must be estimated. Standard practice involves estimating these stratum-specific confounder distributions independently (e.g. via the empirical distribution or Rubin's Bayesian bootstrap), which becomes problematic for sparsely populated strata with few observed confounder vectors. In this paper, we develop a nonparametric hierarchical Bayesian bootstrap (HBB) prior over the stratum-specific confounder distributions for HTE estimation. The HBB partially pools the stratum-specific distributions, thereby allowing principled borrowing of confounder information across strata when sparsity is a concern. We show that posterior inference under the HBB can yield efficiency gains over standard marginalization approaches while avoiding strong parametric assumptions about the confounder distribution. We use our approach to estimate the adverse event risk of proton versus photon chemoradiotherapy across various cancer types.","tags":[],"title":"Hierarchical Bayesian Bootstrap for Heterogeneous Treatment Effect Estimation","type":"publication"},{"authors":null,"categories":["Bayesian","causal inference","Nonparametrics","Survival Analysis"],"content":"  This post is a brief summary of highlights from our recent working paper titled “Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing.” It’s part of a series of projects on developing robust Bayesian nonparametric models for sequential decision making in a variety of complex settings. In this case, we deal with a situation in which the decisions are informatively timed - with a motivating application in chemotherapy treatments for pediatric acute myeloid leukemia (AML).\nThis is partially funded by a PCORI grant grant awarded earlier this year (2022).\nSetting and Problem Chemotherapy treatment in AML is not “one and done” but involves making a sequence of treatment decisions over time, with each subsequent treatment decision depending on how the patient has responded to previous treatments and the evolution of their disease process. There are many chemo agents. This work focuses on understanding the effect of anthracyclines (ACT) in particular on survival. ACT is known to be effective in certain cases, but it can also lead to cardiotoxicity and subsequent early death.\nThis presents clinicians with a risk-reward tradeoff - trting too aggressively or too conservatively with ACT may reduce survival. To help inform decisions, an echocardiogram is done to help decide if the heart is healthy enough to tolerate ACT. Clinicians follow various rules of thumb in practice. For instance: withholding ACT if a patient’s current ejection fraction (measured via echocardiogram) falls below some absolute threshold (eg 50%) or if it declines more than some relative (e.g. declines more than 20% from time of enrollment). Briefly, ejection fraction (EF) is the proportion of blood that is pumped out of the heart’s left ventricle during a beat. In healthy individuals, this is fairly high (from 50-75%) but can be lower among patients with cardiotoxicity. These treatment rules are “dynamic” in that ACT is decided based on evolving ejection fraction. This is in contrast to static rules such as “always treat with ACT”, “never treat with ACT” or “alternate ACT” - regardless of EF. The goal is to develop a strategy for estimating survival rates under various hypothetical ACT assignment rules. If we have such a method, we could evaluate the efficacy of the various rules of thumb employed in clinics and arrive at a more data-driven approach.\nLuckily, using data from the Phase III AAML1031 trial, we can estimate effects of such ACT treatment strategies on survival. In the trial, patients move through a sequence of 4 chemo courses. Ahead of each course, an echo is conducted \u0026amp; used to decide ACT inclusion.\n Challenges There are many impediments to valid statistical estimation. 1) ACT is not randomized in the trial but informed by time-varying features such as EF. We need to adjust for such features to get an apples-to-apples comparison of different ACT rules. 2) Some patients die or drop out before ever completing the sequence. In the latter case, we are left with incomplete survival information. 3) Treatment courses are not initiated at pre-fixed times, but depending on when subjects recover from the previous chemotherapy course.\nThis last point suggests that the waiting times between treatments are potential confounders (e.g. slower recovery after previous treatment may inform subsequent treatment and drive survival) - quite a unique type of time-varying confounding which requires modifications to the usual g-computation algorithm for sequential treatments\n A Bayesian Semiparametric Method We model the causal structure using a non-homogenous continuous-time transition process. After each course, patients can transition into a state of subsequent death or transition into a state of subsequent treatment - whichever comes first - in continuous-time. These process is characterized by a pair of transition probabilities: one pair for each treatment course. We use Bayesian semiparametric hazard models to estimate the transition probabilities at each stage as a function of features available at the start of this course.\nCausal effects of certain rules are computed by simulating from the transition process under a specified rule and computing the proportion of simulated subjects who survive past a certain time point (e.g. 3 years, if we want to compute 3-year survival).\n ","date":1670025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670025600,"objectID":"4d766e5353d6b3cefd46995fca5885af","permalink":"https://stablemarkets.netlify.app/post/post6/bayeseq_aml/","publishdate":"2022-12-03T00:00:00Z","relpermalink":"/post/post6/bayeseq_aml/","section":"post","summary":"This post is a brief summary of highlights from our recent working paper titled “Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing.” It’s part of a series of projects on developing robust Bayesian nonparametric models for sequential decision making in a variety of complex settings.","tags":null,"title":"Bayesian Sequential Decision-Making with Informative Timing","type":"post"},{"authors":null,"categories":null,"content":"This is a collection of some applied work with particularly interesting Bayesian and/or causal modeling. For instance, Hubbard et al. use a Bayesian mixture model to infer patients' unknown diabetes status in noisy EHR data. Harrison et al. use a Bayesian hierarchical zero-inflated model to assess difference in costs associated with a cost-lowering healthcare intervention. Takvorian et al. use a difference-in-differences strategy to assess the impact of medicare expansion under ACA on cancer treatment delivery.\n","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"1681684a919254f094a7eefaea0afe2f","permalink":"https://stablemarkets.netlify.app/project/applied/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/project/applied/","section":"project","summary":"Collaborative data analysis projects using Bayesian and/or Causal methods.","tags":["applied"],"title":"Collaborative Causal and Bayesian Modeling","type":"project"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1605712320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605712320,"objectID":"d20b55b99dcc3cd80f676aa5e47395b3","permalink":"https://stablemarkets.netlify.app/talk/introduction-to-nonparametric-bayes/","publishdate":"2020-11-18T15:12:00Z","relpermalink":"/talk/introduction-to-nonparametric-bayes/","section":"event","summary":"","tags":[],"title":"Introduction to Nonparametric Bayes","type":"event"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1601132700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601132700,"objectID":"f902a804b3045436f5640346fd41433f","permalink":"https://stablemarkets.netlify.app/talk/invited-discussion-on-bayesian-causal-forests/","publishdate":"2020-09-26T15:05:00Z","relpermalink":"/talk/invited-discussion-on-bayesian-causal-forests/","section":"event","summary":"Invited Discussion of Bayesian Causal Forests hosted by *Bayesian Analysis*","tags":[],"title":"Invited Discussion on Bayesian Causal Forests","type":"event"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1597331100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597331100,"objectID":"d4a2bcb10675a3b713062eab29d35946","permalink":"https://stablemarkets.netlify.app/talk/bayesian-causal-inference-with-stan/","publishdate":"2020-08-13T15:05:00Z","relpermalink":"/talk/bayesian-causal-inference-with-stan/","section":"event","summary":"","tags":[],"title":"Bayesian Causal Inference with Stan","type":"event"},{"authors":null,"categories":null,"content":"Bayesian nonparametrics is a powerful class of methods including Gaussian processes, Bayesian Additive Regression Trees (BART), Gamma Processes, Dirichlet Process, etc. The central idea is to abstract away from parameters. For instance, standard methods may assume a regression function is linear, indexed by finitely many slope/intercept parameters. A prior on these finitely many parameters then induces a prior on the regression function. To avoid specifying such restrictive functional form, we need to cut out the middle-man (the parameters) and specify priors over the function directly. Bayesian nonparametrics is characterized by priors over such abstract objects: priors over regressions, priors over baseline hazard functions, prior distributions over distributions themselves.\nThese methods give you the flexibility of machine learning, with the added benefit uncertainty quantitification via full posterior inference.\nSee below for work related to nonparametric Bayesian inference.\n","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"a3571e5251f66a929b20a69d499d2346","permalink":"https://stablemarkets.netlify.app/project/bnp/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/project/bnp/","section":"project","summary":"Bayesian modeling - flexiblilty, uncertainty quantification, full posterior inference.","tags":["bnp"],"title":"Non-parametric Bayes","type":"project"},{"authors":null,"categories":null,"content":"Causal inference is broadly concerned with estimating parameters governing the causal mechanisms between an intervention or treatment of interest and an outcome. Causal inference provides a framework for 1) constructing different estimands that have explicitly causal, rather than associational, interpretations 2) formulating the assumptions under which we can estimate these using observed data, 3) devising sensitivity analyses around violations of these assumptions, and 4) making valid inferences about these estimans. These are just some of the many advances made in the causal literature.\nIn practice, causal inference requires complex, high-dimensional models. Here, the Bayesian paradigm has a lot to offer. For instance,\n Shrinking heterogeneous (stratum-specific) causal effects towards an overall average causal effect for sparse strata. Sparsity priors such as Horseshoes and Spike-and-Slab that can regularize high-dimensional nuissance parameters. Such parameters are common in g-computation. Sensitivity Analyses around causal identification assumptions. Uncertainty about the direction and magnitude of the bias can be expressed via a prior and baked into posterior inference.  See below for work related to Bayesian causal modeling.\n","date":1591315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591315200,"objectID":"a9a59f14fccb40cb8ed638e6ab450059","permalink":"https://stablemarkets.netlify.app/project/causalbayes/","publishdate":"2020-06-05T00:00:00Z","relpermalink":"/project/causalbayes/","section":"project","summary":"Shrinkage, partial pooling, nonparametrics, and sensitivity analysis via priors - just some of the value Bayesian modeling can add to causal inference. ","tags":["causalbayes"],"title":"Bayesian Causal Inference","type":"project"},{"authors":null,"categories":["Bayesian","causal inference"],"content":"  Writing up a quick post to clarify a point of common confusion when doing posterior inference for causal effects. All causal inference (regardless of statistical modeling paradigm in consideration) begins with expressing the target estimand in terms of unobservables (e.g. potential outcomes) and linking them to observed data with ``identification assumptions.’’ Two common estimands (which are often confused) are the population level average treatment effect (ATE) and the sample level ATE. These are two very different estimands and inferential procedures therefore differ for each. Yet, they seem to be confused with each other quite often. This post clarifies the distinction.\nSuppose we have a binary treatment \\(A_i\\in\\{0,1\\}\\), outcome \\(Y_i\\), and a set of pre-treatment confounders \\(L_i\\) for \\(n\\) independent subjects. Let the observed data be \\(D = \\{Y_i, A_i, L_i \\}_{i=1}^n\\). Much of the following can be found in Oganisian \u0026amp; Roy, 2020 and Ding and Li, 2018.\nThe population-level ATE The population-level in potential outcome notation can be expressed as \\[ \\Psi = E[Y^1 - Y^0] \\] Under certain identification assumptions, this is identified via the g-formula: \\[ \\Psi(\\mu, P_L) = \\int_{\\mathcal{L}} \\Big( \\mu(1, l)- \\mu(0, l)\\Big) dP_L(l) \\] Where \\(\\mu(a, l) = E[Y\\mid A=a, L=l]\\) is the outcome regression function. Here the notation \\(\\Psi(\\mu, P_L)\\) makes it explicit that the causal estimand is a function of two unknowns: the unknown regression function and the unknown confounder distribution. If we have estimates of these objects, \\(\\hat \\mu\\) and \\(\\hat P_L\\), frequentist inference can be done via a plug-in estimator \\[ \\hat\\Psi(\\hat \\mu, \\hat P_L) = \\int_{\\mathcal{L}} \\Big( \\hat\\mu(1, l)- \\hat\\mu(0, l)\\Big) d\\hat P_L(l) \\] For example, if we could fit a standard GLM with some inverse-link function,\\(g^{-1}\\), \\(\\hat\\mu(a, l) = g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 A + \\hat L\u0026#39;\\beta_2)\\). Typically we use the empirical distribution for the confounder distribution estimate, \\(\\hat P_L(l) = \\sum_{i=1}^n \\frac{1}{n} I(L_i = l)\\). So we have \\[ \\begin{align*} \\hat\\Psi(\\hat \\mu, \\hat P_L) \u0026amp; = \\int_{\\mathcal{L}} \\Big( \\hat\\mu(1, l)- \\hat\\mu(0, l)\\Big) d\\hat P_L(l) \\\\ \u0026amp; = \\sum_{i=1}^n \\frac{1}{n} \\Big( \\hat\\mu(1, L_i)- \\hat\\mu(0, L_i)\\Big) \\\\ \\end{align*} \\] That is, we end up average the difference in the mean function,\\(\\hat\\mu(a, l)\\), under both treatments over the empirical distribution of confounders. Typically bootstrap is used to compute interval estimates for the population ATE.\nBayesian inference for this quantity is also straightforward: obtain the \\(m^{th}\\) posterior draw of the regression function \\(\\mu^{(m)}(a,l)\\) under your favorite Bayesian model (a Generalized Linear Model, Gaussian process, Dirichlet Process, BART, etc). Then obtain the \\(m^{th}\\) posterior draw of the confounder distribution. It is common to use the ``Bayesian Bootstrap’’ for this - a Bayesian analogue of the empirical distribution: \\(P_L^{(m)}(l) = \\sum_{i=1}^n \\gamma_i^{(m)} I(L_i = l)\\), where \\((\\gamma_1^{(m)}, \\gamma_2^{(m)}, \\dots, \\gamma_n^{(m)}) \\sim Dir(1_n)\\) are drawn froma Dirichlet Distribution. Then the posterior draw of the population-level ATE is \\[ \\begin{align*} \\Psi(\\mu^{(m)}, P_L^{(m)}) \u0026amp; = \\int_{\\mathcal{L}} \\Big(\\mu^{(m)}(1, l)-\\mu^{(m)}(0, l)\\Big) d P_L^{(m)}(l) \\\\ \u0026amp; = \\sum_{i=1}^n \\gamma_i\\Big(\\mu^{(m)}(1, L_i)-\\mu^{(m)}(0, L_i)\\Big) \\\\ \\end{align*} \\] Again, we are taking a weighted average of the difference in the mean function draw, \\(\\mu^{(m)}(a, l)\\), under each intervention. Under the Bayesian bootstrap, the posterior expectation of each \\(\\gamma_i\\) is 1/n - so you think can think of this as being centered around the empirical distribution. Doing this for \\(m=1,2,\\dots, M\\) we get a set of posterior draws for the population ATE which can be used for point and interval estimation.\n The sample-level ATE Letting \\(\\textbf{Y}^m = \\{ Y_i^{1-A_i} \\}_{i=1}^n\\) be the set of missing counterfactuals for the patients in the sample. The sample-level ATE is given by \\[ \\psi(\\textbf{Y}^m) = \\frac{1}{n} \\sum_{i=1}^n (Y_i^1 - Y_i^0) \\] This is a very different quantity from the population-level estimand \\[ \\Psi(\\mu, P_L) = \\int_{\\mathcal{L}} \\Big( \\mu(1, l)- \\mu(0, l)\\Big) dP_L(l) \\]\n The uncertainty in the population-level ATE, \\(\\Psi(\\mu, P_L)\\), is due to uncertainty about the unknown regression function, \\(\\mu\\), and confounder distribution, \\(P_L\\). The uncertainty in the sample-level estimand is due to uncertainty about the missing counterfactuals, \\(\\textbf{Y}^m\\). After all, under SUTVA, we only observe \\(Y_i^{A_i}\\) while \\(Y_i^{1-A_i}\\) is unobserved for subject with assignment \\(A_i\\).  The Bayesian solutions differ accordingly:\n To do posterior inference for the population-level ATE, we must draw the unknonwn regression function and confounder distributions from the posterior, \\(f(\\mu, P_L \\mid D)\\) To do posterior inference for the sample-level ATE, we must draw the unknown counterfactuals \\(Y_i^{1-A_i}\\) from the posterior \\(f( \\{ Y_i^{1-A_i} \\}_{i=1}^n \\mid D)\\).  That last bullet is Bayesian inference for causal effects as originally described by Donald Rubin. While \\(\\Psi\\) is identifiable, \\(\\psi\\) is not identifiable except up to a sensitivity parameter. To show this, apply Bayes’ rule: \\[ f( \\{ Y_i^{1-A_i} \\}_{i=1}^n \\mid D) \\propto f_A(A_i \\mid L_i, Y_i^{A_i}, Y_i^{1-A_i} ) f_*(Y^A_i, Y_i^{1-A_i}\\mid A_i, L_i) f_L(L_i) \\] if the usual ignorability holds (due to say a randomized treatment) - i.e. \\(Y^1, Y^0 \\perp A \\mid L\\) - the selection mechanism no longer depends on potential outcomes: \\[f_A(A_i \\mid L_i, Y_i^{A_i}, Y_i^{1-A_i} ) = f_A(A_i \\mid L_i )\\] and, along with \\(f_L\\) can be dropped while maintaining proportionality. But the crucial thing here is that the remaining joint distribution \\(f_*(Y^A_i, Y_i^{1-A_i}\\mid A_i, L_i)\\) is - we never observe both potential outcomes for subject \\(i\\). This complicates sample-level inference. We could still make inferences. For instance, by invoking de Finetti, we could model it as \\(N_2(Y_i^{A_i}, Y_i^{1-A_i}\\mid A_i, L_i; \\eta, \\Sigma ) p(\\eta)p(\\Sigma)\\). Where \\(N_2\\) indicates a bivariate normal distribution with mean vector \\(\\eta\\) and 2x2 covariance matrix, \\(\\Sigma\\). But the off-diagonal terms of \\(\\Sigma\\), $ Cov( Y_i^{A_i}, Y_i^{1-A_i} ) $, cannot be learned from data. Thus we call it a sensitivity parameter. The posterior is still defined, but will be completely driven by the prior \\(p(\\Sigma)\\). Thus, the sample-level effect is significantly more complicated .\n ","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584835200,"objectID":"54d4fe9fc7bdf983341d458b4890e1f6","permalink":"https://stablemarkets.netlify.app/post/post5/sate-v-pate/","publishdate":"2020-03-22T00:00:00Z","relpermalink":"/post/post5/sate-v-pate/","section":"post","summary":"Writing up a quick post to clarify a point of common confusion when doing posterior inference for causal effects. All causal inference (regardless of statistical modeling paradigm in consideration) begins with expressing the target estimand in terms of unobservables (e.","tags":null,"title":"Sample versus Population ATE in Bayesian Caual Estimation","type":"post"},{"authors":["SU Takvorian","A. Oganisian","R. Mamtani","N. Mitra","LN Shulman","JE Bekelman","RM Werner"],"categories":null,"content":"","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582070400,"objectID":"fab9c353a453ba78e3826b7ba348f08d","permalink":"https://stablemarkets.netlify.app/publication/acadid/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/publication/acadid/","section":"publication","summary":"","tags":[],"title":"Association of ACA Expansion with Timely Cancer Treatment","type":"publication"},{"authors":["A. Oganisian","Jason Roy"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"55d6157f4ebf1a0d997228a3d4835e60","permalink":"https://stablemarkets.netlify.app/publication/bayescausal/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/publication/bayescausal/","section":"publication","summary":"Substantial advances in Bayesian methods for causal inference have been made in recent years. We provide an introduction to Bayesian inference for causal effects for practicing statisticians who have some familiarity with Bayesian models and would like an overview of what it can add to causal estimation in practical settings. In the paper, we demonstrate how priors can induce shrinkage and sparsity in parametric models and be used to perform probabilistic sensitivity analyses around causal assumptions. We provide an overview of nonparametric Bayesian estimation and survey their applications in the causal inference literature. Inference in the point-treatment and time-varying treatment settings are considered. For the latter, we explore both static and dynamic treatment regimes. Throughout, we illustrate implementation using off-the-shelf open source software. We hope to leave the reader with implementation-level knowledge of Bayesian causal inference using both parametric and nonparametric models. All synthetic examples and code used in the paper are publicly available on a companion GitHub repository.","tags":[],"title":"A Practical Introduction to Bayesian Estimation of Causal Effects: Parametric and Nonparametric Approaches","type":"publication"},{"authors":["A. Oganisian","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"1e1b6240aabfa9239a9352c9177e52fc","permalink":"https://stablemarkets.netlify.app/publication/bnpce/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/publication/bnpce/","section":"publication","summary":"","tags":[],"title":"Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation and Adaptive Subgroup Discovery","type":"publication"},{"authors":["JM Harrison","A. Oganisian","DT Grande","N. Mitra","Manik Chhabra","KH Chaiyachati"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"1a7898100da94e3d22a8b2ec09afc4c7","permalink":"https://stablemarkets.netlify.app/publication/ajmc/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/publication/ajmc/","section":"publication","summary":"","tags":[],"title":"Economic Outcomes of Insurer-Led Care Management for High-Cost Medicaid Patients","type":"publication"},{"authors":["A. Oganisian","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"6e0ffb78f851063e9879b31a13d0ea87","permalink":"https://stablemarkets.netlify.app/publication/zdp/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/zdp/","section":"publication","summary":"Researchers are often interested in predicting outcomes, detecting distinct subgroups of their data, or estimating causal treatment effects. Pathological data distributions that exhibit skewness and zero-inflation complicate these tasks—requiring highly flexible, data-adaptive modeling. In this paper, we present a multipurpose Bayesian nonparametric model for continuous, zero-inflated outcomes that simultaneously predicts structural zeros, captures skewness, and clusters patients with similar joint data distributions. The flexibility of our approach yields predictions that capture the joint data distribution better than commonly used zero-inflated methods. Moreover, we demonstrate that our model can be coherently incorporated into a standardization procedure for computing causal effect estimates that are robust to such data pathologies. Uncertainty at all levels of this model flow through to the causal effect estimates of interest—allowing easy point estimation, interval estimation, and posterior predictive checks verifying positivity, a required causal identification assumption. Our simulation results show point estimates to have low bias and interval estimates to have close to nominal coverage under complicated data settings. Under simpler settings, these results hold while incurring lower efficiency loss than comparator methods. We use our proposed method to analyze zero-inflated inpatient medical costs among endometrial cancer patients receiving either chemotherapy or radiation therapy in the SEER-Medicare database.","tags":[],"title":"Bayesian Nonparametric Method for Zero-Inflated Outcomes: Prediction, Clustering, and Causal Inference","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1564585500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564585500,"objectID":"def7b396a070a1e0367acef7fa0252b0","permalink":"https://stablemarkets.netlify.app/talk/bnp-model-for-zero-inflated-outcomes-clustering-prediction-causal-inference/","publishdate":"2019-07-31T15:05:00Z","relpermalink":"/talk/bnp-model-for-zero-inflated-outcomes-clustering-prediction-causal-inference/","section":"event","summary":"","tags":[],"title":"BNP Model for Zero-Inflated Outcomes: Clustering, Prediction, Causal Inference","type":"event"},{"authors":null,"categories":["Bayesian","MCMC","Survival Analysis","R","Stan"],"content":"  This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in Stan.\nLet’s start with simulating some randomly censored data from a Weibull model. In this case, we just include a binary indicator and are interested in characterizing survival between these two groups.\nset.seed(1) n \u0026lt;- 1000 # simulate covariates (just a binary treatment indicator) A \u0026lt;- rbinom(n, 1, .5) X \u0026lt;- model.matrix(~ A) # true parameters true_beta \u0026lt;- (1/2)*matrix(c(-1/3, 2), ncol=1) true_mu \u0026lt;- X %*% true_beta true_sigma \u0026lt;- 1 true_alpha \u0026lt;- 1/true_sigma true_lambda \u0026lt;- exp(-1*true_mu*true_alpha) # simulate censoring and survival times survt = rweibull(n, shape=true_alpha, scale = true_lambda) cent = rweibull(n, shape=true_alpha, scale = true_lambda) ## observed data: #censoring indicator delta \u0026lt;- cent \u0026lt; survt survt[delta==1] \u0026lt;- cent[delta==1] # censor survival time. # count number of missing/censored survival times n_miss \u0026lt;- sum(delta) d_list \u0026lt;- list(N_m = n_miss, N_o = n - n_miss, P=2, # number of betas # data for censored subjects y_m=survt[delta==1], X_m=X[delta==1,], # data for uncensored subjects y_o=survt[delta==0], X_o=X[delta==0,]) The list d_list is what we’ll eventually feed to Stan. Below is the Stan model for Weibull distributed survival times. Note in the transformed parameters block we specify the canonical accelerated failure time (AFT) parameterization - modeling the scale as a function of the shape parameter, \\(\\alpha\\), and covariates.\nIn the model block, we specify the likelihood as the Weibull density for uncensored subjects, and then augment the likelihood with evaluations from the Weibull survival function (_lccdf).\nThe generated quantities block transforms the parameters to get posterior draws of the hazard ratio (as specified in my previous post ) as well as posterior draws of the survival function.\ndata { int\u0026lt;lower=0\u0026gt; P; // number of beta parameters // data for censored subjects int\u0026lt;lower=0\u0026gt; N_m; matrix[N_m,P] X_m; vector[N_m] y_m; // data for observed subjects int\u0026lt;lower=0\u0026gt; N_o; matrix[N_o,P] X_o; real y_o[N_o]; } parameters { vector[P] beta; real\u0026lt;lower=0\u0026gt; alpha; // Weibull Shape } transformed parameters{ // model Weibull rate as function of covariates vector[N_m] lambda_m; vector[N_o] lambda_o; // standard weibull AFT re-parameterization lambda_m = exp((X_m*beta)*alpha); lambda_o = exp((X_o*beta)*alpha); } model { beta ~ normal(0, 100); alpha ~ exponential(1); // evaluate likelihood for censored and uncensored subjects target += weibull_lpdf(y_o | alpha, lambda_o); target += weibull_lccdf(y_m | alpha, lambda_m); } // generate posterior quantities of interest generated quantities{ vector[1000] post_pred_trt; vector[1000] post_pred_pbo; real lambda_trt; real lambda_pbo; real hazard_ratio; // generate hazard ratio lambda_trt = exp((beta[1] + beta[2])*alpha ) ; lambda_pbo = exp((beta[1])*alpha ) ; hazard_ratio = exp(beta[2]*alpha ) ; // generate survival times (for plotting survival curves) for(i in 1:1000){ post_pred_trt[i] = weibull_rng(alpha, lambda_trt); post_pred_pbo[i] = weibull_rng(alpha, lambda_pbo); } }  The Stan model specified above is stored in an object called weibull_mod, which is called below in sampling(). The code below samples from the posterior and outputs posterior draws of the hazard and predicted survival times.\nweibull_fit \u0026lt;- sampling(weibull_mod, data = d_list, chains = 1, iter=20000, warmup=19000, save_warmup=F, pars= c(\u0026#39;hazard_ratio\u0026#39;,\u0026#39;post_pred_trt\u0026#39;,\u0026#39;post_pred_pbo\u0026#39;)) ## ## SAMPLING FOR MODEL \u0026#39;80acc0f9293b946800a710dd7f5e211c\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000172 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.72 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 1: Iteration: 4000 / 20000 [ 20%] (Warmup) ## Chain 1: Iteration: 6000 / 20000 [ 30%] (Warmup) ## Chain 1: Iteration: 8000 / 20000 [ 40%] (Warmup) ## Chain 1: Iteration: 10000 / 20000 [ 50%] (Warmup) ## Chain 1: Iteration: 12000 / 20000 [ 60%] (Warmup) ## Chain 1: Iteration: 14000 / 20000 [ 70%] (Warmup) ## Chain 1: Iteration: 16000 / 20000 [ 80%] (Warmup) ## Chain 1: Iteration: 18000 / 20000 [ 90%] (Warmup) ## Chain 1: Iteration: 19001 / 20000 [ 95%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 12.2212 seconds (Warm-up) ## Chain 1: 1.03363 seconds (Sampling) ## Chain 1: 13.2548 seconds (Total) ## Chain 1: post_draws\u0026lt;-extract(weibull_fit) Below we plot posterior distribution of the hazard ratio. The red line indicates the true value under which we generated the data.\nhist(post_draws$hazard_ratio, xlab=\u0026#39;Hazard Ratio\u0026#39;, main=\u0026#39;Hazard Ratio Posterior Distribution\u0026#39;) abline(v=exp(-1*true_beta[2,1]*true_alpha), col=\u0026#39;red\u0026#39;) mean(post_draws$hazard_ratio) ## [1] 0.3658342 quantile(post_draws$hazard_ratio, probs = c(.025, .975)) ## 2.5% 97.5% ## 0.3039049 0.4376196 Below we plot the survival functions. Note these results are very similar to the augmented sampler coded in the previous post.\nplot(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), xlab=\u0026#39;Time\u0026#39;,ylab=\u0026#39;Survival Probability\u0026#39;, conf.int=T) for(i in 1:1000){ trt_ecdf \u0026lt;- ecdf(post_draws$post_pred_trt[i,]) curve(1 - trt_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;gray\u0026#39;) pbo_ecdf \u0026lt;- ecdf(post_draws$post_pred_pbo[i,]) curve(1 - pbo_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;lightblue\u0026#39;) } lines(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), add=T, conf.int=T) legend(\u0026#39;topright\u0026#39;, legend = c(\u0026#39;KM Curve and Intervals (TRT)\u0026#39;, \u0026#39;Posterior Survival Draws (TRT)\u0026#39;, \u0026#39;KM Curve and Intervals (PBO)\u0026#39;, \u0026#39;Posterior Survival Draws (PBO)\u0026#39;), col=c(\u0026#39;black\u0026#39;,\u0026#39;gray\u0026#39;,\u0026#39;blue\u0026#39;,\u0026#39;lightblue\u0026#39;), lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty=\u0026#39;n\u0026#39;) ","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552089600,"objectID":"bfde801dcb422036eb2e93e4bb69973f","permalink":"https://stablemarkets.netlify.app/post/post2/specifying-accelerated-failure-time-models-in-stan/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/post/post2/specifying-accelerated-failure-time-models-in-stan/","section":"post","summary":"This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post.","tags":null,"title":"Specifying Accelerated Failure Time Models in STAN","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://stablemarkets.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Rebecca A. Hubbard","Jing Huang","Joanna Harton","A. Oganisian","Grace Choi","Levon Utidjian","Ihuoma Eneli","L. Charles Bailey","Yong Chen"],"categories":null,"content":"","date":1535932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535932800,"objectID":"ada066a1cbb5930c0d855c946c9e9e37","permalink":"https://stablemarkets.netlify.app/publication/bayes_latent_phenotype/","publishdate":"2018-09-03T00:00:00Z","relpermalink":"/publication/bayes_latent_phenotype/","section":"publication","summary":"","tags":[],"title":"A Bayesian latent class approach for EHR‐based phenotyping","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8dac710f4894c97d90d24755584b2109","permalink":"https://stablemarkets.netlify.app/project/chirp/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/chirp/","section":"project","summary":"R Package for Dirichlet Process Mixtures of zero-inflated, logistic, and linear regressions.","tags":["bnp","software"],"title":"ChiRP","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://stablemarkets.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]