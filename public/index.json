[{"authors":["admin"],"categories":null,"content":"I’m a biostatistics PhD candidate at the University of Pennsylvania, an Associate Fellow at the LDI, and a member of the Center for Causal Inference. My research centers around developing Bayesian nonparametric methods with applications to health economics, prediction, and causal inference problems. Before Penn, I was a Senior Analyst at Analysis Group\u0026rsquo;s health economics and outcomes research (HEOR) group.\nI blog about statistics, bayesian methods, computation/MCMC, visualization, and other things I happen to stumble upon during research.\nMy blog posts are syndicated on R-bloggers.\n","date":1556056531,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1556056531,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I’m a biostatistics PhD candidate at the University of Pennsylvania, an Associate Fellow at the LDI, and a member of the Center for Causal Inference. My research centers around developing Bayesian nonparametric methods with applications to health economics, prediction, and causal inference problems. Before Penn, I was a Senior Analyst at Analysis Group\u0026rsquo;s health economics and outcomes research (HEOR) group.\nI blog about statistics, bayesian methods, computation/MCMC, visualization, and other things I happen to stumble upon during research.","tags":null,"title":"Arman Oganisian","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1551886505,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["Bayesian","MCMC"],"content":" ","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584915003,"objectID":"dc31a7dab7b65695ebd51686396e0dea","permalink":"/post/post4/tempered-mcmc/","publishdate":"2020-03-22T00:00:00Z","relpermalink":"/post/post4/tempered-mcmc/","section":"post","summary":" ","tags":null,"title":"Coupled Chains with Tempered Posterior","type":"post"},{"authors":["**A. Oganisian**","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1581397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583955479,"objectID":"1e1b6240aabfa9239a9352c9177e52fc","permalink":"/publication/bnpce/","publishdate":"2020-02-11T00:00:00-05:00","relpermalink":"/publication/bnpce/","section":"publication","summary":"","tags":[],"title":"Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation and Adaptive Subgroup Discovery","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1564599900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565465627,"objectID":"4599b005e30173d606617e39930ff91a","permalink":"/talk/jsm2019/","publishdate":"2019-07-31T15:05:00-04:00","relpermalink":"/talk/jsm2019/","section":"talk","summary":"","tags":[],"title":"BNP Model for Zero-Inflated Outcomes: Clustering, Prediction, Causal Inference","type":"talk"},{"authors":null,"categories":["Bayesian","Nonparametrics","MCMC","Survival Analysis","R"],"content":"  Motviation The Gamma Process Prior Independent Hazards Correlated Hazards    Heads up: equations may not render on blog aggregation sites. See original post here for good formatting. If you like this post, you can follow me on twitter.\nMotviation Suppose we observe survival/event times from some distribution \\[T_{i\\in1:n} \\stackrel{iid}{\\sim} f(t)\\] where \\(f\\) is the density and \\(F(t)=1-S(t)\\) is the corresponding CDF expressed in terms of the survival function \\(S(t)\\). We can represent the hazard function of this distribution in terms of the density, \\[\\lambda(t) = \\frac{f(t)}{S(t)}\\] The hazard, CDF, and survival functions are all related. Thus, if we have a model for the hazard, we also have a model for the survival function and the survival time distribution. The well-known Cox proportional hazard approach models the hazard as a function of covariates \\(x_i \\in \\mathbb{R}^p\\) that multiply some baseline hazard \\(\\lambda_0(t)\\), \\[ \\lambda(t_i) = \\lambda_0(t_i)\\exp(x_i\u0026#39;\\theta)\\] Frequentist estimation of \\(\\theta\\) follows from maximizing the profile likelihood - which avoids the need to specify the baseline hazard \\(\\lambda_0(t)\\). The model is semi-parametric because, while we don’t model the baseline hazard, we require that the multiplicative relationship between covariates and the hazard is correct.\nThis already works fine, so why go Bayesian? Here are just a few (hopefully) compelling reasons:\n We may want to nonparametrically estimate the baseline hazard itself. Posterior inference is exact, so we don’t need to rely on asymptotic uncertainty estimates (though we may want to evaluate the frequentist properties of resulting point and interval estimates). Easy credible interval estimation for any function of the parameters. If we have posterior samples for the hazard, we also get automatic inference for the survival function as well.  Full Bayesian inference requires a proper probability model for both \\(\\theta\\) and \\(\\lambda_0\\). This post walks through a Bayesian approach that places a nonparametric prior on \\(\\lambda_0\\) - specifically the Gamma Process.\n The Gamma Process Prior Independent Hazards Most of this comes from Kalbfleisch (1978), with an excellent technical outline by Ibrahim (2001).\nRecall that the cumulative baseline hazard \\(H_0(t) = \\int_0^t \\lambda_0(t) dt\\) where the integral is the Riemann-Stieltjes integral. The central idea is to develop a prior for the cumulative hazard \\(H_0(t)\\), which will then admit a prior for the hazard, \\(\\lambda_0(t)\\).\nThe Gamma Process is such a prior. Each realization of a Gamma Process is a cumulative hazard function that is centered around some prior cumulative hazard function, \\(H^*\\), with a sort of dispersion/concentration parameter, \\(\\beta\\) that controls how tightly the realizations are distributed around the prior \\(H^*\\).\nOkay, now the math. Let \\(\\mathcal{G}(\\alpha, \\beta)\\) denote the Gamma distribution with shape parameter \\(\\alpha\\) and rate parameter \\(\\beta\\). Let \\(H^*(t)\\) for \\(t\\geq 0\\) be our prior cumulative hazard function. For example we could choose \\(H^*\\) to be the exponential cumulative hazard, \\(H^*(t)= \\eta\\cdot t\\), where \\(\\eta\\) is a fixed hyperparameter. By definition \\(H^*(0)=0\\). The Gamma Process is defined as having the following properties:\n \\(H_0(0) = 0\\) \\(\\lambda_0(t) = H_0(t) - H_0(s) \\sim \\mathcal G \\Big(\\ \\beta\\big(H^*(t) - H^*(s)\\big)\\ , \\ \\beta \\ \\Big)\\), for \\(t\u0026gt;s\\)  The increments in the cumulative hazard is the hazard function. The gamma process has the property that these increments are independent and Gamma-distributed. For a set of time increments \\(t\\geq0\\), we can use the properties above to generate one realization of hazards \\(\\{\\lambda_0(t) \\}_{t\\geq0}\\). Equivaltently, one realization of the cumulative hazard function is \\(\\{H_0(t)\\}_{t\\geq0}\\), where \\(H_0(t) = \\sum_{k=0}^t \\lambda_0(k)\\). We denote the Gamma Process just described as \\[H_0(t) \\sim \\mathcal{GP}\\Big(\\ \\beta H^*(t), \\ \\beta \\Big), \\ \\ t\\geq0\\]\nBelow in Panel A are some prior realizations of \\(H_0(t)\\) with a Weibull \\(H^*\\) prior for various concentration parameters, \\(\\beta\\). Notice for low \\(\\beta\\) the realizations are widely dispersed around the mean cumulative hazard. Higher \\(\\beta\\) yields to tighter dispersion around \\(H^*\\).\nSince there’s a correspondence between the \\(H_0(t)\\), \\(\\lambda_0(t)\\), and \\(S_0(t)\\), we could also plot prior realizations of the baseline survival function \\(S_0(t) = \\exp\\big\\{- H_0(t) \\big\\}\\) using the realization \\(\\{H_0(t)\\}_{t\\geq0}\\). This is shown in Panel B with the Weibull survival function \\(S^*\\) corresponding to \\(H^*\\).\n Correlated Hazards In the previous section, the hazards \\(\\lambda_0(t)\\) between increments were a priori independent - a naive prior belief perhaps. Instead, we might expect that the hazard at the current time point is centered around the hazard in the previous time point. We’d also expect that a higher hazard at the previous time point likely means a higher hazard at the current time point (positive correlation across time).\nNieto‐Barajas et al (2002) came up with a correlated Gamma Process that expresses exactly this prior belief. The basic idea is to introduce a latent stochastic process \\(\\{u_t\\}_{t\\geq0}\\) that links \\(\\lambda_0(t)\\) with \\(\\lambda_0(t-1)\\). Here is the correlated Gamma Process,\n Draw a hazard rate for the first time interval, \\(I_1=[0, t_1)\\), \\(\\lambda_1 \\sim \\mathcal G(\\beta H^*(t_1), \\beta)\\), Draw a latent variable \\(u_1 | \\lambda_1 \\sim Pois(c \\cdot \\lambda_1)\\) Draw a hazard rate for second time interval \\(I_2 = [t_1, t_2)\\), \\(\\lambda_2 \\sim \\mathcal G(\\beta( H^*(t_2) - H^*(t_1) ) + u_1, \\beta + c )\\) In general for \\(k\\geq1\\), define \\(\\alpha_k = \\beta( H^*(t_k) - H^*(t_{k-1}) )\\)\n \\(u_k | \\lambda_k \\sim Pois(c\\cdot \\lambda_k)\\) \\(\\lambda_{k+1} | u_k \\sim \\mathcal G( \\alpha_k + u_k, \\beta + c )\\)   Notice that if \\(c=0\\), then \\(u=0\\) with probability \\(1\\) and the process reduces to the independent Gamma Process in the previous section. Now consider \\(c=1\\). Then, the hazard rate in the next interval has mean \\(E[\\lambda_{k+1} | u_k] = \\frac{ \\alpha_k + u_k }{\\beta+c}\\). Now \\(u_k \\sim Pois(\\lambda_k)\\) is centered around the current \\(\\lambda_k\\) - allowinng \\(\\lambda_k\\) to influence \\(\\lambda_{k+1}\\) through the latent variable \\(u_k\\). The higher the current hazard, \\(\\lambda_k\\), the higher \\(u_k\\), and the higher the mean of the next hazard, \\(\\lambda_{k+1}\\).\nBelow are some realizations of a correlated and independent Gamma processes centered around the \\(Weibull(2,1.5)\\) hazard shown in red. One realization is higlighted in blue to make it easier to see the differences between correlated and independent realizations\nNotice the correlated gamma process looks very snake-y. This is because of the autoregressive structure on \\(\\{\\lambda_0(t)\\}_{t\\geq0}\\) induced by the latent process\\(\\{ u_t\\}_{t\\geq0}\\).\n  ","date":1557619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557711210,"objectID":"67d84bd128a77efde8ec1373bf36b72a","permalink":"/post/post3/gamma-process-prior/","publishdate":"2019-05-12T00:00:00Z","relpermalink":"/post/post3/gamma-process-prior/","section":"post","summary":"Motviation The Gamma Process Prior Independent Hazards Correlated Hazards    Heads up: equations may not render on blog aggregation sites. See original post here for good formatting. If you like this post, you can follow me on twitter.\nMotviation Suppose we observe survival/event times from some distribution \\[T_{i\\in1:n} \\stackrel{iid}{\\sim} f(t)\\] where \\(f\\) is the density and \\(F(t)=1-S(t)\\) is the corresponding CDF expressed in terms of the survival function \\(S(t)\\).","tags":null,"title":"Gamma Process Prior for Semiparametric Survival Analysis","type":"post"},{"authors":null,"categories":["Bayesian","MCMC","Survival Analysis","R","Stan"],"content":" This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in Stan.\nLet’s start with simulating some randomly censored data from a Weibull model. In this case, we just include a binary indicator and are interested in characterizing survival between these two groups.\nset.seed(1) n \u0026lt;- 1000 # simulate covariates (just a binary treatment indicator) A \u0026lt;- rbinom(n, 1, .5) X \u0026lt;- model.matrix(~ A) # true parameters true_beta \u0026lt;- (1/2)*matrix(c(-1/3, 2), ncol=1) true_mu \u0026lt;- X %*% true_beta true_sigma \u0026lt;- 1 true_alpha \u0026lt;- 1/true_sigma true_lambda \u0026lt;- exp(-1*true_mu*true_alpha) # simulate censoring and survival times survt = rweibull(n, shape=true_alpha, scale = true_lambda) cent = rweibull(n, shape=true_alpha, scale = true_lambda) ## observed data: #censoring indicator delta \u0026lt;- cent \u0026lt; survt survt[delta==1] \u0026lt;- cent[delta==1] # censor survival time. # count number of missing/censored survival times n_miss \u0026lt;- sum(delta) d_list \u0026lt;- list(N_m = n_miss, N_o = n - n_miss, P=2, # number of betas # data for censored subjects y_m=survt[delta==1], X_m=X[delta==1,], # data for uncensored subjects y_o=survt[delta==0], X_o=X[delta==0,]) The list d_list is what we’ll eventually feed to Stan. Below is the Stan model for Weibull distributed survival times. Note in the transformed parameters block we specify the canonical accelerated failure time (AFT) parameterization - modeling the scale as a function of the shape parameter, \\(\\alpha\\), and covariates.\nIn the model block, we specify the likelihood as the Weibull density for uncensored subjects, and then augment the likelihood with evaluations from the Weibull survival function (_lccdf).\nThe generated quantities block transforms the parameters to get posterior draws of the hazard ratio (as specified in my previous post ) as well as posterior draws of the survival function.\ndata { int\u0026lt;lower=0\u0026gt; P; // number of beta parameters // data for censored subjects int\u0026lt;lower=0\u0026gt; N_m; matrix[N_m,P] X_m; vector[N_m] y_m; // data for observed subjects int\u0026lt;lower=0\u0026gt; N_o; matrix[N_o,P] X_o; real y_o[N_o]; } parameters { vector[P] beta; real\u0026lt;lower=0\u0026gt; alpha; // Weibull Shape } transformed parameters{ // model Weibull rate as function of covariates vector[N_m] lambda_m; vector[N_o] lambda_o; // standard weibull AFT re-parameterization lambda_m = exp((X_m*beta)*alpha); lambda_o = exp((X_o*beta)*alpha); } model { beta ~ normal(0, 100); alpha ~ exponential(1); // evaluate likelihood for censored and uncensored subjects target += weibull_lpdf(y_o | alpha, lambda_o); target += weibull_lccdf(y_m | alpha, lambda_m); } // generate posterior quantities of interest generated quantities{ vector[1000] post_pred_trt; vector[1000] post_pred_pbo; real lambda_trt; real lambda_pbo; real hazard_ratio; // generate hazard ratio lambda_trt = exp((beta[1] + beta[2])*alpha ) ; lambda_pbo = exp((beta[1])*alpha ) ; hazard_ratio = exp(beta[2]*alpha ) ; // generate survival times (for plotting survival curves) for(i in 1:1000){ post_pred_trt[i] = weibull_rng(alpha, lambda_trt); post_pred_pbo[i] = weibull_rng(alpha, lambda_pbo); } }  The Stan model specified above is stored in an object called weibull_mod, which is called below in sampling(). The code below samples from the posterior and outputs posterior draws of the hazard and predicted survival times.\nweibull_fit \u0026lt;- sampling(weibull_mod, data = d_list, chains = 1, iter=20000, warmup=19000, save_warmup=F, pars= c(\u0026#39;hazard_ratio\u0026#39;,\u0026#39;post_pred_trt\u0026#39;,\u0026#39;post_pred_pbo\u0026#39;)) ## ## SAMPLING FOR MODEL \u0026#39;80acc0f9293b946800a710dd7f5e211c\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000148 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.48 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 1: Iteration: 4000 / 20000 [ 20%] (Warmup) ## Chain 1: Iteration: 6000 / 20000 [ 30%] (Warmup) ## Chain 1: Iteration: 8000 / 20000 [ 40%] (Warmup) ## Chain 1: Iteration: 10000 / 20000 [ 50%] (Warmup) ## Chain 1: Iteration: 12000 / 20000 [ 60%] (Warmup) ## Chain 1: Iteration: 14000 / 20000 [ 70%] (Warmup) ## Chain 1: Iteration: 16000 / 20000 [ 80%] (Warmup) ## Chain 1: Iteration: 18000 / 20000 [ 90%] (Warmup) ## Chain 1: Iteration: 19001 / 20000 [ 95%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 9.23524 seconds (Warm-up) ## Chain 1: 0.808913 seconds (Sampling) ## Chain 1: 10.0441 seconds (Total) ## Chain 1: post_draws\u0026lt;-extract(weibull_fit) Below we plot posterior distribution of the hazard ratio. The red line indicates the true value under which we generated the data.\nhist(post_draws$hazard_ratio, xlab=\u0026#39;Hazard Ratio\u0026#39;, main=\u0026#39;Hazard Ratio Posterior Distribution\u0026#39;) abline(v=exp(-1*true_beta[2,1]*true_alpha), col=\u0026#39;red\u0026#39;) mean(post_draws$hazard_ratio) ## [1] 0.3646075 quantile(post_draws$hazard_ratio, probs = c(.025, .975)) ## 2.5% 97.5% ## 0.3037961 0.4438446 Below we plot the survival functions. Note these results are very similar to the augmented sampler coded in the previous post.\nplot(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), xlab=\u0026#39;Time\u0026#39;,ylab=\u0026#39;Survival Probability\u0026#39;, conf.int=T) for(i in 1:1000){ trt_ecdf \u0026lt;- ecdf(post_draws$post_pred_trt[i,]) curve(1 - trt_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;gray\u0026#39;) pbo_ecdf \u0026lt;- ecdf(post_draws$post_pred_pbo[i,]) curve(1 - pbo_ecdf(x), from = 0, to=4, add=T, col=\u0026#39;lightblue\u0026#39;) } lines(survfit(Surv(survt, 1-delta) ~ A ), col=c(\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;), add=T, conf.int=T) legend(\u0026#39;topright\u0026#39;, legend = c(\u0026#39;KM Curve and Intervals (TRT)\u0026#39;, \u0026#39;Posterior Survival Draws (TRT)\u0026#39;, \u0026#39;KM Curve and Intervals (PBO)\u0026#39;, \u0026#39;Posterior Survival Draws (PBO)\u0026#39;), col=c(\u0026#39;black\u0026#39;,\u0026#39;gray\u0026#39;,\u0026#39;blue\u0026#39;,\u0026#39;lightblue\u0026#39;), lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty=\u0026#39;n\u0026#39;) ","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584915003,"objectID":"bfde801dcb422036eb2e93e4bb69973f","permalink":"/post/post2/specifying-accelerated-failure-time-models-in-stan/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/post/post2/specifying-accelerated-failure-time-models-in-stan/","section":"post","summary":"This post is an add-on to my previous post about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in Stan.\nLet’s start with simulating some randomly censored data from a Weibull model.","tags":null,"title":"Specifying Accelerated Failure Time Models in STAN","type":"post"},{"authors":null,"categories":["Bayesian","Survival Analysis","MCMC","R"],"content":"  Motivation Model Set Up Data Augmentation Metropolis-in-Gibbs Sampler Simulation Example in R   Motivation When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.\nAs with most of my posts, all MCMC is coded from scratch. It helps me and it helps readers understand the underlying algorithm - an intuition that is more difficult to get if you’re just specifying the model in Stan.\n Model Set Up Suppose we observe \\(i=1,\\dots, r\\) survival times, \\(T^o_i\\). Survival times past the end of our study (at time \\(\\tau\\)) are censored for subjects \\(i=r+1, \\dots, n\\). We know that the survival times for these subjects are greater than \\(\\tau\\), but that is all. Say we also have some \\(p\\times 1\\) covariate vector, \\(x_i\\). Finally, we have indicator of whether survival time is observed \\(\\delta_{1:n}\\) for each subject. A parametric approach follows by assuming a model for \\(T\\), we choose the Weibull\n\\[ T^o_i \\sim Weibull(\\alpha, \\lambda_i) \\] Where \\(\\alpha\\) is the shape parameter and \\(\\lambda_i\\) is a subject-specific scale. An Accelerated Failure Time model (AFT) follows from modeling a reparameterization of the scale function \\(\\lambda_i = exp(-\\mu_i\\alpha)\\), where \\(\\mu_i = x_i^T\\beta\\).\nWe’ll consider the setting where we regress on a binary treatment indicator, \\(\\mu_i = \\beta_0 + \\beta_1A\\) where \\(A=1\\) indicates treated and \\(A=0\\) indicates untreated/placebo. This is a funky reparameterization, but it yields intuitive interpretations for \\(\\beta_1\\) in terms of the Weibull’s hazard function, \\(h(t|\\beta,x, \\alpha) = \\lambda_i\\alpha x^{\\alpha-1}\\). Substituting \\(\\lambda_i\\), we see the hazard for treated subjects is \\(h(t|A=1) = e^{-(\\beta_0 + \\beta_1)*\\alpha}\\alpha t^{\\alpha-1}\\) and for untreated subjects it is \\(h(t|A=1) = e^{-(\\beta_0)*\\alpha}\\alpha t^{\\alpha-1}\\). The hazard ratio is,\n\\[HR = \\frac{h(t|A=1) }{h(t|A=0)} = e^{-\\beta_1*\\alpha} \\] If \\(HR=.5\\), then the hazard of death, for example, at time \\(t\\) is \\(50\\%\\) lower in the treated group, relative to the untreated.\nFrom a Bayesian point of view, we are interested in the posterior \\(p(\\beta, \\alpha | T^o_{1:r} , \\delta_{1:n}, \\tau)\\). Once we have this, we can get a whole posterior distribution for the survival function itself - as well as any quantity derived from it. For example, posterior mean and credible intervals for \\(HR\\) (just a function of \\(\\beta_1\\) and \\(\\alpha\\)). We can also get posterior survival curve estimates for each treatment group. For the Weibull, the survival curve is given by \\(S(t|\\beta,\\alpha, A) = exp(-\\lambda t^\\alpha)\\) - again just a function of \\(\\beta_1\\) and \\(\\alpha\\).\n Data Augmentation We’ll first look at the joint data distribution (the likelihood) for this problem. The central idea is to view the survival times for the \\(n-r\\) censored subjects as missing data, \\(T^m_{r+1:n}\\). We refer to the full data as \\(T_{i=1:n} = (T_{i:r}^o, T_{r+1:n}^m)\\). Now we construct a complete-data (augmented) likelihood with these values. The observed likelihood and complete-data likelihood are related by\n\\[ \\begin{aligned} p(T^o_{1:r}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\int p(T_{1:n}, \\delta_{1:n} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\\\ \u0026amp; = \\int p(\\delta_{1:n} | T_{1:n}, \\tau, \\beta, \\alpha) \\ p(T_{1:n} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\end{aligned} \\] Now in this ideal, complete-data setting, we observe patients with either \\(\\delta_i = 1 \\ \\cap \\ T_i \u0026gt; \\tau\\) or with \\(\\delta_i = 0 \\ \\cap \\ T_i \u0026lt; \\tau\\). That is, \\(p(\\delta_{i} | T_i, \\tau, \\beta, \\alpha)=1\\) if either of these conditions hold and \\(0\\) otherwise.\nWe also assume that subjects are independent so that \\(p(T_{i=1:n} | \\tau, \\beta, \\alpha) = p(T^o_{1:r}| \\tau, \\beta, \\alpha)p( T^m_{r+1:n} | \\tau, \\beta, \\alpha)\\). So the likelihood simplifies to: \\[ \\begin{aligned} p(T^o_{1:r}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\prod_{i=1}^n\\int p(\\delta_{i} | T_{i}, \\tau, \\beta, \\alpha) \\ p(T_{i} | \\tau, \\beta, \\alpha) \\ dT^m_{r+1:n} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int p(\\delta_{i} | T^m_{i}, \\tau, \\beta, \\alpha) \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int I(T_i^m \u0026gt; \\tau) \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} \\int_\\tau^\\infty \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i} \\\\ \\end{aligned} \\] The first line follows by independence of observations. The second line follows by separating censored and uncensored subjects. \\(p(\\delta_i | -)=1\\) for all uncensored subjects, but \\(p(\\delta_i | -)=1\\) for censored subjects only when \\(T_i^m \\in (0, \\infty)\\). Otherwise, the integrand is 0. Therefore, in the fourth line we only need to integrate of the region where the integrand is non-zero.\nNow the integral is over the region \\(T_i^m \\in (0, \\infty)\\). But in this region \\(p(\\delta_{i} | T^m_{i}, \\tau, \\beta, \\alpha)=1\\) only when \\(T_i^m \u0026gt;\\tau\\).\nThis is the usual likelihood for frequentist survival models: uncensored subjects contribute to the likelihood via the density while censored subjects contribute to the likelihood via the survival function \\(\\int_\\tau^\\infty \\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\ dT^m_{i}\\). Functions for this integral exist in for most basic distributions in R. For our Weibull model, it is 1-pweibull(). We would simply place priors on \\(\\beta\\) and \\(\\alpha\\), then sample from the posterior using MCMC.\nBut what if this integral was too hard to evaluate (as it may be for more complicated censoring mechanisms) and the complete data likelihood given below is easier?\n\\[ \\begin{aligned} p(T^o_{1:r}, T^m_{r+1:n}, \\delta_{1:n}| \\tau, \\beta, \\alpha) \u0026amp; = \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha)\\\\ \\end{aligned} \\] Then we can design a Gibbs sampler around this complete data likelihood.\n Metropolis-in-Gibbs Sampler The target posterior of interest is \\[p(\\beta, \\alpha, T_{r+1:n}^m | T^o_{1:r}, \\delta_{1:n}) = p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n}) \\ p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n})\\] Where each conditional posterior is known up to a proportionality constant. With a joint prior \\(p(\\beta, \\alpha)\\) specified, we have\n\\[ \\begin{aligned} p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n}) \u0026amp; \\propto \\prod_{i| \\delta_i=0} p(T_{i}^o | \\tau, \\beta, \\alpha) \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\\\ \u0026amp; \\propto p(\\beta, \\alpha) \\prod_{i=1}^n p(T_{i}| \\tau, \\beta, \\alpha) \\\\ \\end{aligned} \\] Note here that \\(p(T_{i}| \\tau, \\beta, \\alpha)\\) is the assumed Weibull density. We can use a Metropolis step to sample \\((\\beta, \\alpha)\\) from this distribution.\nThe second conditional posterior is \\[\\begin{equation} \\begin{aligned} p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n}) \\propto \\prod_{i| \\delta_i=1} I(T_i^m \u0026gt; \\tau)\\ p(T_{i}^m | \\tau, \\beta, \\alpha) \\end{aligned} \\end{equation}\\] This is a truncated Weibull distribution (truncated at the bottom by \\(\\tau\\)). We can also sample from this using a Metropolis step.\nThe Gibbs sampler alternates between sampling from these two conditionals:\nGiven parameters \\((\\beta, \\alpha)\\), impute \\(T^m_i\\) by drawing from \\(p(T_{r+1:n}^m | \\beta, \\alpha, T^o_{1:r}, \\delta_{1:n})\\), for each \\(i=r+1,\\dots, n\\). Combine these imputed values, \\(T^m_{r+1:n}\\), with observed data \\(T_{1:n}^o\\), and update the parameters \\((\\beta, \\alpha)\\) from \\(p(\\beta, \\alpha | T_{r+1:n}^m, T^o_{1:r}, \\delta_{1:n})\\).  As the parameter estimates update, the imputations get better. As the imputations get better, the parameter estimates improve. Over time the process yields draws from the joint posterior \\(p(\\beta, \\alpha, T_{r+1:n}^m | T^o_{1:r}, \\delta_{1:n})\\)\nWe retain the sample of \\((\\beta, \\alpha)\\) for inference and toss samples of \\(T^m\\).\n Simulation Example in R All of the code implementing the augmented sampler (from scratch!) can be found on my GitHub. Basically I simulate a data set with a binary treatment indicator for 1,000 subjects with censoring and survival times independently drawn from a Weibull. \\\nFor the \\(\\beta\\) vector, I use independent \\(N(0,sd=100)\\) priors. For the shape parameter, I use an \\(Exp(1)\\) prior. I run a single MCMC chain for 20,000 iterations and toss the first 15,000 out as burn-in.\nHere is the estimated survival function for each treatment group. Overlayed are the non-parametric estimates from a stratified Kaplan-Meier (KM) estimator. Note the parametric model is correctly specified here, so it does just as well as the KM in terms of estimating the mean curve. But the parametric model provides a less noisy fit - notice the credible bands are narrower at later time points when the at-risk counts get low in each treatment arm.\nThat’s just a helpful reminder of the efficiency gains parametric models have over nonparametric ones (when they’re correctly specified. Let’s take a look at the posterior distribution of the hazard ratio. The true value is indicated by the red line.\nWe could have run this thing for longer (and with multiple chains with different starting values). But I think this gets the point across. The posterior mean and \\(95\\%\\) credible interval are \\(.32 \\ (.24-.40)\\). The true value is \\(.367\\). Not too bad. Remember this is only a single simulated dataset.\n ","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554258875,"objectID":"1ed5a14dc1eb43b855339c99bfc17580","permalink":"/post/post1/bayesian-survival-analysis-with-data-augmentation/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/post1/bayesian-survival-analysis-with-data-augmentation/","section":"post","summary":"Motivation Model Set Up Data Augmentation Metropolis-in-Gibbs Sampler Simulation Example in R   Motivation When dealing with time-to-event data, right-censoring is a common occurance. Although most are familiar with likelihood construction under right-censoring (and corresponding frequentist estimation), there’s very little available online about Bayesian approaches even for fully parametric models. Here I’ll briefly outline a Bayesian estimation procedure for a Weibull model with right-censoring. The estimation procedure is MCMC based using a data augmentation approach.","tags":null,"title":"Bayesian Survival Analysis with Data Augmentation","type":"post"},{"authors":["**A. Oganisian**","Nandita Mitra","Jason Roy"],"categories":null,"content":"","date":1548997200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552106655,"objectID":"6e0ffb78f851063e9879b31a13d0ea87","permalink":"/publication/zdp/","publishdate":"2019-02-01T00:00:00-05:00","relpermalink":"/publication/zdp/","section":"publication","summary":"","tags":[],"title":"Bayesian Nonparametric Method for Zero-Inflated Outcomes: Prediction, Clustering, and Causal Inference","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Rebecca A. Hubbard","Jing Huang","Joanna Harton","**A. Oganisian**","Grace Choi","Levon Utidjian","Ihuoma Eneli","L. Charles Bailey","Yong Chen"],"categories":null,"content":"","date":1535947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552106655,"objectID":"ada066a1cbb5930c0d855c946c9e9e37","permalink":"/publication/bayes_latent_phenotype/","publishdate":"2018-09-03T00:00:00-04:00","relpermalink":"/publication/bayes_latent_phenotype/","section":"publication","summary":"","tags":[],"title":"A Bayesian latent class approach for EHR‐based phenotyping","type":"publication"},{"authors":["Arman Oganisian"],"categories":null,"content":"","date":1532977500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565466237,"objectID":"4e1d43d1c2ed5b55119cc8b6d77030a5","permalink":"/talk/jsm2017/","publishdate":"2018-07-30T15:05:00-04:00","relpermalink":"/talk/jsm2017/","section":"talk","summary":"","tags":[],"title":"A Bayesian Nonparametric Method for Zero-Inflated Data with Applications to Medical Costs","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551886505,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]