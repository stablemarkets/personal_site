<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Survival Analysis | </title>
    <link>https://stablemarkets.netlify.app/category/survival-analysis/</link>
      <atom:link href="https://stablemarkets.netlify.app/category/survival-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Survival Analysis</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 03 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://stablemarkets.netlify.app/media/icon_hu56437e0029043a6980b2632a303bfe7c_22443_512x512_fill_lanczos_center_3.png</url>
      <title>Survival Analysis</title>
      <link>https://stablemarkets.netlify.app/category/survival-analysis/</link>
    </image>
    
    <item>
      <title>Bayesian Sequential Decision-Making with Informative Timing</title>
      <link>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is a brief summary of highlights from our &lt;a href=&#34;https://arxiv.org/abs/2211.16393&#34;&gt;recent working paper&lt;/a&gt; titled “Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing.” It’s part of a series of projects on developing robust Bayesian nonparametric models for sequential decision making in a variety of complex settings. In this case, we deal with a situation in which the decisions are informatively timed - with a motivating application in chemotherapy treatments for pediatric acute myeloid leukemia (AML).&lt;/p&gt;
&lt;p&gt;This is partially funded by a &lt;a href=&#34;https://www.pcori.org/research-results/2022/statistical-methods-optimizing-dynamic-patient-level-treatment-and-monitoring-strategies&#34;&gt;PCORI grant&lt;/a&gt; grant awarded earlier this year (2022).&lt;/p&gt;
&lt;div id=&#34;setting-and-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting and Problem&lt;/h1&gt;
&lt;p&gt;Chemotherapy treatment in AML is not “one and done” but involves making a sequence of treatment decisions over time, with each subsequent treatment decision depending on how the patient has responded to previous treatments and the evolution of their disease process. There are many chemo agents. This work focuses on understanding the effect of anthracyclines (ACT) in particular on survival. ACT is known to be effective in certain cases, but it can also lead to cardiotoxicity and subsequent early death.&lt;/p&gt;
&lt;p&gt;This presents clinicians with a risk-reward tradeoff - trting too aggressively or too conservatively with ACT may reduce survival. To help inform decisions, an echocardiogram is done to help decide if the heart is healthy enough to tolerate ACT. Clinicians follow various rules of thumb in practice. For instance: withholding ACT if a patient’s current ejection fraction (measured via echocardiogram) falls below some absolute threshold (eg 50%) or if it declines more than some relative (e.g. declines more than 20% from time of enrollment). Briefly, ejection fraction (EF) is the proportion of blood that is pumped out of the heart’s left ventricle during a beat. In healthy individuals, this is fairly high (from 50-75%) but can be lower among patients with cardiotoxicity. These treatment rules are “dynamic” in that ACT is decided based on evolving ejection fraction. This is in contrast to static rules such as “always treat with ACT”, “never treat with ACT” or “alternate ACT” - regardless of EF. The goal is to develop a strategy for estimating survival rates under various hypothetical ACT assignment rules. If we have such a method, we could evaluate the efficacy of the various rules of thumb employed in clinics and arrive at a more data-driven approach.&lt;/p&gt;
&lt;p&gt;Luckily, using data from the Phase III AAML1031 trial, we can estimate effects of such ACT treatment strategies on survival. In the trial, patients move through a sequence of 4 chemo courses. Ahead of each course, an echo is conducted &amp;amp; used to decide ACT inclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Challenges&lt;/h1&gt;
&lt;p&gt;There are many impediments to valid statistical estimation. 1) ACT is not randomized in the trial but informed by time-varying features such as EF. We need to adjust for such features to get an apples-to-apples comparison of different ACT rules. 2) Some patients die or drop out before ever completing the sequence. In the latter case, we are left with incomplete survival information. 3) Treatment courses are not initiated at pre-fixed times, but depending on when subjects recover from the previous chemotherapy course.&lt;/p&gt;
&lt;p&gt;This last point suggests that the waiting times between treatments are potential confounders (e.g. slower recovery after previous treatment may inform subsequent treatment and drive survival) - quite a unique type of time-varying confounding which requires modifications to the usual g-computation algorithm for sequential treatments&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-bayesian-semiparametric-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Bayesian Semiparametric Method&lt;/h1&gt;
&lt;p&gt;We model the causal structure using a non-homogenous continuous-time transition process. After each course, patients can transition into a state of subsequent death or transition into a state of subsequent treatment - whichever comes first - in continuous-time. These process is characterized by a pair of transition probabilities: one pair for each treatment course. We use Bayesian semiparametric hazard models to estimate the transition probabilities at each stage as a function of features available at the start of this course.&lt;/p&gt;
&lt;p&gt;Causal effects of certain rules are computed by simulating from the transition process under a specified rule and computing the proportion of simulated subjects who survive past a certain time point (e.g. 3 years, if we want to compute 3-year survival).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Specifying Accelerated Failure Time Models in STAN</title>
      <link>https://stablemarkets.netlify.app/post/post2/specifying-accelerated-failure-time-models-in-stan/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post2/specifying-accelerated-failure-time-models-in-stan/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is an add-on to my &lt;a href=&#34;https://stablemarkets.netlify.com/post/post1/bayesian-survival-analysis-with-data-augmentation/&#34;&gt;previous post&lt;/a&gt; about augmented gibbs sampling for censored survival times. If you’re not a complete maniac like me, then you probably don’t want to code your own sampler from scratch like I did in that previous post. Luckily you don’t have to because you can easily specify that same model in &lt;a href=&#34;https://mc-stan.org/users/interfaces/rstan&#34;&gt;Stan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start with simulating some randomly censored data from a Weibull model. In this case, we just include a binary indicator and are interested in characterizing survival between these two groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

n &amp;lt;- 1000

# simulate covariates (just a binary treatment indicator)
A &amp;lt;- rbinom(n, 1, .5)
X &amp;lt;- model.matrix(~ A)

# true parameters
true_beta &amp;lt;- (1/2)*matrix(c(-1/3, 2), ncol=1)
true_mu &amp;lt;- X %*% true_beta

true_sigma &amp;lt;- 1

true_alpha &amp;lt;- 1/true_sigma
true_lambda &amp;lt;- exp(-1*true_mu*true_alpha)

# simulate censoring and survival times
survt = rweibull(n, shape=true_alpha, scale = true_lambda) 
cent = rweibull(n, shape=true_alpha, scale = true_lambda)

## observed data:
#censoring indicator
delta &amp;lt;- cent &amp;lt; survt
survt[delta==1] &amp;lt;- cent[delta==1] # censor survival time.

# count number of missing/censored survival times
n_miss &amp;lt;- sum(delta)

d_list &amp;lt;- list(N_m = n_miss, N_o = n - n_miss, P=2, # number of betas
               # data for censored subjects
               y_m=survt[delta==1], X_m=X[delta==1,],
               # data for uncensored subjects
               y_o=survt[delta==0], X_o=X[delta==0,])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The list &lt;code&gt;d_list&lt;/code&gt; is what we’ll eventually feed to Stan. Below is the Stan model for Weibull distributed survival times. Note in the transformed parameters block we specify the canonical accelerated failure time (AFT) parameterization - modeling the scale as a function of the shape parameter, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, and covariates.&lt;/p&gt;
&lt;p&gt;In the model block, we specify the likelihood as the Weibull density for uncensored subjects, and then augment the likelihood with evaluations from the Weibull survival function (&lt;code&gt;_lccdf&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The generated quantities block transforms the parameters to get posterior draws of the hazard ratio (as specified in my &lt;a href=&#34;https://stablemarkets.netlify.com/post/post1/bayesian-survival-analysis-with-data-augmentation/&#34;&gt;previous post&lt;/a&gt; ) as well as posterior draws of the survival function.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower=0&amp;gt; P; // number of beta parameters
  
  // data for censored subjects
  int&amp;lt;lower=0&amp;gt; N_m;
  matrix[N_m,P] X_m;
  vector[N_m] y_m;
  
  // data for observed subjects
  int&amp;lt;lower=0&amp;gt; N_o;
  matrix[N_o,P] X_o;
  real y_o[N_o];
}

parameters {
  vector[P] beta;                
  real&amp;lt;lower=0&amp;gt; alpha; // Weibull Shape      
}

transformed parameters{
  // model Weibull rate as function of covariates
  vector[N_m] lambda_m;
  vector[N_o] lambda_o;
  
  // standard weibull AFT re-parameterization
  lambda_m = exp((X_m*beta)*alpha);
  lambda_o = exp((X_o*beta)*alpha);
}

model {
  beta ~ normal(0, 100);
  alpha ~ exponential(1);
  
  // evaluate likelihood for censored and uncensored subjects
  target += weibull_lpdf(y_o | alpha, lambda_o);
  target += weibull_lccdf(y_m | alpha, lambda_m);
}


// generate posterior quantities of interest
generated quantities{
  vector[1000] post_pred_trt;
  vector[1000] post_pred_pbo;
  real lambda_trt; 
  real lambda_pbo; 
  real hazard_ratio;
  
  // generate hazard ratio
  lambda_trt = exp((beta[1] + beta[2])*alpha ) ;
  lambda_pbo = exp((beta[1])*alpha ) ;
  
  hazard_ratio = exp(beta[2]*alpha ) ;
  
  // generate survival times (for plotting survival curves)
  for(i in 1:1000){
    post_pred_trt[i] = weibull_rng(alpha,  lambda_trt);
    post_pred_pbo[i] = weibull_rng(alpha,  lambda_pbo);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan model specified above is stored in an object called &lt;code&gt;weibull_mod&lt;/code&gt;, which is called below in &lt;code&gt;sampling()&lt;/code&gt;. The code below samples from the posterior and outputs posterior draws of the hazard and predicted survival times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weibull_fit &amp;lt;- sampling(weibull_mod,
                data = d_list, 
                chains = 1, iter=20000, warmup=19000, save_warmup=F,
                pars= c(&amp;#39;hazard_ratio&amp;#39;,&amp;#39;post_pred_trt&amp;#39;,&amp;#39;post_pred_pbo&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;80acc0f9293b946800a710dd7f5e211c&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000172 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.72 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)
## Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)
## Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)
## Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)
## Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)
## Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)
## Chain 1: Iteration: 12000 / 20000 [ 60%]  (Warmup)
## Chain 1: Iteration: 14000 / 20000 [ 70%]  (Warmup)
## Chain 1: Iteration: 16000 / 20000 [ 80%]  (Warmup)
## Chain 1: Iteration: 18000 / 20000 [ 90%]  (Warmup)
## Chain 1: Iteration: 19001 / 20000 [ 95%]  (Sampling)
## Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 12.2212 seconds (Warm-up)
## Chain 1:                1.03363 seconds (Sampling)
## Chain 1:                13.2548 seconds (Total)
## Chain 1:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_draws&amp;lt;-extract(weibull_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot posterior distribution of the hazard ratio. The red line indicates the true value under which we generated the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(post_draws$hazard_ratio,
     xlab=&amp;#39;Hazard Ratio&amp;#39;, main=&amp;#39;Hazard Ratio Posterior Distribution&amp;#39;)
abline(v=exp(-1*true_beta[2,1]*true_alpha), col=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post2/2019-03-09-specifying-accelerated-failure-time-models-in-stan_files/figure-html/plot_hazard_ratio-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(post_draws$hazard_ratio)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3658342&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(post_draws$hazard_ratio, probs = c(.025, .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.3039049 0.4376196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot the survival functions. Note these results are very similar to the augmented sampler coded in the previous post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(survfit(Surv(survt, 1-delta) ~ A ), col=c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;),
     xlab=&amp;#39;Time&amp;#39;,ylab=&amp;#39;Survival Probability&amp;#39;, conf.int=T)

for(i in 1:1000){
  trt_ecdf &amp;lt;- ecdf(post_draws$post_pred_trt[i,])
  curve(1 - trt_ecdf(x), from = 0, to=4, add=T, col=&amp;#39;gray&amp;#39;)
  
  pbo_ecdf &amp;lt;- ecdf(post_draws$post_pred_pbo[i,])
  curve(1 - pbo_ecdf(x), from = 0, to=4, add=T, col=&amp;#39;lightblue&amp;#39;)
}

lines(survfit(Surv(survt, 1-delta) ~ A ), col=c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;), add=T,
      conf.int=T)

legend(&amp;#39;topright&amp;#39;, 
       legend = c(&amp;#39;KM Curve and Intervals (TRT)&amp;#39;,
                  &amp;#39;Posterior Survival Draws (TRT)&amp;#39;,
                  &amp;#39;KM Curve and Intervals (PBO)&amp;#39;,
                  &amp;#39;Posterior Survival Draws (PBO)&amp;#39;),
       col=c(&amp;#39;black&amp;#39;,&amp;#39;gray&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;lightblue&amp;#39;), 
       lty=c(1,0,1,0), pch=c(NA,15,NA,15), bty=&amp;#39;n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post2/2019-03-09-specifying-accelerated-failure-time-models-in-stan_files/figure-html/plot_survival-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
