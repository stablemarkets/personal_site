<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nonparametrics | </title>
    <link>https://stablemarkets.netlify.app/category/nonparametrics/</link>
      <atom:link href="https://stablemarkets.netlify.app/category/nonparametrics/index.xml" rel="self" type="application/rss+xml" />
    <description>Nonparametrics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 03 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://stablemarkets.netlify.app/media/icon_hu56437e0029043a6980b2632a303bfe7c_22443_512x512_fill_lanczos_center_3.png</url>
      <title>Nonparametrics</title>
      <link>https://stablemarkets.netlify.app/category/nonparametrics/</link>
    </image>
    
    <item>
      <title>Bayesian Sequential Decision-Making with Informative Timing</title>
      <link>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post6/bayeseq_aml/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is a brief summary of highlights from our &lt;a href=&#34;https://arxiv.org/abs/2211.16393&#34;&gt;recent working paper&lt;/a&gt; titled “Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing.” It’s part of a series of projects on developing robust Bayesian nonparametric models for sequential decision making in a variety of complex settings. In this case, we deal with a situation in which the decisions are informatively timed - with a motivating application in chemotherapy treatments for pediatric acute myeloid leukemia (AML).&lt;/p&gt;
&lt;p&gt;This is partially funded by a &lt;a href=&#34;https://www.pcori.org/research-results/2022/statistical-methods-optimizing-dynamic-patient-level-treatment-and-monitoring-strategies&#34;&gt;PCORI grant&lt;/a&gt; grant awarded earlier this year (2022).&lt;/p&gt;
&lt;div id=&#34;setting-and-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting and Problem&lt;/h1&gt;
&lt;p&gt;Chemotherapy treatment in AML is not “one and done” but involves making a sequence of treatment decisions over time, with each subsequent treatment decision depending on how the patient has responded to previous treatments and the evolution of their disease process. There are many chemo agents. This work focuses on understanding the effect of anthracyclines (ACT) in particular on survival. ACT is known to be effective in certain cases, but it can also lead to cardiotoxicity and subsequent early death.&lt;/p&gt;
&lt;p&gt;This presents clinicians with a risk-reward tradeoff - trting too aggressively or too conservatively with ACT may reduce survival. To help inform decisions, an echocardiogram is done to help decide if the heart is healthy enough to tolerate ACT. Clinicians follow various rules of thumb in practice. For instance: withholding ACT if a patient’s current ejection fraction (measured via echocardiogram) falls below some absolute threshold (eg 50%) or if it declines more than some relative (e.g. declines more than 20% from time of enrollment). Briefly, ejection fraction (EF) is the proportion of blood that is pumped out of the heart’s left ventricle during a beat. In healthy individuals, this is fairly high (from 50-75%) but can be lower among patients with cardiotoxicity. These treatment rules are “dynamic” in that ACT is decided based on evolving ejection fraction. This is in contrast to static rules such as “always treat with ACT”, “never treat with ACT” or “alternate ACT” - regardless of EF. The goal is to develop a strategy for estimating survival rates under various hypothetical ACT assignment rules. If we have such a method, we could evaluate the efficacy of the various rules of thumb employed in clinics and arrive at a more data-driven approach.&lt;/p&gt;
&lt;p&gt;Luckily, using data from the Phase III AAML1031 trial, we can estimate effects of such ACT treatment strategies on survival. In the trial, patients move through a sequence of 4 chemo courses. Ahead of each course, an echo is conducted &amp;amp; used to decide ACT inclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Challenges&lt;/h1&gt;
&lt;p&gt;There are many impediments to valid statistical estimation. 1) ACT is not randomized in the trial but informed by time-varying features such as EF. We need to adjust for such features to get an apples-to-apples comparison of different ACT rules. 2) Some patients die or drop out before ever completing the sequence. In the latter case, we are left with incomplete survival information. 3) Treatment courses are not initiated at pre-fixed times, but depending on when subjects recover from the previous chemotherapy course.&lt;/p&gt;
&lt;p&gt;This last point suggests that the waiting times between treatments are potential confounders (e.g. slower recovery after previous treatment may inform subsequent treatment and drive survival) - quite a unique type of time-varying confounding which requires modifications to the usual g-computation algorithm for sequential treatments&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-bayesian-semiparametric-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Bayesian Semiparametric Method&lt;/h1&gt;
&lt;p&gt;We model the causal structure using a non-homogenous continuous-time transition process. After each course, patients can transition into a state of subsequent death or transition into a state of subsequent treatment - whichever comes first - in continuous-time. These process is characterized by a pair of transition probabilities: one pair for each treatment course. We use Bayesian semiparametric hazard models to estimate the transition probabilities at each stage as a function of features available at the start of this course.&lt;/p&gt;
&lt;p&gt;Causal effects of certain rules are computed by simulating from the transition process under a specified rule and computing the proportion of simulated subjects who survive past a certain time point (e.g. 3 years, if we want to compute 3-year survival).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gamma Process Prior for Semiparametric Survival Analysis</title>
      <link>https://stablemarkets.netlify.app/post/post3/gamma-process-prior/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      <guid>https://stablemarkets.netlify.app/post/post3/gamma-process-prior/</guid>
      <description>
&lt;script src=&#34;https://stablemarkets.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motviation&#34;&gt;Motviation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-gamma-process-prior&#34;&gt;The Gamma Process Prior&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#independent-hazards&#34;&gt;Independent Hazards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlated-hazards&#34;&gt;Correlated Hazards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Heads up: equations may not render on blog aggregation sites. See original post &lt;a href=&#34;https://stablemarkets.netlify.com/post/post3/gamma-process-prior/&#34;&gt;here&lt;/a&gt; for good formatting. If you like this post, you can follow me on &lt;a href=&#34;https://twitter.com/StableMarkets&#34;&gt;twitter&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;motviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motviation&lt;/h2&gt;
&lt;p&gt;Suppose we observe survival/event times from some distribution
&lt;span class=&#34;math display&#34;&gt;\[T_{i\in1:n} \stackrel{iid}{\sim} f(t)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the density and &lt;span class=&#34;math inline&#34;&gt;\(F(t)=1-S(t)\)&lt;/span&gt; is the corresponding CDF expressed in terms of the survival function &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;. We can represent the hazard function of this distribution in terms of the density,
&lt;span class=&#34;math display&#34;&gt;\[\lambda(t) = \frac{f(t)}{S(t)}\]&lt;/span&gt;
The hazard, CDF, and survival functions are all related. Thus, if we have a model for the hazard, we also have a model for the survival function and the survival time distribution. The well-known Cox proportional hazard approach models the hazard as a function of covariates &lt;span class=&#34;math inline&#34;&gt;\(x_i \in \mathbb{R}^p\)&lt;/span&gt; that multiply some baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[ \lambda(t_i) = \lambda_0(t_i)\exp(x_i&amp;#39;\theta)\]&lt;/span&gt;
Frequentist estimation of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; follows from maximizing the profile likelihood - which avoids the need to specify the baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;. The model is semi-parametric because, while we don’t model the baseline hazard, we require that the multiplicative relationship between covariates and the hazard is correct.&lt;/p&gt;
&lt;p&gt;This already works fine, so why go Bayesian? Here are just a few (hopefully) compelling reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We may want to nonparametrically estimate the baseline hazard itself.&lt;/li&gt;
&lt;li&gt;Posterior inference is exact, so we don’t need to rely on asymptotic uncertainty estimates (though we may want to evaluate the frequentist properties of resulting point and interval estimates).&lt;/li&gt;
&lt;li&gt;Easy credible interval estimation for any function of the parameters. If we have posterior samples for the hazard, we also get automatic inference for the survival function as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full Bayesian inference requires a proper probability model for both &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt;. This post walks through a Bayesian approach that places a nonparametric prior on &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt; - specifically the Gamma Process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gamma-process-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Gamma Process Prior&lt;/h2&gt;
&lt;div id=&#34;independent-hazards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independent Hazards&lt;/h3&gt;
&lt;p&gt;Most of this comes from &lt;a href=&#34;https://www.jstor.org/stable/pdf/2984758.pdf?refreqid=excelsior%3Ad44a383839b71ed572992049f08f2932&#34;&gt;Kalbfleisch (1978)&lt;/a&gt;, with an excellent technical outline by &lt;a href=&#34;https://www.springer.com/us/book/9780387952772&#34;&gt;Ibrahim (2001)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recall that the cumulative baseline hazard &lt;span class=&#34;math inline&#34;&gt;\(H_0(t) = \int_0^t \lambda_0(t) dt\)&lt;/span&gt; where the integral is the Riemann-Stieltjes integral. The central idea is to develop a prior for the cumulative hazard &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt;, which will then admit a prior for the hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Gamma Process is such a prior. Each realization of a Gamma Process is a cumulative hazard function that is centered around some prior cumulative hazard function, &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;, with a sort of dispersion/concentration parameter, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that controls how tightly the realizations are distributed around the prior &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Okay, now the math. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}(\alpha, \beta)\)&lt;/span&gt; denote the Gamma distribution with shape parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(H^*(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t\geq 0\)&lt;/span&gt; be our prior cumulative hazard function. For example we could choose &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt; to be the exponential cumulative hazard, &lt;span class=&#34;math inline&#34;&gt;\(H^*(t)= \eta\cdot t\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is a fixed hyperparameter. By definition &lt;span class=&#34;math inline&#34;&gt;\(H^*(0)=0\)&lt;/span&gt;. The Gamma Process is defined as having the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0(0) = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t) = H_0(t) - H_0(s) \sim \mathcal G \Big(\ \beta\big(H^*(t) - H^*(s)\big)\ , \ \beta \ \Big)\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The increments in the cumulative hazard is the hazard function. The gamma process has the property that these increments are independent and Gamma-distributed. For a set of time increments &lt;span class=&#34;math inline&#34;&gt;\(t\geq0\)&lt;/span&gt;, we can use the properties above to generate one realization of hazards &lt;span class=&#34;math inline&#34;&gt;\(\{\lambda_0(t) \}_{t\geq0}\)&lt;/span&gt;. Equivaltently, one realization of the cumulative hazard function is &lt;span class=&#34;math inline&#34;&gt;\(\{H_0(t)\}_{t\geq0}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(H_0(t) = \sum_{k=0}^t \lambda_0(k)\)&lt;/span&gt;. We denote the Gamma Process just described as
&lt;span class=&#34;math display&#34;&gt;\[H_0(t) \sim \mathcal{GP}\Big(\ \beta H^*(t), \ \beta \Big),  \ \ t\geq0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Below in Panel A are some prior realizations of &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt; with a Weibull &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt; prior for various concentration parameters, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Notice for low &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; the realizations are widely dispersed around the mean cumulative hazard. Higher &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; yields to tighter dispersion around &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since there’s a correspondence between the &lt;span class=&#34;math inline&#34;&gt;\(H_0(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(S_0(t)\)&lt;/span&gt;, we could also plot prior realizations of the baseline survival function &lt;span class=&#34;math inline&#34;&gt;\(S_0(t) = \exp\big\{- H_0(t) \big\}\)&lt;/span&gt; using the realization &lt;span class=&#34;math inline&#34;&gt;\(\{H_0(t)\}_{t\geq0}\)&lt;/span&gt;. This is shown in Panel B with the Weibull survival function &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; corresponding to &lt;span class=&#34;math inline&#34;&gt;\(H^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post3/2019-05-11-the-gamma-process-prior_files/figure-html/unnamed-chunk-1-.gif&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlated-hazards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlated Hazards&lt;/h3&gt;
&lt;p&gt;In the previous section, the hazards &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt; between increments were a priori independent - a naive prior belief perhaps. Instead, we might expect that the hazard at the current time point is centered around the hazard in the previous time point. We’d also expect that a higher hazard at the previous time point likely means a higher hazard at the current time point (positive correlation across time).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00298&#34;&gt;Nieto‐Barajas et al (2002)&lt;/a&gt; came up with a correlated Gamma Process that expresses exactly this prior belief. The basic idea is to introduce a latent stochastic process &lt;span class=&#34;math inline&#34;&gt;\(\{u_t\}_{t\geq0}\)&lt;/span&gt; that links &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0(t-1)\)&lt;/span&gt;. Here is the correlated Gamma Process,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Draw a hazard rate for the first time interval, &lt;span class=&#34;math inline&#34;&gt;\(I_1=[0, t_1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \sim \mathcal G(\beta H^*(t_1), \beta)\)&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw a latent variable &lt;span class=&#34;math inline&#34;&gt;\(u_1 | \lambda_1 \sim Pois(c \cdot \lambda_1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw a hazard rate for second time interval &lt;span class=&#34;math inline&#34;&gt;\(I_2 = [t_1, t_2)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2 \sim \mathcal G(\beta( H^*(t_2) - H^*(t_1) ) + u_1, \beta + c )\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In general for &lt;span class=&#34;math inline&#34;&gt;\(k\geq1\)&lt;/span&gt;, define &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \beta( H^*(t_k) - H^*(t_{k-1}) )\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(u_k | \lambda_k \sim Pois(c\cdot \lambda_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1} | u_k \sim \mathcal G( \alpha_k + u_k, \beta + c )\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that if &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(u=0\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and the process reduces to the independent Gamma Process in the previous section. Now consider &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt;. Then, the hazard rate in the next interval has mean &lt;span class=&#34;math inline&#34;&gt;\(E[\lambda_{k+1} | u_k] = \frac{ \alpha_k + u_k }{\beta+c}\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(u_k \sim Pois(\lambda_k)\)&lt;/span&gt; is centered around the current &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt; - allowinng &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt; to influence &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1}\)&lt;/span&gt; through the latent variable &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;. The higher the current hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_k\)&lt;/span&gt;, the higher &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;, and the higher the mean of the next hazard, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{k+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Below are some realizations of a correlated and independent Gamma processes centered around the &lt;span class=&#34;math inline&#34;&gt;\(Weibull(2,1.5)\)&lt;/span&gt; hazard shown in red. One realization is higlighted in blue to make it easier to see the differences between correlated and independent realizations&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stablemarkets.netlify.app/post/post3/2019-05-11-the-gamma-process-prior_files/figure-html/unnamed-chunk-2-.gif&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice the correlated gamma process looks very snake-y. This is because of the autoregressive structure on &lt;span class=&#34;math inline&#34;&gt;\(\{\lambda_0(t)\}_{t\geq0}\)&lt;/span&gt; induced by the latent process&lt;span class=&#34;math inline&#34;&gt;\(\{ u_t\}_{t\geq0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
